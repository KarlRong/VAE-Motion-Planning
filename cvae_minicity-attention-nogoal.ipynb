{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda:1\n",
      "GeForce GTX 1660 SUPER\n",
      "P106-100\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device\", device)\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.get_device_name(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from data.MinicityDataset import TrafficDataset\n",
    "\n",
    "dbpath = '/home/rong/disk/database/minicity.db'\n",
    "bs = 20\n",
    "train_loader = DataLoader(TrafficDataset(dbpath = dbpath,\n",
    "                            train = True),\n",
    "                         batch_size = bs, shuffle=True, drop_last = True)\n",
    "test_loader = DataLoader(TrafficDataset(dbpath = dbpath,\n",
    "                            train = False),\n",
    "                          batch_size = bs, shuffle=True, drop_last = True)\n",
    "\n",
    "# batch = next(iter(train_loader))\n",
    "# batch['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convVAE(\n",
      "  (condnn): CondNN(\n",
      "    (cnn): Conv3d(\n",
      "      (adap_pool): AdaptiveAvgPool3d(output_size=(25, 200, 200))\n",
      "      (conv_layer1): Sequential(\n",
      "        (0): Conv3d(1, 16, kernel_size=(2, 3, 3), stride=(1, 1, 1))\n",
      "        (1): LeakyReLU(negative_slope=0.01)\n",
      "        (2): Conv3d(16, 16, kernel_size=(2, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (3): LeakyReLU(negative_slope=0.01)\n",
      "        (4): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (conv_layer2): Sequential(\n",
      "        (0): Conv3d(16, 32, kernel_size=(2, 3, 3), stride=(1, 1, 1))\n",
      "        (1): LeakyReLU(negative_slope=0.01)\n",
      "        (2): Conv3d(32, 32, kernel_size=(2, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (3): LeakyReLU(negative_slope=0.01)\n",
      "        (4): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (conv_layer5): Conv3d(32, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1))\n",
      "      (adap_pool2): AdaptiveAvgPool3d(output_size=(4, 10, 10))\n",
      "    )\n",
      "    (Attention): Attention(\n",
      "      (encoder_att): Linear(in_features=67, out_features=64, bias=True)\n",
      "      (condition_att): Linear(in_features=8, out_features=64, bias=True)\n",
      "      (full_att): Linear(in_features=64, out_features=1, bias=True)\n",
      "      (relu): ReLU()\n",
      "      (softmax): Softmax(dim=1)\n",
      "    )\n",
      "    (fc1): Linear(in_features=172, out_features=300, bias=True)\n",
      "  )\n",
      "  (encoder): Encoder(\n",
      "    (sequential): Sequential(\n",
      "      (0): Linear(in_features=304, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=1024, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "    (linear_means): Linear(in_features=512, out_features=50, bias=True)\n",
      "    (linear_log_var): Linear(in_features=512, out_features=50, bias=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (sequential): Sequential(\n",
      "      (0): Linear(in_features=350, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=1024, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (5): ReLU()\n",
      "      (6): Linear(in_features=512, out_features=4, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from models.minicity_nogoal_bigger import convVAE\n",
    "\n",
    "X_dim = 4\n",
    "traj_size = 25 * 4\n",
    "z_dim = 50\n",
    "cnn_out_size = 300\n",
    "cond_out_size = 300\n",
    "\n",
    "model = convVAE(sample_size = X_dim, \n",
    "                  traj_size = traj_size,\n",
    "                  cnnout_size = cnn_out_size, \n",
    "                  cond_out_size = cond_out_size, \n",
    "                  encoder_layer_sizes = [512,1024,512], \n",
    "                  latent_size = z_dim, \n",
    "                  decoder_layer_sizes = [512,1024,512]).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(recon_x, x, w, mean, log_var):\n",
    "    MSE = torch.mean((w.expand_as(x) * (recon_x-x)**2))\n",
    "    KLD = - 0.0005 * torch.mean(torch.sum(1 + log_var - mean.pow(2) - log_var.exp(), 1))\n",
    "    return MSE + KLD, MSE\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, writer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    mse_loss = 0\n",
    "    w = torch.tensor([8, 10, 2, 3], dtype=torch.float).to(device)\n",
    "    adap_pool = nn.AdaptiveAvgPool3d((25,200, 200))\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        startgoal = batch[\"start_goal\"].to(device)\n",
    "        startgoal[:, 4] = 0\n",
    "        startgoal[:, 5] = 0\n",
    "        occ = batch[\"observation\"]\n",
    "        occ = occ[:, :, 200:600, 200:600]        \n",
    "        occ = adap_pool(occ)\n",
    "        occ = occ.to(device)\n",
    "        occ = occ.unsqueeze(1)\n",
    "        data = batch[\"data\"].to(device)\n",
    "        traj = batch[\"traj\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data, startgoal, traj,  occ)\n",
    "        loss, mse= loss_fn(recon_batch, data, w, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        mse_loss += mse.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item()))\n",
    "        \n",
    "            writer.add_scalar('BatchLoss/loss', loss.item(), batch_idx)\n",
    "            writer.add_scalar('BatchLoss/mse_loss', mse.item(), batch_idx)\n",
    "\n",
    "    epoch_loss = train_loss * len(data) / len(train_loader.dataset)\n",
    "    epoch_mse = mse_loss * len(data) / len(train_loader.dataset)\n",
    "    print('====> Epoch: {} Average loss: {:.7f}'.format(\n",
    "          epoch, epoch_loss))\n",
    "    return epoch, epoch_loss, epoch_mse\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    mse_loss = 0\n",
    "    w = torch.tensor([8, 10, 2, 3], dtype=torch.float).to(device)\n",
    "    for batch_idx, batch in enumerate(test_loader):\n",
    "        startgoal = batch[\"start_goal\"].to(device)\n",
    "        occ = batch[\"observation\"].to(device)\n",
    "        occ = occ.unsqueeze(1)\n",
    "        data = batch[\"data\"].to(device)\n",
    "        \n",
    "        recon_batch, mu, logvar = model(sample, startend, occ)\n",
    "        loss, mse= loss_fn(recon_batch, data, w, mu, logvar)\n",
    "        test_loss += loss.item()\n",
    "        mse_loss += mse.item()\n",
    "\n",
    "    epoch_loss = test_loss * len(data) / len(test_loader.dataset)\n",
    "    epoch_mse = mse_loss * len(data) / len(test_loader.dataset)\n",
    "    print('====> Epoch: {} Average test loss: {:.7f}'.format(\n",
    "          epoch, epoch_loss))\n",
    "    return epoch, epoch_loss, epoch_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Writer will output to ./runs/ directory by default\n",
    "writer = SummaryWriter('runs/minicity_attention_nogoal_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 2 GPUs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rong/.local/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py:26: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): convVAE(\n",
       "    (condnn): CondNN(\n",
       "      (cnn): Conv3d(\n",
       "        (adap_pool): AdaptiveAvgPool3d(output_size=(25, 200, 200))\n",
       "        (conv_layer1): Sequential(\n",
       "          (0): Conv3d(1, 16, kernel_size=(2, 3, 3), stride=(1, 1, 1))\n",
       "          (1): LeakyReLU(negative_slope=0.01)\n",
       "          (2): Conv3d(16, 16, kernel_size=(2, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "          (3): LeakyReLU(negative_slope=0.01)\n",
       "          (4): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (conv_layer2): Sequential(\n",
       "          (0): Conv3d(16, 32, kernel_size=(2, 3, 3), stride=(1, 1, 1))\n",
       "          (1): LeakyReLU(negative_slope=0.01)\n",
       "          (2): Conv3d(32, 32, kernel_size=(2, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "          (3): LeakyReLU(negative_slope=0.01)\n",
       "          (4): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (conv_layer5): Conv3d(32, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1))\n",
       "        (adap_pool2): AdaptiveAvgPool3d(output_size=(4, 10, 10))\n",
       "      )\n",
       "      (Attention): Attention(\n",
       "        (encoder_att): Linear(in_features=67, out_features=64, bias=True)\n",
       "        (condition_att): Linear(in_features=8, out_features=64, bias=True)\n",
       "        (full_att): Linear(in_features=64, out_features=1, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (softmax): Softmax(dim=1)\n",
       "      )\n",
       "      (fc1): Linear(in_features=172, out_features=300, bias=True)\n",
       "    )\n",
       "    (encoder): Encoder(\n",
       "      (sequential): Sequential(\n",
       "        (0): Linear(in_features=304, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (linear_means): Linear(in_features=512, out_features=50, bias=True)\n",
       "      (linear_log_var): Linear(in_features=512, out_features=50, bias=True)\n",
       "    )\n",
       "    (decoder): Decoder(\n",
       "      (sequential): Sequential(\n",
       "        (0): Linear(in_features=350, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (5): ReLU()\n",
       "        (6): Linear(in_features=512, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.device_count() > 1:\n",
    "  print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "  # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "  model = nn.DataParallel(model, device_ids=[1, 0])\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/210000 (0%)]\tLoss: 0.134827\n",
      "Train Epoch: 0 [400/210000 (0%)]\tLoss: 0.096139\n",
      "Train Epoch: 0 [800/210000 (0%)]\tLoss: 0.031028\n",
      "Train Epoch: 0 [1200/210000 (1%)]\tLoss: 0.031510\n",
      "Train Epoch: 0 [1600/210000 (1%)]\tLoss: 0.056573\n",
      "Train Epoch: 0 [2000/210000 (1%)]\tLoss: 0.049827\n",
      "Train Epoch: 0 [2400/210000 (1%)]\tLoss: 0.032613\n",
      "Train Epoch: 0 [2800/210000 (1%)]\tLoss: 0.034210\n",
      "Train Epoch: 0 [3200/210000 (2%)]\tLoss: 0.028985\n",
      "Train Epoch: 0 [3600/210000 (2%)]\tLoss: 0.055694\n",
      "Train Epoch: 0 [4000/210000 (2%)]\tLoss: 0.031094\n",
      "Train Epoch: 0 [4400/210000 (2%)]\tLoss: 0.066184\n",
      "Train Epoch: 0 [4800/210000 (2%)]\tLoss: 0.023478\n",
      "Train Epoch: 0 [5200/210000 (2%)]\tLoss: 0.045533\n",
      "Train Epoch: 0 [5600/210000 (3%)]\tLoss: 0.037847\n",
      "Train Epoch: 0 [6000/210000 (3%)]\tLoss: 0.038264\n",
      "Train Epoch: 0 [6400/210000 (3%)]\tLoss: 0.049728\n",
      "Train Epoch: 0 [6800/210000 (3%)]\tLoss: 0.021330\n",
      "Train Epoch: 0 [7200/210000 (3%)]\tLoss: 0.022804\n",
      "Train Epoch: 0 [7600/210000 (4%)]\tLoss: 0.022089\n",
      "Train Epoch: 0 [8000/210000 (4%)]\tLoss: 0.051670\n",
      "Train Epoch: 0 [8400/210000 (4%)]\tLoss: 0.051846\n",
      "Train Epoch: 0 [8800/210000 (4%)]\tLoss: 0.022952\n",
      "Train Epoch: 0 [9200/210000 (4%)]\tLoss: 0.040598\n",
      "Train Epoch: 0 [9600/210000 (5%)]\tLoss: 0.016725\n",
      "Train Epoch: 0 [10000/210000 (5%)]\tLoss: 0.028120\n",
      "Train Epoch: 0 [10400/210000 (5%)]\tLoss: 0.011963\n",
      "Train Epoch: 0 [10800/210000 (5%)]\tLoss: 0.008031\n",
      "Train Epoch: 0 [11200/210000 (5%)]\tLoss: 0.011021\n",
      "Train Epoch: 0 [11600/210000 (6%)]\tLoss: 0.018949\n",
      "Train Epoch: 0 [12000/210000 (6%)]\tLoss: 0.011765\n",
      "Train Epoch: 0 [12400/210000 (6%)]\tLoss: 0.008750\n",
      "Train Epoch: 0 [12800/210000 (6%)]\tLoss: 0.013254\n",
      "Train Epoch: 0 [13200/210000 (6%)]\tLoss: 0.013168\n",
      "Train Epoch: 0 [13600/210000 (6%)]\tLoss: 0.006577\n",
      "Train Epoch: 0 [14000/210000 (7%)]\tLoss: 0.010876\n",
      "Train Epoch: 0 [14400/210000 (7%)]\tLoss: 0.010146\n",
      "Train Epoch: 0 [14800/210000 (7%)]\tLoss: 0.010998\n",
      "Train Epoch: 0 [15200/210000 (7%)]\tLoss: 0.008166\n",
      "Train Epoch: 0 [15600/210000 (7%)]\tLoss: 0.008943\n",
      "Train Epoch: 0 [16000/210000 (8%)]\tLoss: 0.014654\n",
      "Train Epoch: 0 [16400/210000 (8%)]\tLoss: 0.012989\n",
      "Train Epoch: 0 [16800/210000 (8%)]\tLoss: 0.017745\n",
      "Train Epoch: 0 [17200/210000 (8%)]\tLoss: 0.011040\n",
      "Train Epoch: 0 [17600/210000 (8%)]\tLoss: 0.012082\n",
      "Train Epoch: 0 [18000/210000 (9%)]\tLoss: 0.008329\n",
      "Train Epoch: 0 [18400/210000 (9%)]\tLoss: 0.017412\n",
      "Train Epoch: 0 [18800/210000 (9%)]\tLoss: 0.010300\n",
      "Train Epoch: 0 [19200/210000 (9%)]\tLoss: 0.008105\n",
      "Train Epoch: 0 [19600/210000 (9%)]\tLoss: 0.008001\n",
      "Train Epoch: 0 [20000/210000 (10%)]\tLoss: 0.007486\n",
      "Train Epoch: 0 [20400/210000 (10%)]\tLoss: 0.010455\n",
      "Train Epoch: 0 [20800/210000 (10%)]\tLoss: 0.014278\n",
      "Train Epoch: 0 [21200/210000 (10%)]\tLoss: 0.007862\n",
      "Train Epoch: 0 [21600/210000 (10%)]\tLoss: 0.018539\n",
      "Train Epoch: 0 [22000/210000 (10%)]\tLoss: 0.014455\n",
      "Train Epoch: 0 [22400/210000 (11%)]\tLoss: 0.014857\n",
      "Train Epoch: 0 [22800/210000 (11%)]\tLoss: 0.009610\n",
      "Train Epoch: 0 [23200/210000 (11%)]\tLoss: 0.009908\n",
      "Train Epoch: 0 [23600/210000 (11%)]\tLoss: 0.007675\n",
      "Train Epoch: 0 [24000/210000 (11%)]\tLoss: 0.005614\n",
      "Train Epoch: 0 [24400/210000 (12%)]\tLoss: 0.006364\n",
      "Train Epoch: 0 [24800/210000 (12%)]\tLoss: 0.007159\n",
      "Train Epoch: 0 [25200/210000 (12%)]\tLoss: 0.011034\n",
      "Train Epoch: 0 [25600/210000 (12%)]\tLoss: 0.005728\n",
      "Train Epoch: 0 [26000/210000 (12%)]\tLoss: 0.007478\n",
      "Train Epoch: 0 [26400/210000 (13%)]\tLoss: 0.005406\n",
      "Train Epoch: 0 [26800/210000 (13%)]\tLoss: 0.007159\n",
      "Train Epoch: 0 [27200/210000 (13%)]\tLoss: 0.013733\n",
      "Train Epoch: 0 [27600/210000 (13%)]\tLoss: 0.010247\n",
      "Train Epoch: 0 [28000/210000 (13%)]\tLoss: 0.012888\n",
      "Train Epoch: 0 [28400/210000 (14%)]\tLoss: 0.007751\n",
      "Train Epoch: 0 [28800/210000 (14%)]\tLoss: 0.007411\n",
      "Train Epoch: 0 [29200/210000 (14%)]\tLoss: 0.007311\n",
      "Train Epoch: 0 [29600/210000 (14%)]\tLoss: 0.007113\n",
      "Train Epoch: 0 [30000/210000 (14%)]\tLoss: 0.006102\n",
      "Train Epoch: 0 [30400/210000 (14%)]\tLoss: 0.008673\n",
      "Train Epoch: 0 [30800/210000 (15%)]\tLoss: 0.004488\n",
      "Train Epoch: 0 [31200/210000 (15%)]\tLoss: 0.009187\n",
      "Train Epoch: 0 [31600/210000 (15%)]\tLoss: 0.008599\n",
      "Train Epoch: 0 [32000/210000 (15%)]\tLoss: 0.007589\n",
      "Train Epoch: 0 [32400/210000 (15%)]\tLoss: 0.005120\n",
      "Train Epoch: 0 [32800/210000 (16%)]\tLoss: 0.007618\n",
      "Train Epoch: 0 [33200/210000 (16%)]\tLoss: 0.006374\n",
      "Train Epoch: 0 [33600/210000 (16%)]\tLoss: 0.004469\n",
      "Train Epoch: 0 [34000/210000 (16%)]\tLoss: 0.006418\n",
      "Train Epoch: 0 [34400/210000 (16%)]\tLoss: 0.005903\n",
      "Train Epoch: 0 [34800/210000 (17%)]\tLoss: 0.008421\n",
      "Train Epoch: 0 [35200/210000 (17%)]\tLoss: 0.004106\n",
      "Train Epoch: 0 [35600/210000 (17%)]\tLoss: 0.003802\n",
      "Train Epoch: 0 [36000/210000 (17%)]\tLoss: 0.004203\n",
      "Train Epoch: 0 [36400/210000 (17%)]\tLoss: 0.006046\n",
      "Train Epoch: 0 [36800/210000 (18%)]\tLoss: 0.004482\n",
      "Train Epoch: 0 [37200/210000 (18%)]\tLoss: 0.008719\n",
      "Train Epoch: 0 [37600/210000 (18%)]\tLoss: 0.010180\n",
      "Train Epoch: 0 [38000/210000 (18%)]\tLoss: 0.004975\n",
      "Train Epoch: 0 [38400/210000 (18%)]\tLoss: 0.008862\n",
      "Train Epoch: 0 [38800/210000 (18%)]\tLoss: 0.010201\n",
      "Train Epoch: 0 [39200/210000 (19%)]\tLoss: 0.004742\n",
      "Train Epoch: 0 [39600/210000 (19%)]\tLoss: 0.005150\n",
      "Train Epoch: 0 [40000/210000 (19%)]\tLoss: 0.005889\n",
      "Train Epoch: 0 [40400/210000 (19%)]\tLoss: 0.004878\n",
      "Train Epoch: 0 [40800/210000 (19%)]\tLoss: 0.007609\n",
      "Train Epoch: 0 [41200/210000 (20%)]\tLoss: 0.004569\n",
      "Train Epoch: 0 [41600/210000 (20%)]\tLoss: 0.008856\n",
      "Train Epoch: 0 [42000/210000 (20%)]\tLoss: 0.007261\n",
      "Train Epoch: 0 [42400/210000 (20%)]\tLoss: 0.005816\n",
      "Train Epoch: 0 [42800/210000 (20%)]\tLoss: 0.006383\n",
      "Train Epoch: 0 [43200/210000 (21%)]\tLoss: 0.006234\n",
      "Train Epoch: 0 [43600/210000 (21%)]\tLoss: 0.005053\n",
      "Train Epoch: 0 [44000/210000 (21%)]\tLoss: 0.004470\n",
      "Train Epoch: 0 [44400/210000 (21%)]\tLoss: 0.010445\n",
      "Train Epoch: 0 [44800/210000 (21%)]\tLoss: 0.007296\n",
      "Train Epoch: 0 [45200/210000 (22%)]\tLoss: 0.006035\n",
      "Train Epoch: 0 [45600/210000 (22%)]\tLoss: 0.005268\n",
      "Train Epoch: 0 [46000/210000 (22%)]\tLoss: 0.004759\n",
      "Train Epoch: 0 [46400/210000 (22%)]\tLoss: 0.008109\n",
      "Train Epoch: 0 [46800/210000 (22%)]\tLoss: 0.006765\n",
      "Train Epoch: 0 [47200/210000 (22%)]\tLoss: 0.004530\n",
      "Train Epoch: 0 [47600/210000 (23%)]\tLoss: 0.005860\n",
      "Train Epoch: 0 [48000/210000 (23%)]\tLoss: 0.004369\n",
      "Train Epoch: 0 [48400/210000 (23%)]\tLoss: 0.004770\n",
      "Train Epoch: 0 [48800/210000 (23%)]\tLoss: 0.008157\n",
      "Train Epoch: 0 [49200/210000 (23%)]\tLoss: 0.003657\n",
      "Train Epoch: 0 [49600/210000 (24%)]\tLoss: 0.003850\n",
      "Train Epoch: 0 [50000/210000 (24%)]\tLoss: 0.008245\n",
      "Train Epoch: 0 [50400/210000 (24%)]\tLoss: 0.011425\n",
      "Train Epoch: 0 [50800/210000 (24%)]\tLoss: 0.004488\n",
      "Train Epoch: 0 [51200/210000 (24%)]\tLoss: 0.005061\n",
      "Train Epoch: 0 [51600/210000 (25%)]\tLoss: 0.009032\n",
      "Train Epoch: 0 [52000/210000 (25%)]\tLoss: 0.006254\n",
      "Train Epoch: 0 [52400/210000 (25%)]\tLoss: 0.007091\n",
      "Train Epoch: 0 [52800/210000 (25%)]\tLoss: 0.005659\n",
      "Train Epoch: 0 [53200/210000 (25%)]\tLoss: 0.006094\n",
      "Train Epoch: 0 [53600/210000 (26%)]\tLoss: 0.005966\n",
      "Train Epoch: 0 [54000/210000 (26%)]\tLoss: 0.007552\n",
      "Train Epoch: 0 [54400/210000 (26%)]\tLoss: 0.006578\n",
      "Train Epoch: 0 [54800/210000 (26%)]\tLoss: 0.004913\n",
      "Train Epoch: 0 [55200/210000 (26%)]\tLoss: 0.004692\n",
      "Train Epoch: 0 [55600/210000 (26%)]\tLoss: 0.005729\n",
      "Train Epoch: 0 [56000/210000 (27%)]\tLoss: 0.003063\n",
      "Train Epoch: 0 [56400/210000 (27%)]\tLoss: 0.005252\n",
      "Train Epoch: 0 [56800/210000 (27%)]\tLoss: 0.005857\n",
      "Train Epoch: 0 [57200/210000 (27%)]\tLoss: 0.004360\n",
      "Train Epoch: 0 [57600/210000 (27%)]\tLoss: 0.005338\n",
      "Train Epoch: 0 [58000/210000 (28%)]\tLoss: 0.005887\n",
      "Train Epoch: 0 [58400/210000 (28%)]\tLoss: 0.003247\n",
      "Train Epoch: 0 [58800/210000 (28%)]\tLoss: 0.004184\n",
      "Train Epoch: 0 [59200/210000 (28%)]\tLoss: 0.004736\n",
      "Train Epoch: 0 [59600/210000 (28%)]\tLoss: 0.006112\n",
      "Train Epoch: 0 [60000/210000 (29%)]\tLoss: 0.007643\n",
      "Train Epoch: 0 [60400/210000 (29%)]\tLoss: 0.004637\n",
      "Train Epoch: 0 [60800/210000 (29%)]\tLoss: 0.005838\n",
      "Train Epoch: 0 [61200/210000 (29%)]\tLoss: 0.006958\n",
      "Train Epoch: 0 [61600/210000 (29%)]\tLoss: 0.003577\n",
      "Train Epoch: 0 [62000/210000 (30%)]\tLoss: 0.005866\n",
      "Train Epoch: 0 [62400/210000 (30%)]\tLoss: 0.005750\n",
      "Train Epoch: 0 [62800/210000 (30%)]\tLoss: 0.006270\n",
      "Train Epoch: 0 [63200/210000 (30%)]\tLoss: 0.003101\n",
      "Train Epoch: 0 [63600/210000 (30%)]\tLoss: 0.004242\n",
      "Train Epoch: 0 [64000/210000 (30%)]\tLoss: 0.006913\n",
      "Train Epoch: 0 [64400/210000 (31%)]\tLoss: 0.006970\n",
      "Train Epoch: 0 [64800/210000 (31%)]\tLoss: 0.009375\n",
      "Train Epoch: 0 [65200/210000 (31%)]\tLoss: 0.011724\n",
      "Train Epoch: 0 [65600/210000 (31%)]\tLoss: 0.014305\n",
      "Train Epoch: 0 [66000/210000 (31%)]\tLoss: 0.007029\n",
      "Train Epoch: 0 [66400/210000 (32%)]\tLoss: 0.005795\n",
      "Train Epoch: 0 [66800/210000 (32%)]\tLoss: 0.006743\n",
      "Train Epoch: 0 [67200/210000 (32%)]\tLoss: 0.008378\n",
      "Train Epoch: 0 [67600/210000 (32%)]\tLoss: 0.004698\n",
      "Train Epoch: 0 [68000/210000 (32%)]\tLoss: 0.005570\n",
      "Train Epoch: 0 [68400/210000 (33%)]\tLoss: 0.005493\n",
      "Train Epoch: 0 [68800/210000 (33%)]\tLoss: 0.005420\n",
      "Train Epoch: 0 [69200/210000 (33%)]\tLoss: 0.006009\n",
      "Train Epoch: 0 [69600/210000 (33%)]\tLoss: 0.004060\n",
      "Train Epoch: 0 [70000/210000 (33%)]\tLoss: 0.004690\n",
      "Train Epoch: 0 [70400/210000 (34%)]\tLoss: 0.005198\n",
      "Train Epoch: 0 [70800/210000 (34%)]\tLoss: 0.006227\n",
      "Train Epoch: 0 [71200/210000 (34%)]\tLoss: 0.005378\n",
      "Train Epoch: 0 [71600/210000 (34%)]\tLoss: 0.005945\n",
      "Train Epoch: 0 [72000/210000 (34%)]\tLoss: 0.004111\n",
      "Train Epoch: 0 [72400/210000 (34%)]\tLoss: 0.004885\n",
      "Train Epoch: 0 [72800/210000 (35%)]\tLoss: 0.003042\n",
      "Train Epoch: 0 [73200/210000 (35%)]\tLoss: 0.004736\n",
      "Train Epoch: 0 [73600/210000 (35%)]\tLoss: 0.004015\n",
      "Train Epoch: 0 [74000/210000 (35%)]\tLoss: 0.005184\n",
      "Train Epoch: 0 [74400/210000 (35%)]\tLoss: 0.006363\n",
      "Train Epoch: 0 [74800/210000 (36%)]\tLoss: 0.007338\n",
      "Train Epoch: 0 [75200/210000 (36%)]\tLoss: 0.006394\n",
      "Train Epoch: 0 [75600/210000 (36%)]\tLoss: 0.003988\n",
      "Train Epoch: 0 [76000/210000 (36%)]\tLoss: 0.007079\n",
      "Train Epoch: 0 [76400/210000 (36%)]\tLoss: 0.006746\n",
      "Train Epoch: 0 [76800/210000 (37%)]\tLoss: 0.004761\n",
      "Train Epoch: 0 [77200/210000 (37%)]\tLoss: 0.006307\n",
      "Train Epoch: 0 [77600/210000 (37%)]\tLoss: 0.005885\n",
      "Train Epoch: 0 [78000/210000 (37%)]\tLoss: 0.006172\n",
      "Train Epoch: 0 [78400/210000 (37%)]\tLoss: 0.006258\n",
      "Train Epoch: 0 [78800/210000 (38%)]\tLoss: 0.004359\n",
      "Train Epoch: 0 [79200/210000 (38%)]\tLoss: 0.006118\n",
      "Train Epoch: 0 [79600/210000 (38%)]\tLoss: 0.009829\n",
      "Train Epoch: 0 [80000/210000 (38%)]\tLoss: 0.003811\n",
      "Train Epoch: 0 [80400/210000 (38%)]\tLoss: 0.004038\n",
      "Train Epoch: 0 [80800/210000 (38%)]\tLoss: 0.005733\n",
      "Train Epoch: 0 [81200/210000 (39%)]\tLoss: 0.005427\n",
      "Train Epoch: 0 [81600/210000 (39%)]\tLoss: 0.003029\n",
      "Train Epoch: 0 [82000/210000 (39%)]\tLoss: 0.005326\n",
      "Train Epoch: 0 [82400/210000 (39%)]\tLoss: 0.007663\n",
      "Train Epoch: 0 [82800/210000 (39%)]\tLoss: 0.004321\n",
      "Train Epoch: 0 [83200/210000 (40%)]\tLoss: 0.004995\n",
      "Train Epoch: 0 [83600/210000 (40%)]\tLoss: 0.006037\n",
      "Train Epoch: 0 [84000/210000 (40%)]\tLoss: 0.003819\n",
      "Train Epoch: 0 [84400/210000 (40%)]\tLoss: 0.004783\n",
      "Train Epoch: 0 [84800/210000 (40%)]\tLoss: 0.007801\n",
      "Train Epoch: 0 [85200/210000 (41%)]\tLoss: 0.007425\n",
      "Train Epoch: 0 [85600/210000 (41%)]\tLoss: 0.004920\n",
      "Train Epoch: 0 [86000/210000 (41%)]\tLoss: 0.004591\n",
      "Train Epoch: 0 [86400/210000 (41%)]\tLoss: 0.005206\n",
      "Train Epoch: 0 [86800/210000 (41%)]\tLoss: 0.005448\n",
      "Train Epoch: 0 [87200/210000 (42%)]\tLoss: 0.005416\n",
      "Train Epoch: 0 [87600/210000 (42%)]\tLoss: 0.003701\n",
      "Train Epoch: 0 [88000/210000 (42%)]\tLoss: 0.005236\n",
      "Train Epoch: 0 [88400/210000 (42%)]\tLoss: 0.004861\n",
      "Train Epoch: 0 [88800/210000 (42%)]\tLoss: 0.004492\n",
      "Train Epoch: 0 [89200/210000 (42%)]\tLoss: 0.003518\n",
      "Train Epoch: 0 [89600/210000 (43%)]\tLoss: 0.002957\n",
      "Train Epoch: 0 [90000/210000 (43%)]\tLoss: 0.006244\n",
      "Train Epoch: 0 [90400/210000 (43%)]\tLoss: 0.005276\n",
      "Train Epoch: 0 [90800/210000 (43%)]\tLoss: 0.004738\n",
      "Train Epoch: 0 [91200/210000 (43%)]\tLoss: 0.003743\n",
      "Train Epoch: 0 [91600/210000 (44%)]\tLoss: 0.003970\n",
      "Train Epoch: 0 [92000/210000 (44%)]\tLoss: 0.004996\n",
      "Train Epoch: 0 [92400/210000 (44%)]\tLoss: 0.006208\n",
      "Train Epoch: 0 [92800/210000 (44%)]\tLoss: 0.006654\n",
      "Train Epoch: 0 [93200/210000 (44%)]\tLoss: 0.004353\n",
      "Train Epoch: 0 [93600/210000 (45%)]\tLoss: 0.004473\n",
      "Train Epoch: 0 [94000/210000 (45%)]\tLoss: 0.004903\n",
      "Train Epoch: 0 [94400/210000 (45%)]\tLoss: 0.004053\n",
      "Train Epoch: 0 [94800/210000 (45%)]\tLoss: 0.005134\n",
      "Train Epoch: 0 [95200/210000 (45%)]\tLoss: 0.005652\n",
      "Train Epoch: 0 [95600/210000 (46%)]\tLoss: 0.004901\n",
      "Train Epoch: 0 [96000/210000 (46%)]\tLoss: 0.003329\n",
      "Train Epoch: 0 [96400/210000 (46%)]\tLoss: 0.004229\n",
      "Train Epoch: 0 [96800/210000 (46%)]\tLoss: 0.004475\n",
      "Train Epoch: 0 [97200/210000 (46%)]\tLoss: 0.004057\n",
      "Train Epoch: 0 [97600/210000 (46%)]\tLoss: 0.004250\n",
      "Train Epoch: 0 [98000/210000 (47%)]\tLoss: 0.004685\n",
      "Train Epoch: 0 [98400/210000 (47%)]\tLoss: 0.006922\n",
      "Train Epoch: 0 [98800/210000 (47%)]\tLoss: 0.003856\n",
      "Train Epoch: 0 [99200/210000 (47%)]\tLoss: 0.005048\n",
      "Train Epoch: 0 [99600/210000 (47%)]\tLoss: 0.005011\n",
      "Train Epoch: 0 [100000/210000 (48%)]\tLoss: 0.004549\n",
      "Train Epoch: 0 [100400/210000 (48%)]\tLoss: 0.004025\n",
      "Train Epoch: 0 [100800/210000 (48%)]\tLoss: 0.003232\n",
      "Train Epoch: 0 [101200/210000 (48%)]\tLoss: 0.004861\n",
      "Train Epoch: 0 [101600/210000 (48%)]\tLoss: 0.006953\n",
      "Train Epoch: 0 [102000/210000 (49%)]\tLoss: 0.004047\n",
      "Train Epoch: 0 [102400/210000 (49%)]\tLoss: 0.003903\n",
      "Train Epoch: 0 [102800/210000 (49%)]\tLoss: 0.005024\n",
      "Train Epoch: 0 [103200/210000 (49%)]\tLoss: 0.004992\n",
      "Train Epoch: 0 [103600/210000 (49%)]\tLoss: 0.004147\n",
      "Train Epoch: 0 [104000/210000 (50%)]\tLoss: 0.004277\n",
      "Train Epoch: 0 [104400/210000 (50%)]\tLoss: 0.003180\n",
      "Train Epoch: 0 [104800/210000 (50%)]\tLoss: 0.005001\n",
      "Train Epoch: 0 [105200/210000 (50%)]\tLoss: 0.005157\n",
      "Train Epoch: 0 [105600/210000 (50%)]\tLoss: 0.010096\n",
      "Train Epoch: 0 [106000/210000 (50%)]\tLoss: 0.005742\n",
      "Train Epoch: 0 [106400/210000 (51%)]\tLoss: 0.006290\n",
      "Train Epoch: 0 [106800/210000 (51%)]\tLoss: 0.004152\n",
      "Train Epoch: 0 [107200/210000 (51%)]\tLoss: 0.007288\n",
      "Train Epoch: 0 [107600/210000 (51%)]\tLoss: 0.007032\n",
      "Train Epoch: 0 [108000/210000 (51%)]\tLoss: 0.003048\n",
      "Train Epoch: 0 [108400/210000 (52%)]\tLoss: 0.004864\n",
      "Train Epoch: 0 [108800/210000 (52%)]\tLoss: 0.005135\n",
      "Train Epoch: 0 [109200/210000 (52%)]\tLoss: 0.004638\n",
      "Train Epoch: 0 [109600/210000 (52%)]\tLoss: 0.003875\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-af950977bd11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mlog_interval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_mse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#     vis.line(X=torch.ones((1,1)).cpu()*epoch,Y=torch.Tensor([epoch_loss]).unsqueeze(0).cpu(),win=loss_window,update='append',name='loss')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#         vis.line(X=torch.ones((1,1)).cpu()*epoch,Y=torch.Tensor([epoch_mse]).unsqueeze(0).cpu(),win=loss_window,update='append',name='mse_loss')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-4e34e00d4312>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, writer)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0madap_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdaptiveAvgPool3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mstartgoal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"start_goal\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mstartgoal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/VAE-Motion-Planning/data/MinicityDataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mrowid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataperow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mdataid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataperow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"select id, startgoal, occ, data, egoid from minicity where id = \"\u001b[0m \u001b[0;34m+\u001b[0m  \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrowid\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetchone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mstart_goal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/VAE-Motion-Planning/data/MinicityDataset.py\u001b[0m in \u001b[0;36mconvert_array\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msqlite3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBinary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mconvert_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epoch, epoch + 200):\n",
    "    log_interval = 20\n",
    "    epoch, epoch_loss, epoch_mse = train(epoch, writer)\n",
    "#     vis.line(X=torch.ones((1,1)).cpu()*epoch,Y=torch.Tensor([epoch_loss]).unsqueeze(0).cpu(),win=loss_window,update='append',name='loss')\n",
    "#         vis.line(X=torch.ones((1,1)).cpu()*epoch,Y=torch.Tensor([epoch_mse]).unsqueeze(0).cpu(),win=loss_window,update='append',name='mse_loss')\n",
    "    writer.add_scalar('Loss/loss', epoch_loss, epoch)\n",
    "    writer.add_scalar('Loss/mse_loss', epoch_mse, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict()\n",
    "            }, 'checkpoints/minicity_attention_no_goal_2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda:1\n",
      "Let's use 2 GPUs!\n",
      "DataParallel(\n",
      "  (module): convVAE(\n",
      "    (condnn): CondNN(\n",
      "      (cnn): Conv3d(\n",
      "        (adap_pool): AdaptiveAvgPool3d(output_size=(25, 200, 200))\n",
      "        (conv_layer1): Sequential(\n",
      "          (0): Conv3d(1, 16, kernel_size=(2, 3, 3), stride=(1, 1, 1))\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Conv3d(16, 16, kernel_size=(2, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          (3): LeakyReLU(negative_slope=0.01)\n",
      "          (4): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "        )\n",
      "        (conv_layer2): Sequential(\n",
      "          (0): Conv3d(16, 32, kernel_size=(2, 3, 3), stride=(1, 1, 1))\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Conv3d(32, 32, kernel_size=(2, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          (3): LeakyReLU(negative_slope=0.01)\n",
      "          (4): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "        )\n",
      "        (conv_layer5): Conv3d(32, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1))\n",
      "        (adap_pool2): AdaptiveAvgPool3d(output_size=(4, 10, 10))\n",
      "      )\n",
      "      (Attention): Attention(\n",
      "        (encoder_att): Linear(in_features=67, out_features=64, bias=True)\n",
      "        (condition_att): Linear(in_features=8, out_features=64, bias=True)\n",
      "        (full_att): Linear(in_features=64, out_features=1, bias=True)\n",
      "        (relu): ReLU()\n",
      "        (softmax): Softmax(dim=1)\n",
      "      )\n",
      "      (fc1): Linear(in_features=172, out_features=300, bias=True)\n",
      "    )\n",
      "    (encoder): Encoder(\n",
      "      (sequential): Sequential(\n",
      "        (0): Linear(in_features=304, out_features=512, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=512, out_features=1024, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        (5): ReLU()\n",
      "      )\n",
      "      (linear_means): Linear(in_features=512, out_features=50, bias=True)\n",
      "      (linear_log_var): Linear(in_features=512, out_features=50, bias=True)\n",
      "    )\n",
      "    (decoder): Decoder(\n",
      "      (sequential): Sequential(\n",
      "        (0): Linear(in_features=350, out_features=512, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=512, out_features=1024, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        (5): ReLU()\n",
      "        (6): Linear(in_features=512, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device\", device)\n",
    "\n",
    "model = convVAE(sample_size = X_dim, \n",
    "                  traj_size = traj_size,\n",
    "                  cnnout_size = cnn_out_size, \n",
    "                  cond_out_size = cond_out_size, \n",
    "                  encoder_layer_sizes = [512,1024,512], \n",
    "                  latent_size = z_dim, \n",
    "                  decoder_layer_sizes = [512,1024,512]).to(device)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "  print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "  # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "  model = nn.DataParallel(model, device_ids=[1, 0])\n",
    "\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "checkpoint = torch.load('checkpoints/minicity_attention_no_goal_2.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "\n",
    "model.eval()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8753\n",
      "torch.Size([25, 4])\n",
      "Using matplotlib backend: Qt5Agg\n",
      "[ 0.      0.     -1.4532  9.86    0.      0.     -0.3976 11.98  ]\n",
      "100.0 100.0\n"
     ]
    }
   ],
   "source": [
    "test_data = test_loader.dataset\n",
    "viz_idx =   torch.randint(0,len(test_data),[1]).item()  \n",
    "#  变道场景idx\n",
    "viz_idx = 8753 #弯道\n",
    "# viz_idx = 143300 #弯道\n",
    "# viz_idx = 5107 #直行\n",
    "# viz_idx = 2984 #转盘\n",
    "# viz_idx = 101538 #转盘\n",
    "# viz_idx = 153309 #阻塞\n",
    "# viz_idx = 109779 #变道\n",
    "\n",
    "print(viz_idx)\n",
    "\n",
    "batch = test_data[viz_idx]\n",
    "startgoal = torch.from_numpy(batch[\"start_goal\"]).to(device)\n",
    "startgoal[4] = 0\n",
    "startgoal[5] = 0\n",
    "occ = torch.from_numpy(batch[\"observation\"])\n",
    "occ = occ[:, 200:600, 200:600]        \n",
    "occ = occ.unsqueeze(0)\n",
    "occ = occ.unsqueeze(1)\n",
    "adap_pool = nn.AdaptiveAvgPool3d((25,200, 200))\n",
    "occ = adap_pool(occ)\n",
    "occ = occ.to(device)\n",
    "traj = torch.from_numpy(batch[\"traj\"]).to(device)\n",
    "print(traj.shape)\n",
    "data = torch.from_numpy(batch[\"data\"]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    y_viz = torch.randn(1,4).to(device)\n",
    "    for i in range(0, 20):\n",
    "        num_viz = 10\n",
    "        y_viz_p, alpha = model.inference(startgoal.expand(num_viz, X_dim * 2).to(device), traj.expand(num_viz, 25, 4),\n",
    "                                occ.expand(num_viz, 1, -1, -1, -1).to(device), num_viz)\n",
    "        torch.cuda.empty_cache()\n",
    "        y_viz = torch.cat((y_viz_p, y_viz), dim = 0)\n",
    "\n",
    "y_viz=y_viz.cpu().detach().numpy()*50\n",
    "occ=occ.cpu().detach().numpy()\n",
    "startgoal=startgoal.cpu().detach().numpy() * 50\n",
    "data=data.cpu().detach().numpy() * 50\n",
    "alpha=alpha.cpu().detach().numpy()\n",
    "torch.cuda.empty_cache()\n",
    "# from utils.NarrowPassage import plotCondition, plotSample, plotSpeed, plotSampleAttention\n",
    "\n",
    "%matplotlib\n",
    "# from utils.HighWay import plotData, plotOrientSpeed, plotAlpha\n",
    "from utils.Minicity import plotData, plotOrientSpeed, plotAlpha\n",
    "\n",
    "y_viz=y_viz[:-1]\n",
    "plotData(occ, startgoal, y_viz)\n",
    "plotOrientSpeed(startgoal, y_viz)\n",
    "# plotAlpha(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotAlpha(alpha):\n",
    "    fig = plt.figure()\n",
    "    alpha = alpha[0,:]\n",
    "    alpha=alpha.reshape(4,10,10)\n",
    "    \n",
    "    myobj = plt.imshow(alpha[0, :, :])\n",
    "\n",
    "    for frame in alpha:\n",
    "        myobj.set_data(frame)\n",
    "        plt.draw()\n",
    "        plt.pause(0.3)\n",
    "plotAlpha(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "startx = 100\n",
    "starty = 100\n",
    "\n",
    "def position2imagep(data, startgoal):\n",
    "    datax = (data[0])*(10/3) + 100\n",
    "    datay = 100-(data[1])*(10/3) \n",
    "    return datax, datay\n",
    "\n",
    "def plotCondition(occ, startgoal):\n",
    "    fig = plt.figure()\n",
    "    occ=occ.squeeze(0)\n",
    "    occ=occ.squeeze(0)\n",
    "    myobj = plt.imshow(occ[0, :, :])\n",
    "    goalx, goaly = position2imagep(startgoal[4:], startgoal)\n",
    "    plt.scatter(startx, starty)\n",
    "    plt.scatter(goalx, goaly)\n",
    "    for frame in occ:\n",
    "        myobj.set_data(frame)\n",
    "        plt.draw()\n",
    "        plt.pause(0.1)\n",
    "\n",
    "def plotData(occ, startgoal,data):\n",
    "    fig = plt.figure()\n",
    "    occ=occ.squeeze(0)\n",
    "    occ=occ.squeeze(0)\n",
    "    myobj = plt.imshow(occ[0, :, :])\n",
    "    goalx, goaly = position2imagep(startgoal[4:], startgoal)\n",
    "    print(startgoal)\n",
    "    print(goalx, goaly)\n",
    "    plt.scatter(startx, starty)\n",
    "    plt.scatter(goalx, goaly)\n",
    "    if len(data.shape) > 1:\n",
    "        for row in data:\n",
    "            datax, datay = position2imagep(row, startgoal)\n",
    "            plt.scatter(datax, datay, marker='*', c='#d62728')\n",
    "    else:\n",
    "        datax, datay = position2imagep(data, startgoal)\n",
    "        plt.scatter(datax, datay, marker='*', c='#d62728')\n",
    "    for frame in occ:\n",
    "        myobj.set_data(frame)\n",
    "        plt.draw()\n",
    "        plt.pause(0.1)\n",
    "\n",
    "def plotOrientSpeed(startgoal, data):\n",
    "    fig = plt.figure()\n",
    "    goalx, goaly = position2imagep(startgoal[4:], startgoal)\n",
    "    plt.scatter(data[:, 2], data[:,3])\n",
    "    plt.scatter(startgoal[2], startgoal[3])\n",
    "    plt.scatter(startgoal[6], startgoal[7])\n",
    "    \n",
    "def plotAlpha(alpha):\n",
    "    fig = plt.figure()\n",
    "    alpha = alpha[0,:]\n",
    "    alpha=alpha.reshape(4,10,10)\n",
    "    \n",
    "    myobj = plt.imshow(alpha[0, :, :])\n",
    "\n",
    "    for frame in alpha:\n",
    "        myobj.set_data(frame)\n",
    "        plt.draw()\n",
    "        plt.pause(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "445954\n"
     ]
    }
   ],
   "source": [
    "test_data = test_loader.dataset\n",
    "viz_idx =   torch.randint(0,len(test_data),[1]).item()  \n",
    "#  变道场景idx\n",
    "#  \n",
    "print(viz_idx)\n",
    "\n",
    "batch = test_data[viz_idx]\n",
    "startgoal = torch.from_numpy(batch[\"start_goal\"]).to(device)\n",
    "occ = torch.from_numpy(batch[\"observation\"])\n",
    "occ = occ.unsqueeze(0)\n",
    "occ = occ.unsqueeze(1)\n",
    "# adap_pool = nn.AdaptiveAvgPool3d((25,100, 600))\n",
    "# occ = adap_pool(occ)\n",
    "occ = occ.to(device)\n",
    "data = torch.from_numpy(batch[\"data\"]).to(device)\n",
    "\n",
    "occ=occ.cpu().detach().numpy()\n",
    "startgoal=startgoal.cpu().detach().numpy()\n",
    "data=data.cpu().detach().numpy()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "plotData(occ, startgoal, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 25, 300, 1853)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "occ.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
