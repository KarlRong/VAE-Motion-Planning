{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda:0\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import sqlite3\n",
    "import torch\n",
    "import io\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device\", device)\n",
    "\n",
    "def adapt_array(arr):\n",
    "    out = io.BytesIO()\n",
    "    np.save(out, arr)\n",
    "    out.seek(0)\n",
    "    return sqlite3.Binary(out.read())\n",
    "\n",
    "def convert_array(text):\n",
    "    out = io.BytesIO(text)\n",
    "    out.seek(0)\n",
    "    return np.load(out)\n",
    "\n",
    "def position_reformed(data, startgoal):\n",
    "    datax = (data[0]-startgoal[0])*(3/2) + startx\n",
    "    datay = (data[1]-startgoal[1])*(3/2) + starty\n",
    "    return datax, datay\n",
    "\n",
    "# Converts np.array to TEXT when inserting\n",
    "sqlite3.register_adapter(np.ndarray, adapt_array)\n",
    "# Converts TEXT to np.array when selecting\n",
    "sqlite3.register_converter(\"array\", convert_array)\n",
    "\n",
    "class TrafficDataset(Dataset):\n",
    "    def __init__(self, dbpath, train=True, ratio_test=0.8, num_sce=100, num_data=25):\n",
    "        self.con = sqlite3.connect(dbpath, detect_types=sqlite3.PARSE_DECLTYPES)\n",
    "        self.path = dbpath\n",
    "        self.cur = self.con.cursor()\n",
    "        self.cur.execute(\"select id from highway\")\n",
    "        self.idlist = self.cur.fetchall()\n",
    "        self.cur.execute(\"select data from highway where id = \" +  str(1))\n",
    "        self.dataperow = len(self.cur.fetchone()[0])\n",
    "        numTrain = int(ratio_test * len(self.idlist))\n",
    "        if (train):\n",
    "            self.filelist = self.idlist[:numTrain]\n",
    "        else:\n",
    "            self.filelist = self.idlist[numTrain:]\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.idlist) * self.dataperow\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        rowid = idx // self.dataperow\n",
    "        dataid = idx % self.dataperow\n",
    "        self.cur.execute(\"select id, startgoal, occ, data from highway where id = \" +  str(rowid+1))\n",
    "        results = self.cur.fetchone()\n",
    "        start_goal = results[1].astype(np.single)\n",
    "        observed = results[2].astype(np.single)\n",
    "        data = results[3][dataid].astype(np.single)\n",
    "        \n",
    "        start_goal[4] = start_goal[4] - start_goal[0]\n",
    "        start_goal[5] = start_goal[5] - start_goal[1]\n",
    "        \n",
    "        data[0] = data[0] - start_goal[0]\n",
    "        data[1] = data[1] - start_goal[1]\n",
    "        \n",
    "        start_goal[0] = 0\n",
    "        start_goal[1] = 0\n",
    "        \n",
    "         \n",
    "        sample = {'start_goal': start_goal,\n",
    "                             'observation': observed,\n",
    "                            'data': data}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "dbpath = '/home/rong/disk/database/highway.db'\n",
    "bs = 8\n",
    "train_loader = DataLoader(TrafficDataset(dbpath = dbpath,\n",
    "                            train = True),\n",
    "                         batch_size = bs, shuffle=True, drop_last = True)\n",
    "test_loader = DataLoader(TrafficDataset(dbpath = dbpath,\n",
    "                            train = False),\n",
    "                          batch_size = bs, shuffle=True, drop_last = True)\n",
    "\n",
    "# batch = next(iter(train_loader))\n",
    "# batch['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "\n",
    "class convVAE(nn.Module):\n",
    "    def __init__(self, sample_size, cnnout_size, cond_out_size, encoder_layer_sizes, latent_size, decoder_layer_sizes):\n",
    "        super(convVAE, self).__init__()\n",
    "\n",
    "        assert type(encoder_layer_sizes) == list\n",
    "        assert type(latent_size) == int\n",
    "        assert type(decoder_layer_sizes) == list\n",
    "        \n",
    "        self.latent_size = latent_size\n",
    "        self.condnn = CondNN(sample_size, cnnout_size, cond_out_size)\n",
    "        self.encoder = Encoder(sample_size + cond_out_size, encoder_layer_sizes, latent_size)\n",
    "        self.decoder = Decoder(latent_size +cond_out_size, decoder_layer_sizes, sample_size)\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def decode(self, x):\n",
    "        return self.decoder(x)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x, startend, occ):\n",
    "        c = self.condnn(startend, occ)\n",
    "        mu, logvar = self.encode(torch.cat((x, c), dim=-1))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(torch.cat((z, c), dim=-1)), mu, logvar\n",
    "    \n",
    "    def inference(self, startend, occ, num_viz):\n",
    "        c = self.condnn(startend, occ)\n",
    "        z = torch.randn(num_viz, self.latent_size, device = c.device)\n",
    "        return self.decode(torch.cat((z, c), dim=-1))\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, layer_sizes, latent_size):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        layer_sizes = [input_size] + layer_sizes\n",
    "        modules = []\n",
    "        for i, (in_size, out_size) in enumerate(zip(layer_sizes[:-1], layer_sizes[1:])):\n",
    "            modules.append(nn.Linear(in_size, out_size))\n",
    "            modules.append(nn.ReLU())\n",
    "#             modules.append(nn.Dropout(p=0.5))\n",
    "\n",
    "        self.sequential = nn.Sequential(*modules)\n",
    "        self.linear_means = nn.Linear(layer_sizes[-1], latent_size)\n",
    "        self.linear_log_var = nn.Linear(layer_sizes[-1], latent_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sequential(x)\n",
    "        means = self.linear_means(x)\n",
    "        log_vars = self.linear_log_var(x)\n",
    "        return means, log_vars\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, layer_sizes, sample_size):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        layer_sizes = [input_size] + layer_sizes\n",
    "        modules = []\n",
    "        for i, (in_size, out_size) in enumerate(zip(layer_sizes[:-1], layer_sizes[1:])):\n",
    "            modules.append(nn.Linear(in_size, out_size))\n",
    "            modules.append(nn.ReLU())\n",
    "#             modules.append(nn.Dropout(p=0.5))\n",
    "        modules.append(nn.Linear(layer_sizes[-1], sample_size))\n",
    "\n",
    "        self.sequential = nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sequential(x)\n",
    "\n",
    "\n",
    "class CondNN(nn.Module):\n",
    "    def __init__(self, sampleSize,  cnn_out_size, outSize):\n",
    "        super(CondNN, self).__init__()\n",
    "        self.sampleSize = sampleSize\n",
    "        self.cnn = Conv3d(cnn_out_size)\n",
    "        self.fc2 = nn.Linear(cnn_out_size + sampleSize * 2, outSize)\n",
    "\n",
    "    def forward(self, startend, occ):\n",
    "        occ = self.cnn(occ)\n",
    "        x = torch.cat((occ, startend), dim=-1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "class Conv3d(nn.Module):\n",
    "    def __init__(self, cnn_out_size):\n",
    "        super(Conv3d, self).__init__()\n",
    "\n",
    "        self.adap_pool = nn.AdaptiveAvgPool3d((25, 100, 600))\n",
    "        self.conv_layer1 = self._make_conv_layer(1, 16)\n",
    "        self.conv_layer2 = self._make_conv_layer(16, 32)\n",
    "#         self.conv_layer3 = self._make_conv_layer(64, 124)\n",
    "#         self.conv_layer4 = self._make_conv_layer(124, 256)\n",
    "        self.conv_layer5=nn.Conv3d(32, 64, kernel_size=(1, 3, 3), padding=0)\n",
    "        \n",
    "        self.adap_pool2 = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "        self.fc5 = nn.Linear(64, 64)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.batch0=nn.BatchNorm1d(64)\n",
    "        self.drop=nn.Dropout(p=0.15)        \n",
    "        self.fc6 = nn.Linear(64, 64)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.batch1=nn.BatchNorm1d(64)\n",
    "        \n",
    "        self.drop=nn.Dropout(p=0.15)\n",
    "        self.fc7 = nn.Linear(64, cnn_out_size)\n",
    "\n",
    "    def _make_conv_layer(self, in_c, out_c):\n",
    "        conv_layer = nn.Sequential(\n",
    "        nn.Conv3d(in_c, out_c, kernel_size=(2, 3, 3), padding=0),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Conv3d(out_c, out_c, kernel_size=(2, 3, 3), padding=1),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.MaxPool3d((2, 2, 2)),\n",
    "        )\n",
    "        return conv_layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.adap_pool(x)\n",
    "#         print(x.size())\n",
    "        x = self.conv_layer1(x)\n",
    "#         print(x.size())\n",
    "        x = self.conv_layer2(x)\n",
    "#         print(x.size())\n",
    "#         x = self.conv_layer3(x)\n",
    "#         print(x.size())\n",
    "#         x = self.conv_layer4(x)\n",
    "#         print(x.size())\n",
    "        x=self.conv_layer5(x)\n",
    "#         print(x.size())\n",
    "        x = self.adap_pool2(x)\n",
    "#         print(x.size())\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc5(x)\n",
    "#         print(x.size())\n",
    "        \n",
    "        x = self.relu(x)\n",
    "        x = self.batch0(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc6(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batch1(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc7(x)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda:0\n",
      "convVAE(\n",
      "  (condnn): CondNN(\n",
      "    (cnn): Conv3d(\n",
      "      (adap_pool): AdaptiveAvgPool3d(output_size=(25, 100, 600))\n",
      "      (conv_layer1): Sequential(\n",
      "        (0): Conv3d(1, 16, kernel_size=(2, 3, 3), stride=(1, 1, 1))\n",
      "        (1): LeakyReLU(negative_slope=0.01)\n",
      "        (2): Conv3d(16, 16, kernel_size=(2, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (3): LeakyReLU(negative_slope=0.01)\n",
      "        (4): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (conv_layer2): Sequential(\n",
      "        (0): Conv3d(16, 32, kernel_size=(2, 3, 3), stride=(1, 1, 1))\n",
      "        (1): LeakyReLU(negative_slope=0.01)\n",
      "        (2): Conv3d(32, 32, kernel_size=(2, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (3): LeakyReLU(negative_slope=0.01)\n",
      "        (4): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (conv_layer5): Conv3d(32, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1))\n",
      "      (adap_pool2): AdaptiveAvgPool3d(output_size=(1, 1, 1))\n",
      "      (fc5): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (relu): LeakyReLU(negative_slope=0.01)\n",
      "      (batch0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (drop): Dropout(p=0.15, inplace=False)\n",
      "      (fc6): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (batch1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (fc7): Linear(in_features=64, out_features=300, bias=True)\n",
      "    )\n",
      "    (fc2): Linear(in_features=308, out_features=300, bias=True)\n",
      "  )\n",
      "  (encoder): Encoder(\n",
      "    (sequential): Sequential(\n",
      "      (0): Linear(in_features=304, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=1024, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "    (linear_means): Linear(in_features=512, out_features=50, bias=True)\n",
      "    (linear_log_var): Linear(in_features=512, out_features=50, bias=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (sequential): Sequential(\n",
      "      (0): Linear(in_features=350, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=1024, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (5): ReLU()\n",
      "      (6): Linear(in_features=512, out_features=4, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "X_dim = 4\n",
    "z_dim = 50\n",
    "cnn_out_size = 300\n",
    "cond_out_size = 300\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device\", device)\n",
    "\n",
    "model = convVAE(sample_size = X_dim, \n",
    "                  cnnout_size = cnn_out_size, \n",
    "                  cond_out_size = cond_out_size, \n",
    "                  encoder_layer_sizes = [512,1024,512], \n",
    "                  latent_size = z_dim, \n",
    "                  decoder_layer_sizes = [512,1024,512]).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(recon_x, x, w, mean, log_var):\n",
    "    MSE = torch.mean((w.expand_as(x) * (recon_x-x)**2))\n",
    "    KLD = - 0.002 * torch.mean(torch.sum(1 + log_var - mean.pow(2) - log_var.exp(), 1))\n",
    "    return MSE + KLD, MSE\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, writer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    mse_loss = 0\n",
    "    w = torch.tensor([0.5, 10, 1, 3], dtype=torch.float).to(device)\n",
    "    adap_pool = nn.AdaptiveAvgPool3d((25,100, 600))\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        startgoal = batch[\"start_goal\"].to(device)\n",
    "        occ = batch[\"observation\"]\n",
    "        occ = adap_pool(occ)\n",
    "        occ = occ.to(device)\n",
    "        occ = occ.unsqueeze(1)\n",
    "        data = batch[\"data\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data, startgoal, occ)\n",
    "        loss, mse= loss_fn(recon_batch, data, w, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        mse_loss += mse.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item()))\n",
    "        \n",
    "            writer.add_scalar('BatchLoss/loss', loss.item(), batch_idx)\n",
    "            writer.add_scalar('BatchLoss/mse_loss', mse.item(), batch_idx)\n",
    "\n",
    "    epoch_loss = train_loss * len(data) / len(train_loader.dataset)\n",
    "    epoch_mse = mse_loss * len(data) / len(train_loader.dataset)\n",
    "    print('====> Epoch: {} Average loss: {:.7f}'.format(\n",
    "          epoch, epoch_loss))\n",
    "    return epoch, epoch_loss, epoch_mse\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    mse_loss = 0\n",
    "    w = torch.tensor([1, 1, 1, 0.5], dtype=torch.float).to(device)\n",
    "    for batch_idx, batch in enumerate(test_loader):\n",
    "        startgoal = batch[\"start_goal\"].to(device)\n",
    "        occ = batch[\"observation\"].to(device)\n",
    "        occ = occ.unsqueeze(1)\n",
    "        data = batch[\"data\"].to(device)\n",
    "        \n",
    "        recon_batch, mu, logvar = model(sample, startend, occ)\n",
    "        loss, mse= loss_fn(recon_batch, data, w, mu, logvar)\n",
    "        test_loss += loss.item()\n",
    "        mse_loss += mse.item()\n",
    "\n",
    "    epoch_loss = test_loss * len(data) / len(test_loader.dataset)\n",
    "    epoch_mse = mse_loss * len(data) / len(test_loader.dataset)\n",
    "    print('====> Epoch: {} Average test loss: {:.7f}'.format(\n",
    "          epoch, epoch_loss))\n",
    "    return epoch, epoch_loss, epoch_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Writer will output to ./runs/ directory by default\n",
    "writer = SummaryWriter('runs/highway_conv_04')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/482500 (0%)]\tLoss: 1114.817627\n",
      "Train Epoch: 0 [160/482500 (0%)]\tLoss: 311.142609\n",
      "Train Epoch: 0 [320/482500 (0%)]\tLoss: 134.652359\n",
      "Train Epoch: 0 [480/482500 (0%)]\tLoss: 50.779686\n",
      "Train Epoch: 0 [640/482500 (0%)]\tLoss: 31.466805\n",
      "Train Epoch: 0 [800/482500 (0%)]\tLoss: 12.722262\n",
      "Train Epoch: 0 [960/482500 (0%)]\tLoss: 6.812252\n",
      "Train Epoch: 0 [1120/482500 (0%)]\tLoss: 4.641794\n",
      "Train Epoch: 0 [1280/482500 (0%)]\tLoss: 5.066861\n",
      "Train Epoch: 0 [1440/482500 (0%)]\tLoss: 3.409468\n",
      "Train Epoch: 0 [1600/482500 (0%)]\tLoss: 27.017250\n",
      "Train Epoch: 0 [1760/482500 (0%)]\tLoss: 15.787836\n",
      "Train Epoch: 0 [1920/482500 (0%)]\tLoss: 6.669167\n",
      "Train Epoch: 0 [2080/482500 (0%)]\tLoss: 6.838213\n",
      "Train Epoch: 0 [2240/482500 (0%)]\tLoss: 10.424520\n",
      "Train Epoch: 0 [2400/482500 (0%)]\tLoss: 7.137385\n",
      "Train Epoch: 0 [2560/482500 (1%)]\tLoss: 2.311974\n",
      "Train Epoch: 0 [2720/482500 (1%)]\tLoss: 1.801174\n",
      "Train Epoch: 0 [2880/482500 (1%)]\tLoss: 2.976290\n",
      "Train Epoch: 0 [3040/482500 (1%)]\tLoss: 1.989319\n",
      "Train Epoch: 0 [3200/482500 (1%)]\tLoss: 634.327332\n",
      "Train Epoch: 0 [3360/482500 (1%)]\tLoss: 25.538593\n",
      "Train Epoch: 0 [3520/482500 (1%)]\tLoss: 14.728304\n",
      "Train Epoch: 0 [3680/482500 (1%)]\tLoss: 30.375124\n",
      "Train Epoch: 0 [3840/482500 (1%)]\tLoss: 2.331496\n",
      "Train Epoch: 0 [4000/482500 (1%)]\tLoss: 2.968918\n",
      "Train Epoch: 0 [4160/482500 (1%)]\tLoss: 2.819517\n",
      "Train Epoch: 0 [4320/482500 (1%)]\tLoss: 0.695030\n",
      "Train Epoch: 0 [4480/482500 (1%)]\tLoss: 1.995182\n",
      "Train Epoch: 0 [4640/482500 (1%)]\tLoss: 1.813687\n",
      "Train Epoch: 0 [4800/482500 (1%)]\tLoss: 2.530740\n",
      "Train Epoch: 0 [4960/482500 (1%)]\tLoss: 0.949626\n",
      "Train Epoch: 0 [5120/482500 (1%)]\tLoss: 3.384704\n",
      "Train Epoch: 0 [5280/482500 (1%)]\tLoss: 1.250238\n",
      "Train Epoch: 0 [5440/482500 (1%)]\tLoss: 5.483534\n",
      "Train Epoch: 0 [5600/482500 (1%)]\tLoss: 6.079526\n",
      "Train Epoch: 0 [5760/482500 (1%)]\tLoss: 1.772459\n",
      "Train Epoch: 0 [5920/482500 (1%)]\tLoss: 378.701782\n",
      "Train Epoch: 0 [6080/482500 (1%)]\tLoss: 11.014563\n",
      "Train Epoch: 0 [6240/482500 (1%)]\tLoss: 4.157414\n",
      "Train Epoch: 0 [6400/482500 (1%)]\tLoss: 2.916197\n",
      "Train Epoch: 0 [6560/482500 (1%)]\tLoss: 3.439205\n",
      "Train Epoch: 0 [6720/482500 (1%)]\tLoss: 9.774199\n",
      "Train Epoch: 0 [6880/482500 (1%)]\tLoss: 4.329425\n",
      "Train Epoch: 0 [7040/482500 (1%)]\tLoss: 1.649795\n",
      "Train Epoch: 0 [7200/482500 (1%)]\tLoss: 1.415945\n",
      "Train Epoch: 0 [7360/482500 (2%)]\tLoss: 1.421024\n",
      "Train Epoch: 0 [7520/482500 (2%)]\tLoss: 0.852806\n",
      "Train Epoch: 0 [7680/482500 (2%)]\tLoss: 179.449295\n",
      "Train Epoch: 0 [7840/482500 (2%)]\tLoss: 24.404203\n",
      "Train Epoch: 0 [8000/482500 (2%)]\tLoss: 4.498215\n",
      "Train Epoch: 0 [8160/482500 (2%)]\tLoss: 2.869371\n",
      "Train Epoch: 0 [8320/482500 (2%)]\tLoss: 2.926791\n",
      "Train Epoch: 0 [8480/482500 (2%)]\tLoss: 6.774524\n",
      "Train Epoch: 0 [8640/482500 (2%)]\tLoss: 1.598590\n",
      "Train Epoch: 0 [8800/482500 (2%)]\tLoss: 22.906269\n",
      "Train Epoch: 0 [8960/482500 (2%)]\tLoss: 2.812881\n",
      "Train Epoch: 0 [9120/482500 (2%)]\tLoss: 1.883166\n",
      "Train Epoch: 0 [9280/482500 (2%)]\tLoss: 3.338856\n",
      "Train Epoch: 0 [9440/482500 (2%)]\tLoss: 2.285777\n",
      "Train Epoch: 0 [9600/482500 (2%)]\tLoss: 1.489180\n",
      "Train Epoch: 0 [9760/482500 (2%)]\tLoss: 1.094655\n",
      "Train Epoch: 0 [9920/482500 (2%)]\tLoss: 1.963739\n",
      "Train Epoch: 0 [10080/482500 (2%)]\tLoss: 0.878975\n",
      "Train Epoch: 0 [10240/482500 (2%)]\tLoss: 1.190351\n",
      "Train Epoch: 0 [10400/482500 (2%)]\tLoss: 2.298422\n",
      "Train Epoch: 0 [10560/482500 (2%)]\tLoss: 1.512523\n",
      "Train Epoch: 0 [10720/482500 (2%)]\tLoss: 2.294007\n",
      "Train Epoch: 0 [10880/482500 (2%)]\tLoss: 2.502929\n",
      "Train Epoch: 0 [11040/482500 (2%)]\tLoss: 6.686215\n",
      "Train Epoch: 0 [11200/482500 (2%)]\tLoss: 0.818270\n",
      "Train Epoch: 0 [11360/482500 (2%)]\tLoss: 2.571400\n",
      "Train Epoch: 0 [11520/482500 (2%)]\tLoss: 2.052848\n",
      "Train Epoch: 0 [11680/482500 (2%)]\tLoss: 3.050996\n",
      "Train Epoch: 0 [11840/482500 (2%)]\tLoss: 5.475768\n",
      "Train Epoch: 0 [12000/482500 (2%)]\tLoss: 3.180094\n",
      "Train Epoch: 0 [12160/482500 (3%)]\tLoss: 3.980369\n",
      "Train Epoch: 0 [12320/482500 (3%)]\tLoss: 2.331761\n",
      "Train Epoch: 0 [12480/482500 (3%)]\tLoss: 2.241905\n",
      "Train Epoch: 0 [12640/482500 (3%)]\tLoss: 3.564190\n",
      "Train Epoch: 0 [12800/482500 (3%)]\tLoss: 2.524186\n",
      "Train Epoch: 0 [12960/482500 (3%)]\tLoss: 1.789378\n",
      "Train Epoch: 0 [13120/482500 (3%)]\tLoss: 1.917805\n",
      "Train Epoch: 0 [13280/482500 (3%)]\tLoss: 0.953420\n",
      "Train Epoch: 0 [13440/482500 (3%)]\tLoss: 1.037301\n",
      "Train Epoch: 0 [13600/482500 (3%)]\tLoss: 1.439528\n",
      "Train Epoch: 0 [13760/482500 (3%)]\tLoss: 1.385422\n",
      "Train Epoch: 0 [13920/482500 (3%)]\tLoss: 2.393098\n",
      "Train Epoch: 0 [14080/482500 (3%)]\tLoss: 1.637020\n",
      "Train Epoch: 0 [14240/482500 (3%)]\tLoss: 4.970603\n",
      "Train Epoch: 0 [14400/482500 (3%)]\tLoss: 2.374487\n",
      "Train Epoch: 0 [14560/482500 (3%)]\tLoss: 1.413888\n",
      "Train Epoch: 0 [14720/482500 (3%)]\tLoss: 0.588305\n",
      "Train Epoch: 0 [14880/482500 (3%)]\tLoss: 1.681520\n",
      "Train Epoch: 0 [15040/482500 (3%)]\tLoss: 4.468108\n",
      "Train Epoch: 0 [15200/482500 (3%)]\tLoss: 0.887692\n",
      "Train Epoch: 0 [15360/482500 (3%)]\tLoss: 1.012778\n",
      "Train Epoch: 0 [15520/482500 (3%)]\tLoss: 1.222437\n",
      "Train Epoch: 0 [15680/482500 (3%)]\tLoss: 1.818901\n",
      "Train Epoch: 0 [15840/482500 (3%)]\tLoss: 0.751514\n",
      "Train Epoch: 0 [16000/482500 (3%)]\tLoss: 3.160085\n",
      "Train Epoch: 0 [16160/482500 (3%)]\tLoss: 1.039662\n",
      "Train Epoch: 0 [16320/482500 (3%)]\tLoss: 0.932999\n",
      "Train Epoch: 0 [16480/482500 (3%)]\tLoss: 0.743930\n",
      "Train Epoch: 0 [16640/482500 (3%)]\tLoss: 2.694988\n",
      "Train Epoch: 0 [16800/482500 (3%)]\tLoss: 1.309154\n",
      "Train Epoch: 0 [16960/482500 (4%)]\tLoss: 4.468410\n",
      "Train Epoch: 0 [17120/482500 (4%)]\tLoss: 2.581519\n",
      "Train Epoch: 0 [17280/482500 (4%)]\tLoss: 5.242168\n",
      "Train Epoch: 0 [17440/482500 (4%)]\tLoss: 0.576920\n",
      "Train Epoch: 0 [17600/482500 (4%)]\tLoss: 1.718012\n",
      "Train Epoch: 0 [17760/482500 (4%)]\tLoss: 0.755970\n",
      "Train Epoch: 0 [17920/482500 (4%)]\tLoss: 1.064242\n",
      "Train Epoch: 0 [18080/482500 (4%)]\tLoss: 1.220614\n",
      "Train Epoch: 0 [18240/482500 (4%)]\tLoss: 0.586002\n",
      "Train Epoch: 0 [18400/482500 (4%)]\tLoss: 2.199586\n",
      "Train Epoch: 0 [18560/482500 (4%)]\tLoss: 0.554402\n",
      "Train Epoch: 0 [18720/482500 (4%)]\tLoss: 3.966883\n",
      "Train Epoch: 0 [18880/482500 (4%)]\tLoss: 0.900460\n",
      "Train Epoch: 0 [19040/482500 (4%)]\tLoss: 2.165827\n",
      "Train Epoch: 0 [19200/482500 (4%)]\tLoss: 5.223333\n",
      "Train Epoch: 0 [19360/482500 (4%)]\tLoss: 0.491903\n",
      "Train Epoch: 0 [19520/482500 (4%)]\tLoss: 7.508374\n",
      "Train Epoch: 0 [19680/482500 (4%)]\tLoss: 4.402809\n",
      "Train Epoch: 0 [19840/482500 (4%)]\tLoss: 1.736994\n",
      "Train Epoch: 0 [20000/482500 (4%)]\tLoss: 1.299474\n",
      "Train Epoch: 0 [20160/482500 (4%)]\tLoss: 0.876165\n",
      "Train Epoch: 0 [20320/482500 (4%)]\tLoss: 3.982181\n",
      "Train Epoch: 0 [20480/482500 (4%)]\tLoss: 0.882683\n",
      "Train Epoch: 0 [20640/482500 (4%)]\tLoss: 2.637282\n",
      "Train Epoch: 0 [20800/482500 (4%)]\tLoss: 0.981352\n",
      "Train Epoch: 0 [20960/482500 (4%)]\tLoss: 1.059436\n",
      "Train Epoch: 0 [21120/482500 (4%)]\tLoss: 2.420749\n",
      "Train Epoch: 0 [21280/482500 (4%)]\tLoss: 1.263947\n",
      "Train Epoch: 0 [21440/482500 (4%)]\tLoss: 1.667924\n",
      "Train Epoch: 0 [21600/482500 (4%)]\tLoss: 4.091963\n",
      "Train Epoch: 0 [21760/482500 (5%)]\tLoss: 16.175756\n",
      "Train Epoch: 0 [21920/482500 (5%)]\tLoss: 1.212591\n",
      "Train Epoch: 0 [22080/482500 (5%)]\tLoss: 2.936015\n",
      "Train Epoch: 0 [22240/482500 (5%)]\tLoss: 1.031791\n",
      "Train Epoch: 0 [22400/482500 (5%)]\tLoss: 2.813639\n",
      "Train Epoch: 0 [22560/482500 (5%)]\tLoss: 2.105407\n",
      "Train Epoch: 0 [22720/482500 (5%)]\tLoss: 3.252131\n",
      "Train Epoch: 0 [22880/482500 (5%)]\tLoss: 2.391251\n",
      "Train Epoch: 0 [23040/482500 (5%)]\tLoss: 9.197837\n",
      "Train Epoch: 0 [23200/482500 (5%)]\tLoss: 1.741116\n",
      "Train Epoch: 0 [23360/482500 (5%)]\tLoss: 1.175457\n",
      "Train Epoch: 0 [23520/482500 (5%)]\tLoss: 0.871824\n",
      "Train Epoch: 0 [23680/482500 (5%)]\tLoss: 1.640825\n",
      "Train Epoch: 0 [23840/482500 (5%)]\tLoss: 3.640476\n",
      "Train Epoch: 0 [24000/482500 (5%)]\tLoss: 2.423892\n",
      "Train Epoch: 0 [24160/482500 (5%)]\tLoss: 0.579576\n",
      "Train Epoch: 0 [24320/482500 (5%)]\tLoss: 1.584379\n",
      "Train Epoch: 0 [24480/482500 (5%)]\tLoss: 1.350341\n",
      "Train Epoch: 0 [24640/482500 (5%)]\tLoss: 1.282525\n",
      "Train Epoch: 0 [24800/482500 (5%)]\tLoss: 0.999982\n",
      "Train Epoch: 0 [24960/482500 (5%)]\tLoss: 2.131527\n",
      "Train Epoch: 0 [25120/482500 (5%)]\tLoss: 1.032093\n",
      "Train Epoch: 0 [25280/482500 (5%)]\tLoss: 0.910018\n",
      "Train Epoch: 0 [25440/482500 (5%)]\tLoss: 2.609511\n",
      "Train Epoch: 0 [25600/482500 (5%)]\tLoss: 3.162996\n",
      "Train Epoch: 0 [25760/482500 (5%)]\tLoss: 1.382293\n",
      "Train Epoch: 0 [25920/482500 (5%)]\tLoss: 2.237465\n",
      "Train Epoch: 0 [26080/482500 (5%)]\tLoss: 2.831155\n",
      "Train Epoch: 0 [26240/482500 (5%)]\tLoss: 1.262959\n",
      "Train Epoch: 0 [26400/482500 (5%)]\tLoss: 0.906494\n",
      "Train Epoch: 0 [26560/482500 (6%)]\tLoss: 0.865578\n",
      "Train Epoch: 0 [26720/482500 (6%)]\tLoss: 0.645602\n",
      "Train Epoch: 0 [26880/482500 (6%)]\tLoss: 1.807313\n",
      "Train Epoch: 0 [27040/482500 (6%)]\tLoss: 0.554100\n",
      "Train Epoch: 0 [27200/482500 (6%)]\tLoss: 2.260040\n",
      "Train Epoch: 0 [27360/482500 (6%)]\tLoss: 2.076720\n",
      "Train Epoch: 0 [27520/482500 (6%)]\tLoss: 0.662304\n",
      "Train Epoch: 0 [27680/482500 (6%)]\tLoss: 0.827830\n",
      "Train Epoch: 0 [27840/482500 (6%)]\tLoss: 1.669388\n",
      "Train Epoch: 0 [28000/482500 (6%)]\tLoss: 1.041602\n",
      "Train Epoch: 0 [28160/482500 (6%)]\tLoss: 1.622843\n",
      "Train Epoch: 0 [28320/482500 (6%)]\tLoss: 1.620309\n",
      "Train Epoch: 0 [28480/482500 (6%)]\tLoss: 6.146080\n",
      "Train Epoch: 0 [28640/482500 (6%)]\tLoss: 1.077627\n",
      "Train Epoch: 0 [28800/482500 (6%)]\tLoss: 1.272707\n",
      "Train Epoch: 0 [28960/482500 (6%)]\tLoss: 2.244108\n",
      "Train Epoch: 0 [29120/482500 (6%)]\tLoss: 0.705711\n",
      "Train Epoch: 0 [29280/482500 (6%)]\tLoss: 2.206122\n",
      "Train Epoch: 0 [29440/482500 (6%)]\tLoss: 0.933255\n",
      "Train Epoch: 0 [29600/482500 (6%)]\tLoss: 5.718238\n",
      "Train Epoch: 0 [29760/482500 (6%)]\tLoss: 0.836760\n",
      "Train Epoch: 0 [29920/482500 (6%)]\tLoss: 1.327607\n",
      "Train Epoch: 0 [30080/482500 (6%)]\tLoss: 0.637504\n",
      "Train Epoch: 0 [30240/482500 (6%)]\tLoss: 0.660865\n",
      "Train Epoch: 0 [30400/482500 (6%)]\tLoss: 0.845602\n",
      "Train Epoch: 0 [30560/482500 (6%)]\tLoss: 1.754910\n",
      "Train Epoch: 0 [30720/482500 (6%)]\tLoss: 1.574268\n",
      "Train Epoch: 0 [30880/482500 (6%)]\tLoss: 0.968229\n",
      "Train Epoch: 0 [31040/482500 (6%)]\tLoss: 8.338678\n",
      "Train Epoch: 0 [31200/482500 (6%)]\tLoss: 1.003020\n",
      "Train Epoch: 0 [31360/482500 (6%)]\tLoss: 1.829234\n",
      "Train Epoch: 0 [31520/482500 (7%)]\tLoss: 1.144840\n",
      "Train Epoch: 0 [31680/482500 (7%)]\tLoss: 2.325348\n",
      "Train Epoch: 0 [31840/482500 (7%)]\tLoss: 0.829964\n",
      "Train Epoch: 0 [32000/482500 (7%)]\tLoss: 1.444081\n",
      "Train Epoch: 0 [32160/482500 (7%)]\tLoss: 1.280720\n",
      "Train Epoch: 0 [32320/482500 (7%)]\tLoss: 0.682270\n",
      "Train Epoch: 0 [32480/482500 (7%)]\tLoss: 1.579295\n",
      "Train Epoch: 0 [32640/482500 (7%)]\tLoss: 1.549273\n",
      "Train Epoch: 0 [32800/482500 (7%)]\tLoss: 0.843293\n",
      "Train Epoch: 0 [32960/482500 (7%)]\tLoss: 1.151219\n",
      "Train Epoch: 0 [33120/482500 (7%)]\tLoss: 0.752955\n",
      "Train Epoch: 0 [33280/482500 (7%)]\tLoss: 1.029634\n",
      "Train Epoch: 0 [33440/482500 (7%)]\tLoss: 4.904078\n",
      "Train Epoch: 0 [33600/482500 (7%)]\tLoss: 0.828017\n",
      "Train Epoch: 0 [33760/482500 (7%)]\tLoss: 0.643135\n",
      "Train Epoch: 0 [33920/482500 (7%)]\tLoss: 0.913519\n",
      "Train Epoch: 0 [34080/482500 (7%)]\tLoss: 1.205809\n",
      "Train Epoch: 0 [34240/482500 (7%)]\tLoss: 0.895608\n",
      "Train Epoch: 0 [34400/482500 (7%)]\tLoss: 0.701875\n",
      "Train Epoch: 0 [34560/482500 (7%)]\tLoss: 1.768179\n",
      "Train Epoch: 0 [34720/482500 (7%)]\tLoss: 0.876011\n",
      "Train Epoch: 0 [34880/482500 (7%)]\tLoss: 0.592306\n",
      "Train Epoch: 0 [35040/482500 (7%)]\tLoss: 1.226242\n",
      "Train Epoch: 0 [35200/482500 (7%)]\tLoss: 1.466402\n",
      "Train Epoch: 0 [35360/482500 (7%)]\tLoss: 0.824425\n",
      "Train Epoch: 0 [35520/482500 (7%)]\tLoss: 0.904253\n",
      "Train Epoch: 0 [35680/482500 (7%)]\tLoss: 1.170349\n",
      "Train Epoch: 0 [35840/482500 (7%)]\tLoss: 0.672874\n",
      "Train Epoch: 0 [36000/482500 (7%)]\tLoss: 0.413964\n",
      "Train Epoch: 0 [36160/482500 (7%)]\tLoss: 1.461287\n",
      "Train Epoch: 0 [36320/482500 (8%)]\tLoss: 1.498056\n",
      "Train Epoch: 0 [36480/482500 (8%)]\tLoss: 1.570463\n",
      "Train Epoch: 0 [36640/482500 (8%)]\tLoss: 0.988521\n",
      "Train Epoch: 0 [36800/482500 (8%)]\tLoss: 0.740447\n",
      "Train Epoch: 0 [36960/482500 (8%)]\tLoss: 0.553619\n",
      "Train Epoch: 0 [37120/482500 (8%)]\tLoss: 0.869960\n",
      "Train Epoch: 0 [37280/482500 (8%)]\tLoss: 1.010175\n",
      "Train Epoch: 0 [37440/482500 (8%)]\tLoss: 0.627605\n",
      "Train Epoch: 0 [37600/482500 (8%)]\tLoss: 1.212585\n",
      "Train Epoch: 0 [37760/482500 (8%)]\tLoss: 2.233409\n",
      "Train Epoch: 0 [37920/482500 (8%)]\tLoss: 0.754152\n",
      "Train Epoch: 0 [38080/482500 (8%)]\tLoss: 1.220574\n",
      "Train Epoch: 0 [38240/482500 (8%)]\tLoss: 1.353209\n",
      "Train Epoch: 0 [38400/482500 (8%)]\tLoss: 0.788385\n",
      "Train Epoch: 0 [38560/482500 (8%)]\tLoss: 3.035405\n",
      "Train Epoch: 0 [38720/482500 (8%)]\tLoss: 1.831797\n",
      "Train Epoch: 0 [38880/482500 (8%)]\tLoss: 0.786616\n",
      "Train Epoch: 0 [39040/482500 (8%)]\tLoss: 0.630125\n",
      "Train Epoch: 0 [39200/482500 (8%)]\tLoss: 0.614145\n",
      "Train Epoch: 0 [39360/482500 (8%)]\tLoss: 0.929431\n",
      "Train Epoch: 0 [39520/482500 (8%)]\tLoss: 0.752034\n",
      "Train Epoch: 0 [39680/482500 (8%)]\tLoss: 0.790841\n",
      "Train Epoch: 0 [39840/482500 (8%)]\tLoss: 0.986609\n",
      "Train Epoch: 0 [40000/482500 (8%)]\tLoss: 1.280979\n",
      "Train Epoch: 0 [40160/482500 (8%)]\tLoss: 2.630334\n",
      "Train Epoch: 0 [40320/482500 (8%)]\tLoss: 1.316297\n",
      "Train Epoch: 0 [40480/482500 (8%)]\tLoss: 1.078191\n",
      "Train Epoch: 0 [40640/482500 (8%)]\tLoss: 0.887415\n",
      "Train Epoch: 0 [40800/482500 (8%)]\tLoss: 21.487673\n",
      "Train Epoch: 0 [40960/482500 (8%)]\tLoss: 6.750946\n",
      "Train Epoch: 0 [41120/482500 (9%)]\tLoss: 1.846757\n",
      "Train Epoch: 0 [41280/482500 (9%)]\tLoss: 2.537621\n",
      "Train Epoch: 0 [41440/482500 (9%)]\tLoss: 0.893894\n",
      "Train Epoch: 0 [41600/482500 (9%)]\tLoss: 1.071389\n",
      "Train Epoch: 0 [41760/482500 (9%)]\tLoss: 1.234327\n",
      "Train Epoch: 0 [41920/482500 (9%)]\tLoss: 1.621280\n",
      "Train Epoch: 0 [42080/482500 (9%)]\tLoss: 0.452137\n",
      "Train Epoch: 0 [42240/482500 (9%)]\tLoss: 1.216650\n",
      "Train Epoch: 0 [42400/482500 (9%)]\tLoss: 0.992992\n",
      "Train Epoch: 0 [42560/482500 (9%)]\tLoss: 0.805284\n",
      "Train Epoch: 0 [42720/482500 (9%)]\tLoss: 0.874758\n",
      "Train Epoch: 0 [42880/482500 (9%)]\tLoss: 5.102405\n",
      "Train Epoch: 0 [43040/482500 (9%)]\tLoss: 2.527186\n",
      "Train Epoch: 0 [43200/482500 (9%)]\tLoss: 17.574741\n",
      "Train Epoch: 0 [43360/482500 (9%)]\tLoss: 60.063686\n",
      "Train Epoch: 0 [43520/482500 (9%)]\tLoss: 4.731665\n",
      "Train Epoch: 0 [43680/482500 (9%)]\tLoss: 14.566767\n",
      "Train Epoch: 0 [43840/482500 (9%)]\tLoss: 9.625265\n",
      "Train Epoch: 0 [44000/482500 (9%)]\tLoss: 2.251129\n",
      "Train Epoch: 0 [44160/482500 (9%)]\tLoss: 3.530470\n",
      "Train Epoch: 0 [44320/482500 (9%)]\tLoss: 1.765197\n",
      "Train Epoch: 0 [44480/482500 (9%)]\tLoss: 1.819177\n",
      "Train Epoch: 0 [44640/482500 (9%)]\tLoss: 4.034240\n",
      "Train Epoch: 0 [44800/482500 (9%)]\tLoss: 1.028256\n",
      "Train Epoch: 0 [44960/482500 (9%)]\tLoss: 5.155182\n",
      "Train Epoch: 0 [45120/482500 (9%)]\tLoss: 2.003264\n",
      "Train Epoch: 0 [45280/482500 (9%)]\tLoss: 2.828412\n",
      "Train Epoch: 0 [45440/482500 (9%)]\tLoss: 1.544579\n",
      "Train Epoch: 0 [45600/482500 (9%)]\tLoss: 3.023967\n",
      "Train Epoch: 0 [45760/482500 (9%)]\tLoss: 0.625675\n",
      "Train Epoch: 0 [45920/482500 (10%)]\tLoss: 0.767000\n",
      "Train Epoch: 0 [46080/482500 (10%)]\tLoss: 0.543662\n",
      "Train Epoch: 0 [46240/482500 (10%)]\tLoss: 3.177950\n",
      "Train Epoch: 0 [46400/482500 (10%)]\tLoss: 0.596437\n",
      "Train Epoch: 0 [46560/482500 (10%)]\tLoss: 0.845611\n",
      "Train Epoch: 0 [46720/482500 (10%)]\tLoss: 0.744375\n",
      "Train Epoch: 0 [46880/482500 (10%)]\tLoss: 1.604812\n",
      "Train Epoch: 0 [47040/482500 (10%)]\tLoss: 0.693746\n",
      "Train Epoch: 0 [47200/482500 (10%)]\tLoss: 0.452353\n",
      "Train Epoch: 0 [47360/482500 (10%)]\tLoss: 2.241844\n",
      "Train Epoch: 0 [47520/482500 (10%)]\tLoss: 0.619987\n",
      "Train Epoch: 0 [47680/482500 (10%)]\tLoss: 1.178834\n",
      "Train Epoch: 0 [47840/482500 (10%)]\tLoss: 0.908855\n",
      "Train Epoch: 0 [48000/482500 (10%)]\tLoss: 1.101226\n",
      "Train Epoch: 0 [48160/482500 (10%)]\tLoss: 1.105228\n",
      "Train Epoch: 0 [48320/482500 (10%)]\tLoss: 2.359996\n",
      "Train Epoch: 0 [48480/482500 (10%)]\tLoss: 1.357399\n",
      "Train Epoch: 0 [48640/482500 (10%)]\tLoss: 0.937446\n",
      "Train Epoch: 0 [48800/482500 (10%)]\tLoss: 2.648103\n",
      "Train Epoch: 0 [48960/482500 (10%)]\tLoss: 0.705291\n",
      "Train Epoch: 0 [49120/482500 (10%)]\tLoss: 0.593980\n",
      "Train Epoch: 0 [49280/482500 (10%)]\tLoss: 0.913186\n",
      "Train Epoch: 0 [49440/482500 (10%)]\tLoss: 0.989633\n",
      "Train Epoch: 0 [49600/482500 (10%)]\tLoss: 0.519073\n",
      "Train Epoch: 0 [49760/482500 (10%)]\tLoss: 0.690171\n",
      "Train Epoch: 0 [49920/482500 (10%)]\tLoss: 0.789695\n",
      "Train Epoch: 0 [50080/482500 (10%)]\tLoss: 0.845125\n",
      "Train Epoch: 0 [50240/482500 (10%)]\tLoss: 1.073374\n",
      "Train Epoch: 0 [50400/482500 (10%)]\tLoss: 0.725883\n",
      "Train Epoch: 0 [50560/482500 (10%)]\tLoss: 0.682519\n",
      "Train Epoch: 0 [50720/482500 (11%)]\tLoss: 0.553387\n",
      "Train Epoch: 0 [50880/482500 (11%)]\tLoss: 0.458093\n",
      "Train Epoch: 0 [51040/482500 (11%)]\tLoss: 0.688137\n",
      "Train Epoch: 0 [51200/482500 (11%)]\tLoss: 0.613910\n",
      "Train Epoch: 0 [51360/482500 (11%)]\tLoss: 0.701832\n",
      "Train Epoch: 0 [51520/482500 (11%)]\tLoss: 1.182334\n",
      "Train Epoch: 0 [51680/482500 (11%)]\tLoss: 0.535535\n",
      "Train Epoch: 0 [51840/482500 (11%)]\tLoss: 0.829885\n",
      "Train Epoch: 0 [52000/482500 (11%)]\tLoss: 0.957948\n",
      "Train Epoch: 0 [52160/482500 (11%)]\tLoss: 0.592820\n",
      "Train Epoch: 0 [52320/482500 (11%)]\tLoss: 1.020852\n",
      "Train Epoch: 0 [52480/482500 (11%)]\tLoss: 0.604419\n",
      "Train Epoch: 0 [52640/482500 (11%)]\tLoss: 0.442203\n",
      "Train Epoch: 0 [52800/482500 (11%)]\tLoss: 0.832141\n",
      "Train Epoch: 0 [52960/482500 (11%)]\tLoss: 0.436314\n",
      "Train Epoch: 0 [53120/482500 (11%)]\tLoss: 0.621509\n",
      "Train Epoch: 0 [53280/482500 (11%)]\tLoss: 0.573079\n",
      "Train Epoch: 0 [53440/482500 (11%)]\tLoss: 0.999578\n",
      "Train Epoch: 0 [53600/482500 (11%)]\tLoss: 1.269767\n",
      "Train Epoch: 0 [53760/482500 (11%)]\tLoss: 3.514920\n",
      "Train Epoch: 0 [53920/482500 (11%)]\tLoss: 0.909585\n",
      "Train Epoch: 0 [54080/482500 (11%)]\tLoss: 0.852210\n",
      "Train Epoch: 0 [54240/482500 (11%)]\tLoss: 1.706189\n",
      "Train Epoch: 0 [54400/482500 (11%)]\tLoss: 0.641006\n",
      "Train Epoch: 0 [54560/482500 (11%)]\tLoss: 0.541777\n",
      "Train Epoch: 0 [54720/482500 (11%)]\tLoss: 0.648749\n",
      "Train Epoch: 0 [54880/482500 (11%)]\tLoss: 0.748244\n",
      "Train Epoch: 0 [55040/482500 (11%)]\tLoss: 0.346270\n",
      "Train Epoch: 0 [55200/482500 (11%)]\tLoss: 0.397606\n",
      "Train Epoch: 0 [55360/482500 (11%)]\tLoss: 0.799064\n",
      "Train Epoch: 0 [55520/482500 (12%)]\tLoss: 0.639032\n",
      "Train Epoch: 0 [55680/482500 (12%)]\tLoss: 0.987622\n",
      "Train Epoch: 0 [55840/482500 (12%)]\tLoss: 0.446671\n",
      "Train Epoch: 0 [56000/482500 (12%)]\tLoss: 2.104234\n",
      "Train Epoch: 0 [56160/482500 (12%)]\tLoss: 2.114389\n",
      "Train Epoch: 0 [56320/482500 (12%)]\tLoss: 1.863525\n",
      "Train Epoch: 0 [56480/482500 (12%)]\tLoss: 0.952823\n",
      "Train Epoch: 0 [56640/482500 (12%)]\tLoss: 1.334209\n",
      "Train Epoch: 0 [56800/482500 (12%)]\tLoss: 0.890154\n",
      "Train Epoch: 0 [56960/482500 (12%)]\tLoss: 1.010047\n",
      "Train Epoch: 0 [57120/482500 (12%)]\tLoss: 0.548227\n",
      "Train Epoch: 0 [57280/482500 (12%)]\tLoss: 62.044384\n",
      "Train Epoch: 0 [57440/482500 (12%)]\tLoss: 14.358928\n",
      "Train Epoch: 0 [57600/482500 (12%)]\tLoss: 5.167663\n",
      "Train Epoch: 0 [57760/482500 (12%)]\tLoss: 2.021724\n",
      "Train Epoch: 0 [57920/482500 (12%)]\tLoss: 0.985624\n",
      "Train Epoch: 0 [58080/482500 (12%)]\tLoss: 1.172384\n",
      "Train Epoch: 0 [58240/482500 (12%)]\tLoss: 7.714021\n",
      "Train Epoch: 0 [58400/482500 (12%)]\tLoss: 1.661822\n",
      "Train Epoch: 0 [58560/482500 (12%)]\tLoss: 1.519277\n",
      "Train Epoch: 0 [58720/482500 (12%)]\tLoss: 4.217488\n",
      "Train Epoch: 0 [58880/482500 (12%)]\tLoss: 1.002035\n",
      "Train Epoch: 0 [59040/482500 (12%)]\tLoss: 0.893293\n",
      "Train Epoch: 0 [59200/482500 (12%)]\tLoss: 2.106894\n",
      "Train Epoch: 0 [59360/482500 (12%)]\tLoss: 0.853304\n",
      "Train Epoch: 0 [59520/482500 (12%)]\tLoss: 1.248382\n",
      "Train Epoch: 0 [59680/482500 (12%)]\tLoss: 0.864431\n",
      "Train Epoch: 0 [59840/482500 (12%)]\tLoss: 1.634904\n",
      "Train Epoch: 0 [60000/482500 (12%)]\tLoss: 0.996208\n",
      "Train Epoch: 0 [60160/482500 (12%)]\tLoss: 0.518478\n",
      "Train Epoch: 0 [60320/482500 (13%)]\tLoss: 0.679016\n",
      "Train Epoch: 0 [60480/482500 (13%)]\tLoss: 0.756946\n",
      "Train Epoch: 0 [60640/482500 (13%)]\tLoss: 4.048738\n",
      "Train Epoch: 0 [60800/482500 (13%)]\tLoss: 0.647532\n",
      "Train Epoch: 0 [60960/482500 (13%)]\tLoss: 0.724996\n",
      "Train Epoch: 0 [61120/482500 (13%)]\tLoss: 1.715322\n",
      "Train Epoch: 0 [61280/482500 (13%)]\tLoss: 1.332480\n",
      "Train Epoch: 0 [61440/482500 (13%)]\tLoss: 0.551204\n",
      "Train Epoch: 0 [61600/482500 (13%)]\tLoss: 0.375105\n",
      "Train Epoch: 0 [61760/482500 (13%)]\tLoss: 0.691897\n",
      "Train Epoch: 0 [61920/482500 (13%)]\tLoss: 0.623747\n",
      "Train Epoch: 0 [62080/482500 (13%)]\tLoss: 0.860500\n",
      "Train Epoch: 0 [62240/482500 (13%)]\tLoss: 0.497896\n",
      "Train Epoch: 0 [62400/482500 (13%)]\tLoss: 706.074829\n",
      "Train Epoch: 0 [62560/482500 (13%)]\tLoss: 32.020340\n",
      "Train Epoch: 0 [62720/482500 (13%)]\tLoss: 8.853610\n",
      "Train Epoch: 0 [62880/482500 (13%)]\tLoss: 1.984689\n",
      "Train Epoch: 0 [63040/482500 (13%)]\tLoss: 3.891426\n",
      "Train Epoch: 0 [63200/482500 (13%)]\tLoss: 1.340740\n",
      "Train Epoch: 0 [63360/482500 (13%)]\tLoss: 1.586162\n",
      "Train Epoch: 0 [63520/482500 (13%)]\tLoss: 0.655366\n",
      "Train Epoch: 0 [63680/482500 (13%)]\tLoss: 1.347765\n",
      "Train Epoch: 0 [63840/482500 (13%)]\tLoss: 2.335212\n",
      "Train Epoch: 0 [64000/482500 (13%)]\tLoss: 1.492539\n",
      "Train Epoch: 0 [64160/482500 (13%)]\tLoss: 1.375625\n",
      "Train Epoch: 0 [64320/482500 (13%)]\tLoss: 3.520909\n",
      "Train Epoch: 0 [64480/482500 (13%)]\tLoss: 2.118796\n",
      "Train Epoch: 0 [64640/482500 (13%)]\tLoss: 183.576630\n",
      "Train Epoch: 0 [64800/482500 (13%)]\tLoss: 29.112782\n",
      "Train Epoch: 0 [64960/482500 (13%)]\tLoss: 9.514734\n",
      "Train Epoch: 0 [65120/482500 (13%)]\tLoss: 12.636706\n",
      "Train Epoch: 0 [65280/482500 (14%)]\tLoss: 3.598739\n",
      "Train Epoch: 0 [65440/482500 (14%)]\tLoss: 3.278126\n",
      "Train Epoch: 0 [65600/482500 (14%)]\tLoss: 3.635283\n",
      "Train Epoch: 0 [65760/482500 (14%)]\tLoss: 2.117244\n",
      "Train Epoch: 0 [65920/482500 (14%)]\tLoss: 4.571260\n",
      "Train Epoch: 0 [66080/482500 (14%)]\tLoss: 6.313859\n",
      "Train Epoch: 0 [66240/482500 (14%)]\tLoss: 3.449879\n",
      "Train Epoch: 0 [66400/482500 (14%)]\tLoss: 2.323266\n",
      "Train Epoch: 0 [66560/482500 (14%)]\tLoss: 1.822574\n",
      "Train Epoch: 0 [66720/482500 (14%)]\tLoss: 6.808364\n",
      "Train Epoch: 0 [66880/482500 (14%)]\tLoss: 3.215868\n",
      "Train Epoch: 0 [67040/482500 (14%)]\tLoss: 2.211632\n",
      "Train Epoch: 0 [67200/482500 (14%)]\tLoss: 2.174672\n",
      "Train Epoch: 0 [67360/482500 (14%)]\tLoss: 1.076553\n",
      "Train Epoch: 0 [67520/482500 (14%)]\tLoss: 2.027145\n",
      "Train Epoch: 0 [67680/482500 (14%)]\tLoss: 2.368004\n",
      "Train Epoch: 0 [67840/482500 (14%)]\tLoss: 1.382381\n",
      "Train Epoch: 0 [68000/482500 (14%)]\tLoss: 1.048560\n",
      "Train Epoch: 0 [68160/482500 (14%)]\tLoss: 1.785498\n",
      "Train Epoch: 0 [68320/482500 (14%)]\tLoss: 1.636502\n",
      "Train Epoch: 0 [68480/482500 (14%)]\tLoss: 1.055067\n",
      "Train Epoch: 0 [68640/482500 (14%)]\tLoss: 1.631408\n",
      "Train Epoch: 0 [68800/482500 (14%)]\tLoss: 2.766855\n",
      "Train Epoch: 0 [68960/482500 (14%)]\tLoss: 1.178081\n",
      "Train Epoch: 0 [69120/482500 (14%)]\tLoss: 1.300545\n",
      "Train Epoch: 0 [69280/482500 (14%)]\tLoss: 0.845590\n",
      "Train Epoch: 0 [69440/482500 (14%)]\tLoss: 0.997420\n",
      "Train Epoch: 0 [69600/482500 (14%)]\tLoss: 1.956757\n",
      "Train Epoch: 0 [69760/482500 (14%)]\tLoss: 1.133107\n",
      "Train Epoch: 0 [69920/482500 (14%)]\tLoss: 1.786437\n",
      "Train Epoch: 0 [70080/482500 (15%)]\tLoss: 1.417724\n",
      "Train Epoch: 0 [70240/482500 (15%)]\tLoss: 1.354008\n",
      "Train Epoch: 0 [70400/482500 (15%)]\tLoss: 1.695780\n",
      "Train Epoch: 0 [70560/482500 (15%)]\tLoss: 1.754146\n",
      "Train Epoch: 0 [70720/482500 (15%)]\tLoss: 0.954628\n",
      "Train Epoch: 0 [70880/482500 (15%)]\tLoss: 1.129989\n",
      "Train Epoch: 0 [71040/482500 (15%)]\tLoss: 1.338450\n",
      "Train Epoch: 0 [71200/482500 (15%)]\tLoss: 1.597172\n",
      "Train Epoch: 0 [71360/482500 (15%)]\tLoss: 0.665885\n",
      "Train Epoch: 0 [71520/482500 (15%)]\tLoss: 0.964284\n",
      "Train Epoch: 0 [71680/482500 (15%)]\tLoss: 2.654721\n",
      "Train Epoch: 0 [71840/482500 (15%)]\tLoss: 1.103161\n",
      "Train Epoch: 0 [72000/482500 (15%)]\tLoss: 0.869519\n",
      "Train Epoch: 0 [72160/482500 (15%)]\tLoss: 0.860737\n",
      "Train Epoch: 0 [72320/482500 (15%)]\tLoss: 1.105393\n",
      "Train Epoch: 0 [72480/482500 (15%)]\tLoss: 1.058350\n",
      "Train Epoch: 0 [72640/482500 (15%)]\tLoss: 1.062418\n",
      "Train Epoch: 0 [72800/482500 (15%)]\tLoss: 1.738320\n",
      "Train Epoch: 0 [72960/482500 (15%)]\tLoss: 0.975754\n",
      "Train Epoch: 0 [73120/482500 (15%)]\tLoss: 0.547483\n",
      "Train Epoch: 0 [73280/482500 (15%)]\tLoss: 0.920167\n",
      "Train Epoch: 0 [73440/482500 (15%)]\tLoss: 0.872179\n",
      "Train Epoch: 0 [73600/482500 (15%)]\tLoss: 55.104538\n",
      "Train Epoch: 0 [73760/482500 (15%)]\tLoss: 22.928247\n",
      "Train Epoch: 0 [73920/482500 (15%)]\tLoss: 6.054625\n",
      "Train Epoch: 0 [74080/482500 (15%)]\tLoss: 3.162211\n",
      "Train Epoch: 0 [74240/482500 (15%)]\tLoss: 2.380021\n",
      "Train Epoch: 0 [74400/482500 (15%)]\tLoss: 2.786639\n",
      "Train Epoch: 0 [74560/482500 (15%)]\tLoss: 2.027228\n",
      "Train Epoch: 0 [74720/482500 (15%)]\tLoss: 2.115487\n",
      "Train Epoch: 0 [74880/482500 (16%)]\tLoss: 3.662490\n",
      "Train Epoch: 0 [75040/482500 (16%)]\tLoss: 1.281749\n",
      "Train Epoch: 0 [75200/482500 (16%)]\tLoss: 2.841134\n",
      "Train Epoch: 0 [75360/482500 (16%)]\tLoss: 24.764507\n",
      "Train Epoch: 0 [75520/482500 (16%)]\tLoss: 17.076590\n",
      "Train Epoch: 0 [75680/482500 (16%)]\tLoss: 5.800060\n",
      "Train Epoch: 0 [75840/482500 (16%)]\tLoss: 1.861336\n",
      "Train Epoch: 0 [76000/482500 (16%)]\tLoss: 2.822466\n",
      "Train Epoch: 0 [76160/482500 (16%)]\tLoss: 1.455700\n",
      "Train Epoch: 0 [76320/482500 (16%)]\tLoss: 1.784944\n",
      "Train Epoch: 0 [76480/482500 (16%)]\tLoss: 1.775657\n",
      "Train Epoch: 0 [76640/482500 (16%)]\tLoss: 2.105103\n",
      "Train Epoch: 0 [76800/482500 (16%)]\tLoss: 2.099963\n",
      "Train Epoch: 0 [76960/482500 (16%)]\tLoss: 2.104056\n",
      "Train Epoch: 0 [77120/482500 (16%)]\tLoss: 1.587959\n",
      "Train Epoch: 0 [77280/482500 (16%)]\tLoss: 1.527848\n",
      "Train Epoch: 0 [77440/482500 (16%)]\tLoss: 1.034752\n",
      "Train Epoch: 0 [77600/482500 (16%)]\tLoss: 1.333441\n",
      "Train Epoch: 0 [77760/482500 (16%)]\tLoss: 2.147323\n",
      "Train Epoch: 0 [77920/482500 (16%)]\tLoss: 1.477647\n",
      "Train Epoch: 0 [78080/482500 (16%)]\tLoss: 2.922615\n",
      "Train Epoch: 0 [78240/482500 (16%)]\tLoss: 1.454336\n",
      "Train Epoch: 0 [78400/482500 (16%)]\tLoss: 4.224432\n",
      "Train Epoch: 0 [78560/482500 (16%)]\tLoss: 1.628330\n",
      "Train Epoch: 0 [78720/482500 (16%)]\tLoss: 5.323699\n",
      "Train Epoch: 0 [78880/482500 (16%)]\tLoss: 1.972659\n",
      "Train Epoch: 0 [79040/482500 (16%)]\tLoss: 1.228506\n",
      "Train Epoch: 0 [79200/482500 (16%)]\tLoss: 1.538987\n",
      "Train Epoch: 0 [79360/482500 (16%)]\tLoss: 1.431217\n",
      "Train Epoch: 0 [79520/482500 (16%)]\tLoss: 1.523117\n",
      "Train Epoch: 0 [79680/482500 (17%)]\tLoss: 1.395543\n",
      "Train Epoch: 0 [79840/482500 (17%)]\tLoss: 1.358517\n",
      "Train Epoch: 0 [80000/482500 (17%)]\tLoss: 1.141667\n",
      "Train Epoch: 0 [80160/482500 (17%)]\tLoss: 67.530617\n",
      "Train Epoch: 0 [80320/482500 (17%)]\tLoss: 9.748470\n",
      "Train Epoch: 0 [80480/482500 (17%)]\tLoss: 1.795525\n",
      "Train Epoch: 0 [80640/482500 (17%)]\tLoss: 3.133966\n",
      "Train Epoch: 0 [80800/482500 (17%)]\tLoss: 1.290199\n",
      "Train Epoch: 0 [80960/482500 (17%)]\tLoss: 1.446275\n",
      "Train Epoch: 0 [81120/482500 (17%)]\tLoss: 1.442805\n",
      "Train Epoch: 0 [81280/482500 (17%)]\tLoss: 1.262812\n",
      "Train Epoch: 0 [81440/482500 (17%)]\tLoss: 2.879364\n",
      "Train Epoch: 0 [81600/482500 (17%)]\tLoss: 1.709883\n",
      "Train Epoch: 0 [81760/482500 (17%)]\tLoss: 8.225618\n",
      "Train Epoch: 0 [81920/482500 (17%)]\tLoss: 4.082680\n",
      "Train Epoch: 0 [82080/482500 (17%)]\tLoss: 2.829071\n",
      "Train Epoch: 0 [82240/482500 (17%)]\tLoss: 2.727577\n",
      "Train Epoch: 0 [82400/482500 (17%)]\tLoss: 3.171368\n",
      "Train Epoch: 0 [82560/482500 (17%)]\tLoss: 1.438130\n",
      "Train Epoch: 0 [82720/482500 (17%)]\tLoss: 1.999703\n",
      "Train Epoch: 0 [82880/482500 (17%)]\tLoss: 1.264433\n",
      "Train Epoch: 0 [83040/482500 (17%)]\tLoss: 1.430228\n",
      "Train Epoch: 0 [83200/482500 (17%)]\tLoss: 1.445812\n",
      "Train Epoch: 0 [83360/482500 (17%)]\tLoss: 1.291714\n",
      "Train Epoch: 0 [83520/482500 (17%)]\tLoss: 1.713475\n",
      "Train Epoch: 0 [83680/482500 (17%)]\tLoss: 0.999527\n",
      "Train Epoch: 0 [83840/482500 (17%)]\tLoss: 1.271740\n",
      "Train Epoch: 0 [84000/482500 (17%)]\tLoss: 0.866982\n",
      "Train Epoch: 0 [84160/482500 (17%)]\tLoss: 1.076895\n",
      "Train Epoch: 0 [84320/482500 (17%)]\tLoss: 0.970101\n",
      "Train Epoch: 0 [84480/482500 (18%)]\tLoss: 1.167077\n",
      "Train Epoch: 0 [84640/482500 (18%)]\tLoss: 1.435333\n",
      "Train Epoch: 0 [84800/482500 (18%)]\tLoss: 1.781079\n",
      "Train Epoch: 0 [84960/482500 (18%)]\tLoss: 1.156524\n",
      "Train Epoch: 0 [85120/482500 (18%)]\tLoss: 1.249324\n",
      "Train Epoch: 0 [85280/482500 (18%)]\tLoss: 1.321795\n",
      "Train Epoch: 0 [85440/482500 (18%)]\tLoss: 0.991729\n",
      "Train Epoch: 0 [85600/482500 (18%)]\tLoss: 1.288697\n",
      "Train Epoch: 0 [85760/482500 (18%)]\tLoss: 1.162584\n",
      "Train Epoch: 0 [85920/482500 (18%)]\tLoss: 0.644865\n",
      "Train Epoch: 0 [86080/482500 (18%)]\tLoss: 1.217124\n",
      "Train Epoch: 0 [86240/482500 (18%)]\tLoss: 0.927535\n",
      "Train Epoch: 0 [86400/482500 (18%)]\tLoss: 0.608268\n",
      "Train Epoch: 0 [86560/482500 (18%)]\tLoss: 1.395760\n",
      "Train Epoch: 0 [86720/482500 (18%)]\tLoss: 0.893324\n",
      "Train Epoch: 0 [86880/482500 (18%)]\tLoss: 0.832380\n",
      "Train Epoch: 0 [87040/482500 (18%)]\tLoss: 0.983191\n",
      "Train Epoch: 0 [87200/482500 (18%)]\tLoss: 0.865163\n",
      "Train Epoch: 0 [87360/482500 (18%)]\tLoss: 0.835502\n",
      "Train Epoch: 0 [87520/482500 (18%)]\tLoss: 1.038069\n",
      "Train Epoch: 0 [87680/482500 (18%)]\tLoss: 0.830625\n",
      "Train Epoch: 0 [87840/482500 (18%)]\tLoss: 0.935672\n",
      "Train Epoch: 0 [88000/482500 (18%)]\tLoss: 0.562268\n",
      "Train Epoch: 0 [88160/482500 (18%)]\tLoss: 170.238419\n",
      "Train Epoch: 0 [88320/482500 (18%)]\tLoss: 22.546848\n",
      "Train Epoch: 0 [88480/482500 (18%)]\tLoss: 3.547672\n",
      "Train Epoch: 0 [88640/482500 (18%)]\tLoss: 3.489898\n",
      "Train Epoch: 0 [88800/482500 (18%)]\tLoss: 4.176217\n",
      "Train Epoch: 0 [88960/482500 (18%)]\tLoss: 2.565823\n",
      "Train Epoch: 0 [89120/482500 (18%)]\tLoss: 1.949165\n",
      "Train Epoch: 0 [89280/482500 (19%)]\tLoss: 0.673876\n",
      "Train Epoch: 0 [89440/482500 (19%)]\tLoss: 1.913273\n",
      "Train Epoch: 0 [89600/482500 (19%)]\tLoss: 1.342450\n",
      "Train Epoch: 0 [89760/482500 (19%)]\tLoss: 1.081923\n",
      "Train Epoch: 0 [89920/482500 (19%)]\tLoss: 1.131009\n",
      "Train Epoch: 0 [90080/482500 (19%)]\tLoss: 1.209244\n",
      "Train Epoch: 0 [90240/482500 (19%)]\tLoss: 0.949337\n",
      "Train Epoch: 0 [90400/482500 (19%)]\tLoss: 0.920109\n",
      "Train Epoch: 0 [90560/482500 (19%)]\tLoss: 1.616845\n",
      "Train Epoch: 0 [90720/482500 (19%)]\tLoss: 0.674088\n",
      "Train Epoch: 0 [90880/482500 (19%)]\tLoss: 0.793030\n",
      "Train Epoch: 0 [91040/482500 (19%)]\tLoss: 0.710851\n",
      "Train Epoch: 0 [91200/482500 (19%)]\tLoss: 0.976229\n",
      "Train Epoch: 0 [91360/482500 (19%)]\tLoss: 0.945710\n",
      "Train Epoch: 0 [91520/482500 (19%)]\tLoss: 0.849611\n",
      "Train Epoch: 0 [91680/482500 (19%)]\tLoss: 1.114105\n",
      "Train Epoch: 0 [91840/482500 (19%)]\tLoss: 0.999552\n",
      "Train Epoch: 0 [92000/482500 (19%)]\tLoss: 0.362527\n",
      "Train Epoch: 0 [92160/482500 (19%)]\tLoss: 1.310507\n",
      "Train Epoch: 0 [92320/482500 (19%)]\tLoss: 1.004131\n",
      "Train Epoch: 0 [92480/482500 (19%)]\tLoss: 0.752143\n",
      "Train Epoch: 0 [92640/482500 (19%)]\tLoss: 0.689684\n",
      "Train Epoch: 0 [92800/482500 (19%)]\tLoss: 0.830435\n",
      "Train Epoch: 0 [92960/482500 (19%)]\tLoss: 0.792228\n",
      "Train Epoch: 0 [93120/482500 (19%)]\tLoss: 0.629269\n",
      "Train Epoch: 0 [93280/482500 (19%)]\tLoss: 0.786031\n",
      "Train Epoch: 0 [93440/482500 (19%)]\tLoss: 1.106883\n",
      "Train Epoch: 0 [93600/482500 (19%)]\tLoss: 0.660821\n",
      "Train Epoch: 0 [93760/482500 (19%)]\tLoss: 0.745562\n",
      "Train Epoch: 0 [93920/482500 (19%)]\tLoss: 0.787894\n",
      "Train Epoch: 0 [94080/482500 (19%)]\tLoss: 0.595944\n",
      "Train Epoch: 0 [94240/482500 (20%)]\tLoss: 0.666708\n",
      "Train Epoch: 0 [94400/482500 (20%)]\tLoss: 1.316808\n",
      "Train Epoch: 0 [94560/482500 (20%)]\tLoss: 0.904906\n",
      "Train Epoch: 0 [94720/482500 (20%)]\tLoss: 0.639053\n",
      "Train Epoch: 0 [94880/482500 (20%)]\tLoss: 0.453458\n",
      "Train Epoch: 0 [95040/482500 (20%)]\tLoss: 0.665719\n",
      "Train Epoch: 0 [95200/482500 (20%)]\tLoss: 1.000291\n",
      "Train Epoch: 0 [95360/482500 (20%)]\tLoss: 1.000843\n",
      "Train Epoch: 0 [95520/482500 (20%)]\tLoss: 0.458625\n",
      "Train Epoch: 0 [95680/482500 (20%)]\tLoss: 0.785278\n",
      "Train Epoch: 0 [95840/482500 (20%)]\tLoss: 0.522033\n",
      "Train Epoch: 0 [96000/482500 (20%)]\tLoss: 0.778247\n",
      "Train Epoch: 0 [96160/482500 (20%)]\tLoss: 0.487382\n",
      "Train Epoch: 0 [96320/482500 (20%)]\tLoss: 0.566234\n",
      "Train Epoch: 0 [96480/482500 (20%)]\tLoss: 0.841779\n",
      "Train Epoch: 0 [96640/482500 (20%)]\tLoss: 0.637599\n",
      "Train Epoch: 0 [96800/482500 (20%)]\tLoss: 0.603855\n",
      "Train Epoch: 0 [96960/482500 (20%)]\tLoss: 0.739634\n",
      "Train Epoch: 0 [97120/482500 (20%)]\tLoss: 0.708358\n",
      "Train Epoch: 0 [97280/482500 (20%)]\tLoss: 0.862676\n",
      "Train Epoch: 0 [97440/482500 (20%)]\tLoss: 0.898257\n",
      "Train Epoch: 0 [97600/482500 (20%)]\tLoss: 0.710720\n",
      "Train Epoch: 0 [97760/482500 (20%)]\tLoss: 0.912500\n",
      "Train Epoch: 0 [97920/482500 (20%)]\tLoss: 0.727784\n",
      "Train Epoch: 0 [98080/482500 (20%)]\tLoss: 0.714529\n",
      "Train Epoch: 0 [98240/482500 (20%)]\tLoss: 0.807754\n",
      "Train Epoch: 0 [98400/482500 (20%)]\tLoss: 33.351498\n",
      "Train Epoch: 0 [98560/482500 (20%)]\tLoss: 4.828251\n",
      "Train Epoch: 0 [98720/482500 (20%)]\tLoss: 1.884558\n",
      "Train Epoch: 0 [98880/482500 (20%)]\tLoss: 1.187012\n",
      "Train Epoch: 0 [99040/482500 (21%)]\tLoss: 0.852202\n",
      "Train Epoch: 0 [99200/482500 (21%)]\tLoss: 0.718834\n",
      "Train Epoch: 0 [99360/482500 (21%)]\tLoss: 11.024616\n",
      "Train Epoch: 0 [99520/482500 (21%)]\tLoss: 1.142541\n",
      "Train Epoch: 0 [99680/482500 (21%)]\tLoss: 0.868999\n",
      "Train Epoch: 0 [99840/482500 (21%)]\tLoss: 0.697625\n",
      "Train Epoch: 0 [100000/482500 (21%)]\tLoss: 0.755699\n",
      "Train Epoch: 0 [100160/482500 (21%)]\tLoss: 0.925655\n",
      "Train Epoch: 0 [100320/482500 (21%)]\tLoss: 0.678086\n",
      "Train Epoch: 0 [100480/482500 (21%)]\tLoss: 2.629000\n",
      "Train Epoch: 0 [100640/482500 (21%)]\tLoss: 0.729828\n",
      "Train Epoch: 0 [100800/482500 (21%)]\tLoss: 0.770188\n",
      "Train Epoch: 0 [100960/482500 (21%)]\tLoss: 0.670611\n",
      "Train Epoch: 0 [101120/482500 (21%)]\tLoss: 0.714766\n",
      "Train Epoch: 0 [101280/482500 (21%)]\tLoss: 0.870158\n",
      "Train Epoch: 0 [101440/482500 (21%)]\tLoss: 0.730398\n",
      "Train Epoch: 0 [101600/482500 (21%)]\tLoss: 0.715593\n",
      "Train Epoch: 0 [101760/482500 (21%)]\tLoss: 0.680043\n",
      "Train Epoch: 0 [101920/482500 (21%)]\tLoss: 0.611515\n",
      "Train Epoch: 0 [102080/482500 (21%)]\tLoss: 0.728272\n",
      "Train Epoch: 0 [102240/482500 (21%)]\tLoss: 0.515751\n",
      "Train Epoch: 0 [102400/482500 (21%)]\tLoss: 0.548625\n",
      "Train Epoch: 0 [102560/482500 (21%)]\tLoss: 0.508597\n",
      "Train Epoch: 0 [102720/482500 (21%)]\tLoss: 0.533033\n",
      "Train Epoch: 0 [102880/482500 (21%)]\tLoss: 0.910368\n",
      "Train Epoch: 0 [103040/482500 (21%)]\tLoss: 0.419264\n",
      "Train Epoch: 0 [103200/482500 (21%)]\tLoss: 1.194349\n",
      "Train Epoch: 0 [103360/482500 (21%)]\tLoss: 0.601906\n",
      "Train Epoch: 0 [103520/482500 (21%)]\tLoss: 0.602006\n",
      "Train Epoch: 0 [103680/482500 (21%)]\tLoss: 0.812707\n",
      "Train Epoch: 0 [103840/482500 (22%)]\tLoss: 1.482799\n",
      "Train Epoch: 0 [104000/482500 (22%)]\tLoss: 0.735054\n",
      "Train Epoch: 0 [104160/482500 (22%)]\tLoss: 0.467718\n",
      "Train Epoch: 0 [104320/482500 (22%)]\tLoss: 0.682096\n",
      "Train Epoch: 0 [104480/482500 (22%)]\tLoss: 1.735999\n",
      "Train Epoch: 0 [104640/482500 (22%)]\tLoss: 0.571464\n",
      "Train Epoch: 0 [104800/482500 (22%)]\tLoss: 0.791989\n",
      "Train Epoch: 0 [104960/482500 (22%)]\tLoss: 0.436026\n",
      "Train Epoch: 0 [105120/482500 (22%)]\tLoss: 0.562258\n",
      "Train Epoch: 0 [105280/482500 (22%)]\tLoss: 0.554633\n",
      "Train Epoch: 0 [105440/482500 (22%)]\tLoss: 1.089322\n",
      "Train Epoch: 0 [105600/482500 (22%)]\tLoss: 0.528202\n",
      "Train Epoch: 0 [105760/482500 (22%)]\tLoss: 0.832342\n",
      "Train Epoch: 0 [105920/482500 (22%)]\tLoss: 0.920373\n",
      "Train Epoch: 0 [106080/482500 (22%)]\tLoss: 0.742478\n",
      "Train Epoch: 0 [106240/482500 (22%)]\tLoss: 0.982153\n",
      "Train Epoch: 0 [106400/482500 (22%)]\tLoss: 0.419839\n",
      "Train Epoch: 0 [106560/482500 (22%)]\tLoss: 0.796031\n",
      "Train Epoch: 0 [106720/482500 (22%)]\tLoss: 0.477315\n",
      "Train Epoch: 0 [106880/482500 (22%)]\tLoss: 0.373111\n",
      "Train Epoch: 0 [107040/482500 (22%)]\tLoss: 7.339972\n",
      "Train Epoch: 0 [107200/482500 (22%)]\tLoss: 2.929895\n",
      "Train Epoch: 0 [107360/482500 (22%)]\tLoss: 0.896031\n",
      "Train Epoch: 0 [107520/482500 (22%)]\tLoss: 0.982474\n",
      "Train Epoch: 0 [107680/482500 (22%)]\tLoss: 0.569367\n",
      "Train Epoch: 0 [107840/482500 (22%)]\tLoss: 1.002486\n",
      "Train Epoch: 0 [108000/482500 (22%)]\tLoss: 0.980902\n",
      "Train Epoch: 0 [108160/482500 (22%)]\tLoss: 1.813105\n",
      "Train Epoch: 0 [108320/482500 (22%)]\tLoss: 0.614417\n",
      "Train Epoch: 0 [108480/482500 (22%)]\tLoss: 0.462673\n",
      "Train Epoch: 0 [108640/482500 (23%)]\tLoss: 0.847328\n",
      "Train Epoch: 0 [108800/482500 (23%)]\tLoss: 0.876524\n",
      "Train Epoch: 0 [108960/482500 (23%)]\tLoss: 0.575395\n",
      "Train Epoch: 0 [109120/482500 (23%)]\tLoss: 0.558722\n",
      "Train Epoch: 0 [109280/482500 (23%)]\tLoss: 0.540170\n",
      "Train Epoch: 0 [109440/482500 (23%)]\tLoss: 0.507140\n",
      "Train Epoch: 0 [109600/482500 (23%)]\tLoss: 0.489979\n",
      "Train Epoch: 0 [109760/482500 (23%)]\tLoss: 2.370609\n",
      "Train Epoch: 0 [109920/482500 (23%)]\tLoss: 1.773504\n",
      "Train Epoch: 0 [110080/482500 (23%)]\tLoss: 0.621908\n",
      "Train Epoch: 0 [110240/482500 (23%)]\tLoss: 1.065608\n",
      "Train Epoch: 0 [110400/482500 (23%)]\tLoss: 1.158473\n",
      "Train Epoch: 0 [110560/482500 (23%)]\tLoss: 0.928833\n",
      "Train Epoch: 0 [110720/482500 (23%)]\tLoss: 1.133590\n",
      "Train Epoch: 0 [110880/482500 (23%)]\tLoss: 0.678330\n",
      "Train Epoch: 0 [111040/482500 (23%)]\tLoss: 0.986439\n",
      "Train Epoch: 0 [111200/482500 (23%)]\tLoss: 3.017575\n",
      "Train Epoch: 0 [111360/482500 (23%)]\tLoss: 0.652494\n",
      "Train Epoch: 0 [111520/482500 (23%)]\tLoss: 0.515459\n",
      "Train Epoch: 0 [111680/482500 (23%)]\tLoss: 0.683880\n",
      "Train Epoch: 0 [111840/482500 (23%)]\tLoss: 0.416704\n",
      "Train Epoch: 0 [112000/482500 (23%)]\tLoss: 0.615036\n",
      "Train Epoch: 0 [112160/482500 (23%)]\tLoss: 0.543648\n",
      "Train Epoch: 0 [112320/482500 (23%)]\tLoss: 0.620174\n",
      "Train Epoch: 0 [112480/482500 (23%)]\tLoss: 0.998766\n",
      "Train Epoch: 0 [112640/482500 (23%)]\tLoss: 0.910884\n",
      "Train Epoch: 0 [112800/482500 (23%)]\tLoss: 1.065720\n",
      "Train Epoch: 0 [112960/482500 (23%)]\tLoss: 0.750449\n",
      "Train Epoch: 0 [113120/482500 (23%)]\tLoss: 0.483130\n",
      "Train Epoch: 0 [113280/482500 (23%)]\tLoss: 0.509409\n",
      "Train Epoch: 0 [113440/482500 (24%)]\tLoss: 0.347722\n",
      "Train Epoch: 0 [113600/482500 (24%)]\tLoss: 0.391450\n",
      "Train Epoch: 0 [113760/482500 (24%)]\tLoss: 0.449778\n",
      "Train Epoch: 0 [113920/482500 (24%)]\tLoss: 0.549534\n",
      "Train Epoch: 0 [114080/482500 (24%)]\tLoss: 0.665022\n",
      "Train Epoch: 0 [114240/482500 (24%)]\tLoss: 0.528458\n",
      "Train Epoch: 0 [114400/482500 (24%)]\tLoss: 0.900691\n",
      "Train Epoch: 0 [114560/482500 (24%)]\tLoss: 0.420704\n",
      "Train Epoch: 0 [114720/482500 (24%)]\tLoss: 0.455568\n",
      "Train Epoch: 0 [114880/482500 (24%)]\tLoss: 0.904621\n",
      "Train Epoch: 0 [115040/482500 (24%)]\tLoss: 0.612483\n",
      "Train Epoch: 0 [115200/482500 (24%)]\tLoss: 0.492685\n",
      "Train Epoch: 0 [115360/482500 (24%)]\tLoss: 0.558732\n",
      "Train Epoch: 0 [115520/482500 (24%)]\tLoss: 0.421944\n",
      "Train Epoch: 0 [115680/482500 (24%)]\tLoss: 1.543215\n",
      "Train Epoch: 0 [115840/482500 (24%)]\tLoss: 0.507018\n",
      "Train Epoch: 0 [116000/482500 (24%)]\tLoss: 0.599451\n",
      "Train Epoch: 0 [116160/482500 (24%)]\tLoss: 4.897421\n",
      "Train Epoch: 0 [116320/482500 (24%)]\tLoss: 4.985476\n",
      "Train Epoch: 0 [116480/482500 (24%)]\tLoss: 1.673682\n",
      "Train Epoch: 0 [116640/482500 (24%)]\tLoss: 1.530901\n",
      "Train Epoch: 0 [116800/482500 (24%)]\tLoss: 0.921929\n",
      "Train Epoch: 0 [116960/482500 (24%)]\tLoss: 0.794423\n",
      "Train Epoch: 0 [117120/482500 (24%)]\tLoss: 108.081696\n",
      "Train Epoch: 0 [117280/482500 (24%)]\tLoss: 12.901527\n",
      "Train Epoch: 0 [117440/482500 (24%)]\tLoss: 2.201329\n",
      "Train Epoch: 0 [117600/482500 (24%)]\tLoss: 0.726640\n",
      "Train Epoch: 0 [117760/482500 (24%)]\tLoss: 1.205323\n",
      "Train Epoch: 0 [117920/482500 (24%)]\tLoss: 1.081744\n",
      "Train Epoch: 0 [118080/482500 (24%)]\tLoss: 0.926333\n",
      "Train Epoch: 0 [118240/482500 (25%)]\tLoss: 4.927938\n",
      "Train Epoch: 0 [118400/482500 (25%)]\tLoss: 4.038934\n",
      "Train Epoch: 0 [118560/482500 (25%)]\tLoss: 1.254619\n",
      "Train Epoch: 0 [118720/482500 (25%)]\tLoss: 9.828919\n",
      "Train Epoch: 0 [118880/482500 (25%)]\tLoss: 2.837636\n",
      "Train Epoch: 0 [119040/482500 (25%)]\tLoss: 2.160278\n",
      "Train Epoch: 0 [119200/482500 (25%)]\tLoss: 2.537629\n",
      "Train Epoch: 0 [119360/482500 (25%)]\tLoss: 2.149034\n",
      "Train Epoch: 0 [119520/482500 (25%)]\tLoss: 1.006624\n",
      "Train Epoch: 0 [119680/482500 (25%)]\tLoss: 0.616297\n",
      "Train Epoch: 0 [119840/482500 (25%)]\tLoss: 0.401722\n",
      "Train Epoch: 0 [120000/482500 (25%)]\tLoss: 1.484977\n",
      "Train Epoch: 0 [120160/482500 (25%)]\tLoss: 0.654830\n",
      "Train Epoch: 0 [120320/482500 (25%)]\tLoss: 19.983866\n",
      "Train Epoch: 0 [120480/482500 (25%)]\tLoss: 113.127968\n",
      "Train Epoch: 0 [120640/482500 (25%)]\tLoss: 1.465754\n",
      "Train Epoch: 0 [120800/482500 (25%)]\tLoss: 1.415221\n",
      "Train Epoch: 0 [120960/482500 (25%)]\tLoss: 0.592042\n",
      "Train Epoch: 0 [121120/482500 (25%)]\tLoss: 1.062368\n",
      "Train Epoch: 0 [121280/482500 (25%)]\tLoss: 1.184117\n",
      "Train Epoch: 0 [121440/482500 (25%)]\tLoss: 2.048404\n",
      "Train Epoch: 0 [121600/482500 (25%)]\tLoss: 0.467258\n",
      "Train Epoch: 0 [121760/482500 (25%)]\tLoss: 1.413682\n",
      "Train Epoch: 0 [121920/482500 (25%)]\tLoss: 0.803539\n",
      "Train Epoch: 0 [122080/482500 (25%)]\tLoss: 0.397868\n",
      "Train Epoch: 0 [122240/482500 (25%)]\tLoss: 1.778335\n",
      "Train Epoch: 0 [122400/482500 (25%)]\tLoss: 1.214679\n",
      "Train Epoch: 0 [122560/482500 (25%)]\tLoss: 1.501584\n",
      "Train Epoch: 0 [122720/482500 (25%)]\tLoss: 0.610010\n",
      "Train Epoch: 0 [122880/482500 (25%)]\tLoss: 0.640816\n",
      "Train Epoch: 0 [123040/482500 (26%)]\tLoss: 0.664192\n",
      "Train Epoch: 0 [123200/482500 (26%)]\tLoss: 0.431366\n",
      "Train Epoch: 0 [123360/482500 (26%)]\tLoss: 0.686124\n",
      "Train Epoch: 0 [123520/482500 (26%)]\tLoss: 0.706421\n",
      "Train Epoch: 0 [123680/482500 (26%)]\tLoss: 0.929346\n",
      "Train Epoch: 0 [123840/482500 (26%)]\tLoss: 0.401105\n",
      "Train Epoch: 0 [124000/482500 (26%)]\tLoss: 1.345056\n",
      "Train Epoch: 0 [124160/482500 (26%)]\tLoss: 0.738112\n",
      "Train Epoch: 0 [124320/482500 (26%)]\tLoss: 0.573925\n",
      "Train Epoch: 0 [124480/482500 (26%)]\tLoss: 0.703262\n",
      "Train Epoch: 0 [124640/482500 (26%)]\tLoss: 0.374210\n",
      "Train Epoch: 0 [124800/482500 (26%)]\tLoss: 0.665416\n",
      "Train Epoch: 0 [124960/482500 (26%)]\tLoss: 0.494906\n",
      "Train Epoch: 0 [125120/482500 (26%)]\tLoss: 0.476134\n",
      "Train Epoch: 0 [125280/482500 (26%)]\tLoss: 1.088582\n",
      "Train Epoch: 0 [125440/482500 (26%)]\tLoss: 0.308437\n",
      "Train Epoch: 0 [125600/482500 (26%)]\tLoss: 0.562454\n",
      "Train Epoch: 0 [125760/482500 (26%)]\tLoss: 0.672271\n",
      "Train Epoch: 0 [125920/482500 (26%)]\tLoss: 0.415901\n",
      "Train Epoch: 0 [126080/482500 (26%)]\tLoss: 0.364895\n",
      "Train Epoch: 0 [126240/482500 (26%)]\tLoss: 0.588344\n",
      "Train Epoch: 0 [126400/482500 (26%)]\tLoss: 0.439026\n",
      "Train Epoch: 0 [126560/482500 (26%)]\tLoss: 0.637193\n",
      "Train Epoch: 0 [126720/482500 (26%)]\tLoss: 0.493366\n",
      "Train Epoch: 0 [126880/482500 (26%)]\tLoss: 0.629586\n",
      "Train Epoch: 0 [127040/482500 (26%)]\tLoss: 0.466577\n",
      "Train Epoch: 0 [127200/482500 (26%)]\tLoss: 26.745794\n",
      "Train Epoch: 0 [127360/482500 (26%)]\tLoss: 61.289383\n",
      "Train Epoch: 0 [127520/482500 (26%)]\tLoss: 15.070139\n",
      "Train Epoch: 0 [127680/482500 (26%)]\tLoss: 4.343231\n",
      "Train Epoch: 0 [127840/482500 (26%)]\tLoss: 3.494472\n",
      "Train Epoch: 0 [128000/482500 (27%)]\tLoss: 1.773020\n",
      "Train Epoch: 0 [128160/482500 (27%)]\tLoss: 2.020241\n",
      "Train Epoch: 0 [128320/482500 (27%)]\tLoss: 0.951287\n",
      "Train Epoch: 0 [128480/482500 (27%)]\tLoss: 1.187819\n",
      "Train Epoch: 0 [128640/482500 (27%)]\tLoss: 2.934768\n",
      "Train Epoch: 0 [128800/482500 (27%)]\tLoss: 0.985417\n",
      "Train Epoch: 0 [128960/482500 (27%)]\tLoss: 2.161974\n",
      "Train Epoch: 0 [129120/482500 (27%)]\tLoss: 1.904305\n",
      "Train Epoch: 0 [129280/482500 (27%)]\tLoss: 2.245132\n",
      "Train Epoch: 0 [129440/482500 (27%)]\tLoss: 2.142436\n",
      "Train Epoch: 0 [129600/482500 (27%)]\tLoss: 1.675051\n",
      "Train Epoch: 0 [129760/482500 (27%)]\tLoss: 1.190083\n",
      "Train Epoch: 0 [129920/482500 (27%)]\tLoss: 1.723217\n",
      "Train Epoch: 0 [130080/482500 (27%)]\tLoss: 4.344872\n",
      "Train Epoch: 0 [130240/482500 (27%)]\tLoss: 1.909617\n",
      "Train Epoch: 0 [130400/482500 (27%)]\tLoss: 0.871946\n",
      "Train Epoch: 0 [130560/482500 (27%)]\tLoss: 1.348498\n",
      "Train Epoch: 0 [130720/482500 (27%)]\tLoss: 1.463585\n",
      "Train Epoch: 0 [130880/482500 (27%)]\tLoss: 2.120108\n",
      "Train Epoch: 0 [131040/482500 (27%)]\tLoss: 0.665132\n",
      "Train Epoch: 0 [131200/482500 (27%)]\tLoss: 0.935723\n",
      "Train Epoch: 0 [131360/482500 (27%)]\tLoss: 11.233406\n",
      "Train Epoch: 0 [131520/482500 (27%)]\tLoss: 1.153785\n",
      "Train Epoch: 0 [131680/482500 (27%)]\tLoss: 1.605450\n",
      "Train Epoch: 0 [131840/482500 (27%)]\tLoss: 2.521328\n",
      "Train Epoch: 0 [132000/482500 (27%)]\tLoss: 1.074830\n",
      "Train Epoch: 0 [132160/482500 (27%)]\tLoss: 0.912363\n",
      "Train Epoch: 0 [132320/482500 (27%)]\tLoss: 0.934502\n",
      "Train Epoch: 0 [132480/482500 (27%)]\tLoss: 1.047006\n",
      "Train Epoch: 0 [132640/482500 (27%)]\tLoss: 1.111938\n",
      "Train Epoch: 0 [132800/482500 (28%)]\tLoss: 0.511511\n",
      "Train Epoch: 0 [132960/482500 (28%)]\tLoss: 0.597602\n",
      "Train Epoch: 0 [133120/482500 (28%)]\tLoss: 1.205907\n",
      "Train Epoch: 0 [133280/482500 (28%)]\tLoss: 0.963464\n",
      "Train Epoch: 0 [133440/482500 (28%)]\tLoss: 1.498308\n",
      "Train Epoch: 0 [133600/482500 (28%)]\tLoss: 0.732872\n",
      "Train Epoch: 0 [133760/482500 (28%)]\tLoss: 0.555660\n",
      "Train Epoch: 0 [133920/482500 (28%)]\tLoss: 0.801420\n",
      "Train Epoch: 0 [134080/482500 (28%)]\tLoss: 0.934048\n",
      "Train Epoch: 0 [134240/482500 (28%)]\tLoss: 54.034306\n",
      "Train Epoch: 0 [134400/482500 (28%)]\tLoss: 6.223317\n",
      "Train Epoch: 0 [134560/482500 (28%)]\tLoss: 8.305292\n",
      "Train Epoch: 0 [134720/482500 (28%)]\tLoss: 2.174726\n",
      "Train Epoch: 0 [134880/482500 (28%)]\tLoss: 2.016500\n",
      "Train Epoch: 0 [135040/482500 (28%)]\tLoss: 3.345922\n",
      "Train Epoch: 0 [135200/482500 (28%)]\tLoss: 1.498099\n",
      "Train Epoch: 0 [135360/482500 (28%)]\tLoss: 1.964117\n",
      "Train Epoch: 0 [135520/482500 (28%)]\tLoss: 3.682905\n",
      "Train Epoch: 0 [135680/482500 (28%)]\tLoss: 1.572424\n",
      "Train Epoch: 0 [135840/482500 (28%)]\tLoss: 1.738981\n",
      "Train Epoch: 0 [136000/482500 (28%)]\tLoss: 1.048466\n",
      "Train Epoch: 0 [136160/482500 (28%)]\tLoss: 1.670517\n",
      "Train Epoch: 0 [136320/482500 (28%)]\tLoss: 2.095732\n",
      "Train Epoch: 0 [136480/482500 (28%)]\tLoss: 1.032714\n",
      "Train Epoch: 0 [136640/482500 (28%)]\tLoss: 1.962441\n",
      "Train Epoch: 0 [136800/482500 (28%)]\tLoss: 1.867563\n",
      "Train Epoch: 0 [136960/482500 (28%)]\tLoss: 1.616691\n",
      "Train Epoch: 0 [137120/482500 (28%)]\tLoss: 1.935028\n",
      "Train Epoch: 0 [137280/482500 (28%)]\tLoss: 1.162399\n",
      "Train Epoch: 0 [137440/482500 (28%)]\tLoss: 1.194592\n",
      "Train Epoch: 0 [137600/482500 (29%)]\tLoss: 1.858318\n",
      "Train Epoch: 0 [137760/482500 (29%)]\tLoss: 4.791242\n",
      "Train Epoch: 0 [137920/482500 (29%)]\tLoss: 2.676602\n",
      "Train Epoch: 0 [138080/482500 (29%)]\tLoss: 1.189726\n",
      "Train Epoch: 0 [138240/482500 (29%)]\tLoss: 0.934430\n",
      "Train Epoch: 0 [138400/482500 (29%)]\tLoss: 2.907627\n",
      "Train Epoch: 0 [138560/482500 (29%)]\tLoss: 0.560714\n",
      "Train Epoch: 0 [138720/482500 (29%)]\tLoss: 1.199373\n",
      "Train Epoch: 0 [138880/482500 (29%)]\tLoss: 1.065875\n",
      "Train Epoch: 0 [139040/482500 (29%)]\tLoss: 0.580093\n",
      "Train Epoch: 0 [139200/482500 (29%)]\tLoss: 1.824645\n",
      "Train Epoch: 0 [139360/482500 (29%)]\tLoss: 0.645882\n",
      "Train Epoch: 0 [139520/482500 (29%)]\tLoss: 0.893382\n",
      "Train Epoch: 0 [139680/482500 (29%)]\tLoss: 0.667547\n",
      "Train Epoch: 0 [139840/482500 (29%)]\tLoss: 1.193782\n",
      "Train Epoch: 0 [140000/482500 (29%)]\tLoss: 0.798680\n",
      "Train Epoch: 0 [140160/482500 (29%)]\tLoss: 0.874223\n",
      "Train Epoch: 0 [140320/482500 (29%)]\tLoss: 0.775496\n",
      "Train Epoch: 0 [140480/482500 (29%)]\tLoss: 1.310785\n",
      "Train Epoch: 0 [140640/482500 (29%)]\tLoss: 2.364497\n",
      "Train Epoch: 0 [140800/482500 (29%)]\tLoss: 1.005818\n",
      "Train Epoch: 0 [140960/482500 (29%)]\tLoss: 0.607024\n",
      "Train Epoch: 0 [141120/482500 (29%)]\tLoss: 0.752279\n",
      "Train Epoch: 0 [141280/482500 (29%)]\tLoss: 0.844319\n",
      "Train Epoch: 0 [141440/482500 (29%)]\tLoss: 0.829091\n",
      "Train Epoch: 0 [141600/482500 (29%)]\tLoss: 0.595660\n",
      "Train Epoch: 0 [141760/482500 (29%)]\tLoss: 0.498372\n",
      "Train Epoch: 0 [141920/482500 (29%)]\tLoss: 0.983100\n",
      "Train Epoch: 0 [142080/482500 (29%)]\tLoss: 0.683843\n",
      "Train Epoch: 0 [142240/482500 (29%)]\tLoss: 0.545088\n",
      "Train Epoch: 0 [142400/482500 (30%)]\tLoss: 0.707695\n",
      "Train Epoch: 0 [142560/482500 (30%)]\tLoss: 0.925070\n",
      "Train Epoch: 0 [142720/482500 (30%)]\tLoss: 0.772103\n",
      "Train Epoch: 0 [142880/482500 (30%)]\tLoss: 0.852834\n",
      "Train Epoch: 0 [143040/482500 (30%)]\tLoss: 0.602506\n",
      "Train Epoch: 0 [143200/482500 (30%)]\tLoss: 1.033997\n",
      "Train Epoch: 0 [143360/482500 (30%)]\tLoss: 0.674561\n",
      "Train Epoch: 0 [143520/482500 (30%)]\tLoss: 0.651323\n",
      "Train Epoch: 0 [143680/482500 (30%)]\tLoss: 0.780406\n",
      "Train Epoch: 0 [143840/482500 (30%)]\tLoss: 0.750129\n",
      "Train Epoch: 0 [144000/482500 (30%)]\tLoss: 0.589253\n",
      "Train Epoch: 0 [144160/482500 (30%)]\tLoss: 0.512699\n",
      "Train Epoch: 0 [144320/482500 (30%)]\tLoss: 1.216751\n",
      "Train Epoch: 0 [144480/482500 (30%)]\tLoss: 0.618305\n",
      "Train Epoch: 0 [144640/482500 (30%)]\tLoss: 1.316903\n",
      "Train Epoch: 0 [144800/482500 (30%)]\tLoss: 103.010536\n",
      "Train Epoch: 0 [144960/482500 (30%)]\tLoss: 9.414195\n",
      "Train Epoch: 0 [145120/482500 (30%)]\tLoss: 3.810997\n",
      "Train Epoch: 0 [145280/482500 (30%)]\tLoss: 64.284386\n",
      "Train Epoch: 0 [145440/482500 (30%)]\tLoss: 7.860931\n",
      "Train Epoch: 0 [145600/482500 (30%)]\tLoss: 3.481148\n",
      "Train Epoch: 0 [145760/482500 (30%)]\tLoss: 2.755273\n",
      "Train Epoch: 0 [145920/482500 (30%)]\tLoss: 1.739719\n",
      "Train Epoch: 0 [146080/482500 (30%)]\tLoss: 50.398441\n",
      "Train Epoch: 0 [146240/482500 (30%)]\tLoss: 4.584338\n",
      "Train Epoch: 0 [146400/482500 (30%)]\tLoss: 1.186952\n",
      "Train Epoch: 0 [146560/482500 (30%)]\tLoss: 1.837849\n",
      "Train Epoch: 0 [146720/482500 (30%)]\tLoss: 1.567245\n",
      "Train Epoch: 0 [146880/482500 (30%)]\tLoss: 0.885875\n",
      "Train Epoch: 0 [147040/482500 (30%)]\tLoss: 0.897688\n",
      "Train Epoch: 0 [147200/482500 (31%)]\tLoss: 0.993428\n",
      "Train Epoch: 0 [147360/482500 (31%)]\tLoss: 1.058390\n",
      "Train Epoch: 0 [147520/482500 (31%)]\tLoss: 15.539319\n",
      "Train Epoch: 0 [147680/482500 (31%)]\tLoss: 4.662734\n",
      "Train Epoch: 0 [147840/482500 (31%)]\tLoss: 3.600368\n",
      "Train Epoch: 0 [148000/482500 (31%)]\tLoss: 1.110599\n",
      "Train Epoch: 0 [148160/482500 (31%)]\tLoss: 0.730189\n",
      "Train Epoch: 0 [148320/482500 (31%)]\tLoss: 1.265068\n",
      "Train Epoch: 0 [148480/482500 (31%)]\tLoss: 0.898646\n",
      "Train Epoch: 0 [148640/482500 (31%)]\tLoss: 2.339704\n",
      "Train Epoch: 0 [148800/482500 (31%)]\tLoss: 0.522429\n",
      "Train Epoch: 0 [148960/482500 (31%)]\tLoss: 1.208736\n",
      "Train Epoch: 0 [149120/482500 (31%)]\tLoss: 0.855539\n",
      "Train Epoch: 0 [149280/482500 (31%)]\tLoss: 0.778991\n",
      "Train Epoch: 0 [149440/482500 (31%)]\tLoss: 0.896774\n",
      "Train Epoch: 0 [149600/482500 (31%)]\tLoss: 1.114540\n",
      "Train Epoch: 0 [149760/482500 (31%)]\tLoss: 1.225110\n",
      "Train Epoch: 0 [149920/482500 (31%)]\tLoss: 0.620600\n",
      "Train Epoch: 0 [150080/482500 (31%)]\tLoss: 1.044429\n",
      "Train Epoch: 0 [150240/482500 (31%)]\tLoss: 0.521098\n",
      "Train Epoch: 0 [150400/482500 (31%)]\tLoss: 1.150084\n",
      "Train Epoch: 0 [150560/482500 (31%)]\tLoss: 0.815979\n",
      "Train Epoch: 0 [150720/482500 (31%)]\tLoss: 0.550388\n",
      "Train Epoch: 0 [150880/482500 (31%)]\tLoss: 1.215612\n",
      "Train Epoch: 0 [151040/482500 (31%)]\tLoss: 1.413715\n",
      "Train Epoch: 0 [151200/482500 (31%)]\tLoss: 1.712170\n",
      "Train Epoch: 0 [151360/482500 (31%)]\tLoss: 1.191472\n",
      "Train Epoch: 0 [151520/482500 (31%)]\tLoss: 0.893549\n",
      "Train Epoch: 0 [151680/482500 (31%)]\tLoss: 0.866489\n",
      "Train Epoch: 0 [151840/482500 (31%)]\tLoss: 0.795148\n",
      "Train Epoch: 0 [152000/482500 (32%)]\tLoss: 0.972324\n",
      "Train Epoch: 0 [152160/482500 (32%)]\tLoss: 0.689818\n",
      "Train Epoch: 0 [152320/482500 (32%)]\tLoss: 0.723256\n",
      "Train Epoch: 0 [152480/482500 (32%)]\tLoss: 0.580554\n",
      "Train Epoch: 0 [152640/482500 (32%)]\tLoss: 1.060679\n",
      "Train Epoch: 0 [152800/482500 (32%)]\tLoss: 0.884148\n",
      "Train Epoch: 0 [152960/482500 (32%)]\tLoss: 0.490308\n",
      "Train Epoch: 0 [153120/482500 (32%)]\tLoss: 0.459649\n",
      "Train Epoch: 0 [153280/482500 (32%)]\tLoss: 0.646870\n",
      "Train Epoch: 0 [153440/482500 (32%)]\tLoss: 1.193692\n",
      "Train Epoch: 0 [153600/482500 (32%)]\tLoss: 10.700259\n",
      "Train Epoch: 0 [153760/482500 (32%)]\tLoss: 1.406263\n",
      "Train Epoch: 0 [153920/482500 (32%)]\tLoss: 0.761058\n",
      "Train Epoch: 0 [154080/482500 (32%)]\tLoss: 0.816640\n",
      "Train Epoch: 0 [154240/482500 (32%)]\tLoss: 0.719519\n",
      "Train Epoch: 0 [154400/482500 (32%)]\tLoss: 0.916593\n",
      "Train Epoch: 0 [154560/482500 (32%)]\tLoss: 0.787957\n",
      "Train Epoch: 0 [154720/482500 (32%)]\tLoss: 0.981741\n",
      "Train Epoch: 0 [154880/482500 (32%)]\tLoss: 0.639495\n",
      "Train Epoch: 0 [155040/482500 (32%)]\tLoss: 0.709509\n",
      "Train Epoch: 0 [155200/482500 (32%)]\tLoss: 0.589155\n",
      "Train Epoch: 0 [155360/482500 (32%)]\tLoss: 0.562745\n",
      "Train Epoch: 0 [155520/482500 (32%)]\tLoss: 0.610604\n",
      "Train Epoch: 0 [155680/482500 (32%)]\tLoss: 0.619182\n",
      "Train Epoch: 0 [155840/482500 (32%)]\tLoss: 0.588738\n",
      "Train Epoch: 0 [156000/482500 (32%)]\tLoss: 1.182353\n",
      "Train Epoch: 0 [156160/482500 (32%)]\tLoss: 0.646332\n",
      "Train Epoch: 0 [156320/482500 (32%)]\tLoss: 1.265178\n",
      "Train Epoch: 0 [156480/482500 (32%)]\tLoss: 2.190435\n",
      "Train Epoch: 0 [156640/482500 (32%)]\tLoss: 1.273853\n",
      "Train Epoch: 0 [156800/482500 (32%)]\tLoss: 1.037038\n",
      "Train Epoch: 0 [156960/482500 (33%)]\tLoss: 0.979418\n",
      "Train Epoch: 0 [157120/482500 (33%)]\tLoss: 0.793416\n",
      "Train Epoch: 0 [157280/482500 (33%)]\tLoss: 0.538580\n",
      "Train Epoch: 0 [157440/482500 (33%)]\tLoss: 1.218457\n",
      "Train Epoch: 0 [157600/482500 (33%)]\tLoss: 0.532473\n",
      "Train Epoch: 0 [157760/482500 (33%)]\tLoss: 0.710587\n",
      "Train Epoch: 0 [157920/482500 (33%)]\tLoss: 0.849509\n",
      "Train Epoch: 0 [158080/482500 (33%)]\tLoss: 0.835811\n",
      "Train Epoch: 0 [158240/482500 (33%)]\tLoss: 1.191740\n",
      "Train Epoch: 0 [158400/482500 (33%)]\tLoss: 0.694343\n",
      "Train Epoch: 0 [158560/482500 (33%)]\tLoss: 0.596019\n",
      "Train Epoch: 0 [158720/482500 (33%)]\tLoss: 0.565232\n",
      "Train Epoch: 0 [158880/482500 (33%)]\tLoss: 0.631717\n",
      "Train Epoch: 0 [159040/482500 (33%)]\tLoss: 0.672777\n",
      "Train Epoch: 0 [159200/482500 (33%)]\tLoss: 0.727160\n",
      "Train Epoch: 0 [159360/482500 (33%)]\tLoss: 0.558973\n",
      "Train Epoch: 0 [159520/482500 (33%)]\tLoss: 0.412083\n",
      "Train Epoch: 0 [159680/482500 (33%)]\tLoss: 0.427583\n",
      "Train Epoch: 0 [159840/482500 (33%)]\tLoss: 0.533172\n",
      "Train Epoch: 0 [160000/482500 (33%)]\tLoss: 0.785864\n",
      "Train Epoch: 0 [160160/482500 (33%)]\tLoss: 0.989807\n",
      "Train Epoch: 0 [160320/482500 (33%)]\tLoss: 0.482615\n",
      "Train Epoch: 0 [160480/482500 (33%)]\tLoss: 8.326590\n",
      "Train Epoch: 0 [160640/482500 (33%)]\tLoss: 1.134269\n",
      "Train Epoch: 0 [160800/482500 (33%)]\tLoss: 1.005807\n",
      "Train Epoch: 0 [160960/482500 (33%)]\tLoss: 0.900521\n",
      "Train Epoch: 0 [161120/482500 (33%)]\tLoss: 1.245034\n",
      "Train Epoch: 0 [161280/482500 (33%)]\tLoss: 0.861839\n",
      "Train Epoch: 0 [161440/482500 (33%)]\tLoss: 3.356067\n",
      "Train Epoch: 0 [161600/482500 (33%)]\tLoss: 1.395825\n",
      "Train Epoch: 0 [161760/482500 (34%)]\tLoss: 1.001627\n",
      "Train Epoch: 0 [161920/482500 (34%)]\tLoss: 0.587856\n",
      "Train Epoch: 0 [162080/482500 (34%)]\tLoss: 1.117324\n",
      "Train Epoch: 0 [162240/482500 (34%)]\tLoss: 1.008644\n",
      "Train Epoch: 0 [162400/482500 (34%)]\tLoss: 0.801356\n",
      "Train Epoch: 0 [162560/482500 (34%)]\tLoss: 0.584040\n",
      "Train Epoch: 0 [162720/482500 (34%)]\tLoss: 0.422051\n",
      "Train Epoch: 0 [162880/482500 (34%)]\tLoss: 0.495898\n",
      "Train Epoch: 0 [163040/482500 (34%)]\tLoss: 0.597149\n",
      "Train Epoch: 0 [163200/482500 (34%)]\tLoss: 0.493293\n",
      "Train Epoch: 0 [163360/482500 (34%)]\tLoss: 2.610235\n",
      "Train Epoch: 0 [163520/482500 (34%)]\tLoss: 0.542886\n",
      "Train Epoch: 0 [163680/482500 (34%)]\tLoss: 0.907899\n",
      "Train Epoch: 0 [163840/482500 (34%)]\tLoss: 0.931992\n",
      "Train Epoch: 0 [164000/482500 (34%)]\tLoss: 0.770213\n",
      "Train Epoch: 0 [164160/482500 (34%)]\tLoss: 0.695747\n",
      "Train Epoch: 0 [164320/482500 (34%)]\tLoss: 0.509682\n",
      "Train Epoch: 0 [164480/482500 (34%)]\tLoss: 0.522547\n",
      "Train Epoch: 0 [164640/482500 (34%)]\tLoss: 0.796130\n",
      "Train Epoch: 0 [164800/482500 (34%)]\tLoss: 0.431946\n",
      "Train Epoch: 0 [164960/482500 (34%)]\tLoss: 1.061558\n",
      "Train Epoch: 0 [165120/482500 (34%)]\tLoss: 0.560422\n",
      "Train Epoch: 0 [165280/482500 (34%)]\tLoss: 0.932645\n",
      "Train Epoch: 0 [165440/482500 (34%)]\tLoss: 0.646279\n",
      "Train Epoch: 0 [165600/482500 (34%)]\tLoss: 0.494495\n",
      "Train Epoch: 0 [165760/482500 (34%)]\tLoss: 0.516506\n",
      "Train Epoch: 0 [165920/482500 (34%)]\tLoss: 0.436142\n",
      "Train Epoch: 0 [166080/482500 (34%)]\tLoss: 0.768403\n",
      "Train Epoch: 0 [166240/482500 (34%)]\tLoss: 0.409380\n",
      "Train Epoch: 0 [166400/482500 (34%)]\tLoss: 0.381444\n",
      "Train Epoch: 0 [166560/482500 (35%)]\tLoss: 0.435787\n",
      "Train Epoch: 0 [166720/482500 (35%)]\tLoss: 0.630720\n",
      "Train Epoch: 0 [166880/482500 (35%)]\tLoss: 0.250591\n",
      "Train Epoch: 0 [167040/482500 (35%)]\tLoss: 3.158447\n",
      "Train Epoch: 0 [167200/482500 (35%)]\tLoss: 0.387357\n",
      "Train Epoch: 0 [167360/482500 (35%)]\tLoss: 0.569354\n",
      "Train Epoch: 0 [167520/482500 (35%)]\tLoss: 0.437699\n",
      "Train Epoch: 0 [167680/482500 (35%)]\tLoss: 0.511277\n",
      "Train Epoch: 0 [167840/482500 (35%)]\tLoss: 0.380946\n",
      "Train Epoch: 0 [168000/482500 (35%)]\tLoss: 0.531209\n",
      "Train Epoch: 0 [168160/482500 (35%)]\tLoss: 0.447145\n",
      "Train Epoch: 0 [168320/482500 (35%)]\tLoss: 1.484497\n",
      "Train Epoch: 0 [168480/482500 (35%)]\tLoss: 0.620742\n",
      "Train Epoch: 0 [168640/482500 (35%)]\tLoss: 0.594445\n",
      "Train Epoch: 0 [168800/482500 (35%)]\tLoss: 0.333890\n",
      "Train Epoch: 0 [168960/482500 (35%)]\tLoss: 0.587916\n",
      "Train Epoch: 0 [169120/482500 (35%)]\tLoss: 0.414205\n",
      "Train Epoch: 0 [169280/482500 (35%)]\tLoss: 0.542736\n",
      "Train Epoch: 0 [169440/482500 (35%)]\tLoss: 2.129712\n",
      "Train Epoch: 0 [169600/482500 (35%)]\tLoss: 3.463455\n",
      "Train Epoch: 0 [169760/482500 (35%)]\tLoss: 0.707389\n",
      "Train Epoch: 0 [169920/482500 (35%)]\tLoss: 0.745482\n",
      "Train Epoch: 0 [170080/482500 (35%)]\tLoss: 4.311742\n",
      "Train Epoch: 0 [170240/482500 (35%)]\tLoss: 0.848059\n",
      "Train Epoch: 0 [170400/482500 (35%)]\tLoss: 0.566671\n",
      "Train Epoch: 0 [170560/482500 (35%)]\tLoss: 0.364549\n",
      "Train Epoch: 0 [170720/482500 (35%)]\tLoss: 0.344099\n",
      "Train Epoch: 0 [170880/482500 (35%)]\tLoss: 0.545772\n",
      "Train Epoch: 0 [171040/482500 (35%)]\tLoss: 0.407889\n",
      "Train Epoch: 0 [171200/482500 (35%)]\tLoss: 0.620857\n",
      "Train Epoch: 0 [171360/482500 (36%)]\tLoss: 0.325431\n",
      "Train Epoch: 0 [171520/482500 (36%)]\tLoss: 26.849197\n",
      "Train Epoch: 0 [171680/482500 (36%)]\tLoss: 0.768706\n",
      "Train Epoch: 0 [171840/482500 (36%)]\tLoss: 0.505705\n",
      "Train Epoch: 0 [172000/482500 (36%)]\tLoss: 0.562132\n",
      "Train Epoch: 0 [172160/482500 (36%)]\tLoss: 0.639624\n",
      "Train Epoch: 0 [172320/482500 (36%)]\tLoss: 0.769861\n",
      "Train Epoch: 0 [172480/482500 (36%)]\tLoss: 0.598742\n",
      "Train Epoch: 0 [172640/482500 (36%)]\tLoss: 0.382425\n",
      "Train Epoch: 0 [172800/482500 (36%)]\tLoss: 0.774595\n",
      "Train Epoch: 0 [172960/482500 (36%)]\tLoss: 0.345599\n",
      "Train Epoch: 0 [173120/482500 (36%)]\tLoss: 0.485263\n",
      "Train Epoch: 0 [173280/482500 (36%)]\tLoss: 0.404565\n",
      "Train Epoch: 0 [173440/482500 (36%)]\tLoss: 0.411386\n",
      "Train Epoch: 0 [173600/482500 (36%)]\tLoss: 0.749270\n",
      "Train Epoch: 0 [173760/482500 (36%)]\tLoss: 0.532025\n",
      "Train Epoch: 0 [173920/482500 (36%)]\tLoss: 0.863287\n",
      "Train Epoch: 0 [174080/482500 (36%)]\tLoss: 0.672086\n",
      "Train Epoch: 0 [174240/482500 (36%)]\tLoss: 0.633200\n",
      "Train Epoch: 0 [174400/482500 (36%)]\tLoss: 2.504504\n",
      "Train Epoch: 0 [174560/482500 (36%)]\tLoss: 1.053237\n",
      "Train Epoch: 0 [174720/482500 (36%)]\tLoss: 1.357565\n",
      "Train Epoch: 0 [174880/482500 (36%)]\tLoss: 0.453253\n",
      "Train Epoch: 0 [175040/482500 (36%)]\tLoss: 0.565075\n",
      "Train Epoch: 0 [175200/482500 (36%)]\tLoss: 27.557457\n",
      "Train Epoch: 0 [175360/482500 (36%)]\tLoss: 3.884977\n",
      "Train Epoch: 0 [175520/482500 (36%)]\tLoss: 0.681951\n",
      "Train Epoch: 0 [175680/482500 (36%)]\tLoss: 0.794254\n",
      "Train Epoch: 0 [175840/482500 (36%)]\tLoss: 0.698125\n",
      "Train Epoch: 0 [176000/482500 (36%)]\tLoss: 1.400886\n",
      "Train Epoch: 0 [176160/482500 (37%)]\tLoss: 0.425451\n",
      "Train Epoch: 0 [176320/482500 (37%)]\tLoss: 0.493051\n",
      "Train Epoch: 0 [176480/482500 (37%)]\tLoss: 0.500458\n",
      "Train Epoch: 0 [176640/482500 (37%)]\tLoss: 0.454873\n",
      "Train Epoch: 0 [176800/482500 (37%)]\tLoss: 0.773280\n",
      "Train Epoch: 0 [176960/482500 (37%)]\tLoss: 0.461622\n",
      "Train Epoch: 0 [177120/482500 (37%)]\tLoss: 0.464246\n",
      "Train Epoch: 0 [177280/482500 (37%)]\tLoss: 0.547224\n",
      "Train Epoch: 0 [177440/482500 (37%)]\tLoss: 2.672201\n",
      "Train Epoch: 0 [177600/482500 (37%)]\tLoss: 0.499327\n",
      "Train Epoch: 0 [177760/482500 (37%)]\tLoss: 0.332655\n",
      "Train Epoch: 0 [177920/482500 (37%)]\tLoss: 0.429539\n",
      "Train Epoch: 0 [178080/482500 (37%)]\tLoss: 0.585937\n",
      "Train Epoch: 0 [178240/482500 (37%)]\tLoss: 0.677109\n",
      "Train Epoch: 0 [178400/482500 (37%)]\tLoss: 0.435609\n",
      "Train Epoch: 0 [178560/482500 (37%)]\tLoss: 0.421190\n",
      "Train Epoch: 0 [178720/482500 (37%)]\tLoss: 0.732667\n",
      "Train Epoch: 0 [178880/482500 (37%)]\tLoss: 0.625663\n",
      "Train Epoch: 0 [179040/482500 (37%)]\tLoss: 0.605234\n",
      "Train Epoch: 0 [179200/482500 (37%)]\tLoss: 0.349915\n",
      "Train Epoch: 0 [179360/482500 (37%)]\tLoss: 1.392626\n",
      "Train Epoch: 0 [179520/482500 (37%)]\tLoss: 0.457636\n",
      "Train Epoch: 0 [179680/482500 (37%)]\tLoss: 0.465873\n",
      "Train Epoch: 0 [179840/482500 (37%)]\tLoss: 1.018239\n",
      "Train Epoch: 0 [180000/482500 (37%)]\tLoss: 0.342064\n",
      "Train Epoch: 0 [180160/482500 (37%)]\tLoss: 0.432591\n",
      "Train Epoch: 0 [180320/482500 (37%)]\tLoss: 0.613443\n",
      "Train Epoch: 0 [180480/482500 (37%)]\tLoss: 1.026139\n",
      "Train Epoch: 0 [180640/482500 (37%)]\tLoss: 0.368640\n",
      "Train Epoch: 0 [180800/482500 (37%)]\tLoss: 0.452582\n",
      "Train Epoch: 0 [180960/482500 (38%)]\tLoss: 1.033167\n",
      "Train Epoch: 0 [181120/482500 (38%)]\tLoss: 0.649471\n",
      "Train Epoch: 0 [181280/482500 (38%)]\tLoss: 0.536885\n",
      "Train Epoch: 0 [181440/482500 (38%)]\tLoss: 0.394831\n",
      "Train Epoch: 0 [181600/482500 (38%)]\tLoss: 0.559542\n",
      "Train Epoch: 0 [181760/482500 (38%)]\tLoss: 1.132493\n",
      "Train Epoch: 0 [181920/482500 (38%)]\tLoss: 0.439930\n",
      "Train Epoch: 0 [182080/482500 (38%)]\tLoss: 0.797972\n",
      "Train Epoch: 0 [182240/482500 (38%)]\tLoss: 0.580729\n",
      "Train Epoch: 0 [182400/482500 (38%)]\tLoss: 0.567964\n",
      "Train Epoch: 0 [182560/482500 (38%)]\tLoss: 0.805217\n",
      "Train Epoch: 0 [182720/482500 (38%)]\tLoss: 0.473370\n",
      "Train Epoch: 0 [182880/482500 (38%)]\tLoss: 0.636172\n",
      "Train Epoch: 0 [183040/482500 (38%)]\tLoss: 0.432244\n",
      "Train Epoch: 0 [183200/482500 (38%)]\tLoss: 0.345133\n",
      "Train Epoch: 0 [183360/482500 (38%)]\tLoss: 53.259899\n",
      "Train Epoch: 0 [183520/482500 (38%)]\tLoss: 13.173718\n",
      "Train Epoch: 0 [183680/482500 (38%)]\tLoss: 45.331139\n",
      "Train Epoch: 0 [183840/482500 (38%)]\tLoss: 16.340887\n",
      "Train Epoch: 0 [184000/482500 (38%)]\tLoss: 3.093843\n",
      "Train Epoch: 0 [184160/482500 (38%)]\tLoss: 2.827538\n",
      "Train Epoch: 0 [184320/482500 (38%)]\tLoss: 2.544262\n",
      "Train Epoch: 0 [184480/482500 (38%)]\tLoss: 1.263466\n",
      "Train Epoch: 0 [184640/482500 (38%)]\tLoss: 2.452361\n",
      "Train Epoch: 0 [184800/482500 (38%)]\tLoss: 1.666379\n",
      "Train Epoch: 0 [184960/482500 (38%)]\tLoss: 3.180117\n",
      "Train Epoch: 0 [185120/482500 (38%)]\tLoss: 8.645118\n",
      "Train Epoch: 0 [185280/482500 (38%)]\tLoss: 3.804364\n",
      "Train Epoch: 0 [185440/482500 (38%)]\tLoss: 2.771507\n",
      "Train Epoch: 0 [185600/482500 (38%)]\tLoss: 1.948725\n",
      "Train Epoch: 0 [185760/482500 (38%)]\tLoss: 1.742907\n",
      "Train Epoch: 0 [185920/482500 (39%)]\tLoss: 2.178402\n",
      "Train Epoch: 0 [186080/482500 (39%)]\tLoss: 1.764207\n",
      "Train Epoch: 0 [186240/482500 (39%)]\tLoss: 1.439486\n",
      "Train Epoch: 0 [186400/482500 (39%)]\tLoss: 3.644204\n",
      "Train Epoch: 0 [186560/482500 (39%)]\tLoss: 1.870609\n",
      "Train Epoch: 0 [186720/482500 (39%)]\tLoss: 1.589941\n",
      "Train Epoch: 0 [186880/482500 (39%)]\tLoss: 1.353205\n",
      "Train Epoch: 0 [187040/482500 (39%)]\tLoss: 1.455063\n",
      "Train Epoch: 0 [187200/482500 (39%)]\tLoss: 1.256362\n",
      "Train Epoch: 0 [187360/482500 (39%)]\tLoss: 19.612499\n",
      "Train Epoch: 0 [187520/482500 (39%)]\tLoss: 5.500467\n",
      "Train Epoch: 0 [187680/482500 (39%)]\tLoss: 2.700849\n",
      "Train Epoch: 0 [187840/482500 (39%)]\tLoss: 1.267381\n",
      "Train Epoch: 0 [188000/482500 (39%)]\tLoss: 0.822123\n",
      "Train Epoch: 0 [188160/482500 (39%)]\tLoss: 1.108237\n",
      "Train Epoch: 0 [188320/482500 (39%)]\tLoss: 1.817346\n",
      "Train Epoch: 0 [188480/482500 (39%)]\tLoss: 0.922454\n",
      "Train Epoch: 0 [188640/482500 (39%)]\tLoss: 1.076842\n",
      "Train Epoch: 0 [188800/482500 (39%)]\tLoss: 1.026623\n",
      "Train Epoch: 0 [188960/482500 (39%)]\tLoss: 1.325194\n",
      "Train Epoch: 0 [189120/482500 (39%)]\tLoss: 1.681878\n",
      "Train Epoch: 0 [189280/482500 (39%)]\tLoss: 26.275635\n",
      "Train Epoch: 0 [189440/482500 (39%)]\tLoss: 1.024498\n",
      "Train Epoch: 0 [189600/482500 (39%)]\tLoss: 1.070914\n",
      "Train Epoch: 0 [189760/482500 (39%)]\tLoss: 0.879898\n",
      "Train Epoch: 0 [189920/482500 (39%)]\tLoss: 1.612037\n",
      "Train Epoch: 0 [190080/482500 (39%)]\tLoss: 1.017933\n",
      "Train Epoch: 0 [190240/482500 (39%)]\tLoss: 0.533083\n",
      "Train Epoch: 0 [190400/482500 (39%)]\tLoss: 1.017750\n",
      "Train Epoch: 0 [190560/482500 (39%)]\tLoss: 1.080855\n",
      "Train Epoch: 0 [190720/482500 (40%)]\tLoss: 0.969074\n",
      "Train Epoch: 0 [190880/482500 (40%)]\tLoss: 0.968371\n",
      "Train Epoch: 0 [191040/482500 (40%)]\tLoss: 0.753119\n",
      "Train Epoch: 0 [191200/482500 (40%)]\tLoss: 0.954578\n",
      "Train Epoch: 0 [191360/482500 (40%)]\tLoss: 0.938492\n",
      "Train Epoch: 0 [191520/482500 (40%)]\tLoss: 0.900866\n",
      "Train Epoch: 0 [191680/482500 (40%)]\tLoss: 0.796276\n",
      "Train Epoch: 0 [191840/482500 (40%)]\tLoss: 1.411566\n",
      "Train Epoch: 0 [192000/482500 (40%)]\tLoss: 0.720544\n",
      "Train Epoch: 0 [192160/482500 (40%)]\tLoss: 0.996737\n",
      "Train Epoch: 0 [192320/482500 (40%)]\tLoss: 2.208918\n",
      "Train Epoch: 0 [192480/482500 (40%)]\tLoss: 0.658627\n",
      "Train Epoch: 0 [192640/482500 (40%)]\tLoss: 0.582458\n",
      "Train Epoch: 0 [192800/482500 (40%)]\tLoss: 0.767621\n",
      "Train Epoch: 0 [192960/482500 (40%)]\tLoss: 1.106706\n",
      "Train Epoch: 0 [193120/482500 (40%)]\tLoss: 1.383497\n",
      "Train Epoch: 0 [193280/482500 (40%)]\tLoss: 0.987270\n",
      "Train Epoch: 0 [193440/482500 (40%)]\tLoss: 0.835786\n",
      "Train Epoch: 0 [193600/482500 (40%)]\tLoss: 1.178597\n",
      "Train Epoch: 0 [193760/482500 (40%)]\tLoss: 0.893670\n",
      "Train Epoch: 0 [193920/482500 (40%)]\tLoss: 0.569864\n",
      "Train Epoch: 0 [194080/482500 (40%)]\tLoss: 1.663773\n",
      "Train Epoch: 0 [194240/482500 (40%)]\tLoss: 0.909342\n",
      "Train Epoch: 0 [194400/482500 (40%)]\tLoss: 0.855390\n",
      "Train Epoch: 0 [194560/482500 (40%)]\tLoss: 0.958146\n",
      "Train Epoch: 0 [194720/482500 (40%)]\tLoss: 0.904230\n",
      "Train Epoch: 0 [194880/482500 (40%)]\tLoss: 0.699393\n",
      "Train Epoch: 0 [195040/482500 (40%)]\tLoss: 0.916777\n",
      "Train Epoch: 0 [195200/482500 (40%)]\tLoss: 0.763232\n",
      "Train Epoch: 0 [195360/482500 (40%)]\tLoss: 0.775735\n",
      "Train Epoch: 0 [195520/482500 (41%)]\tLoss: 0.470883\n",
      "Train Epoch: 0 [195680/482500 (41%)]\tLoss: 0.808432\n",
      "Train Epoch: 0 [195840/482500 (41%)]\tLoss: 0.615674\n",
      "Train Epoch: 0 [196000/482500 (41%)]\tLoss: 0.625469\n",
      "Train Epoch: 0 [196160/482500 (41%)]\tLoss: 0.903710\n",
      "Train Epoch: 0 [196320/482500 (41%)]\tLoss: 0.629540\n",
      "Train Epoch: 0 [196480/482500 (41%)]\tLoss: 0.836973\n",
      "Train Epoch: 0 [196640/482500 (41%)]\tLoss: 0.518582\n",
      "Train Epoch: 0 [196800/482500 (41%)]\tLoss: 0.503210\n",
      "Train Epoch: 0 [196960/482500 (41%)]\tLoss: 0.655058\n",
      "Train Epoch: 0 [197120/482500 (41%)]\tLoss: 1.500802\n",
      "Train Epoch: 0 [197280/482500 (41%)]\tLoss: 0.650270\n",
      "Train Epoch: 0 [197440/482500 (41%)]\tLoss: 0.683863\n",
      "Train Epoch: 0 [197600/482500 (41%)]\tLoss: 0.571605\n",
      "Train Epoch: 0 [197760/482500 (41%)]\tLoss: 0.711688\n",
      "Train Epoch: 0 [197920/482500 (41%)]\tLoss: 0.942227\n",
      "Train Epoch: 0 [198080/482500 (41%)]\tLoss: 0.617015\n",
      "Train Epoch: 0 [198240/482500 (41%)]\tLoss: 0.500393\n",
      "Train Epoch: 0 [198400/482500 (41%)]\tLoss: 0.521755\n",
      "Train Epoch: 0 [198560/482500 (41%)]\tLoss: 0.628493\n",
      "Train Epoch: 0 [198720/482500 (41%)]\tLoss: 0.500437\n",
      "Train Epoch: 0 [198880/482500 (41%)]\tLoss: 0.482640\n",
      "Train Epoch: 0 [199040/482500 (41%)]\tLoss: 0.880913\n",
      "Train Epoch: 0 [199200/482500 (41%)]\tLoss: 3.532675\n",
      "Train Epoch: 0 [199360/482500 (41%)]\tLoss: 0.563372\n",
      "Train Epoch: 0 [199520/482500 (41%)]\tLoss: 0.697063\n",
      "Train Epoch: 0 [199680/482500 (41%)]\tLoss: 0.561129\n",
      "Train Epoch: 0 [199840/482500 (41%)]\tLoss: 0.894277\n",
      "Train Epoch: 0 [200000/482500 (41%)]\tLoss: 0.628511\n",
      "Train Epoch: 0 [200160/482500 (41%)]\tLoss: 1.390853\n",
      "Train Epoch: 0 [200320/482500 (42%)]\tLoss: 0.604495\n",
      "Train Epoch: 0 [200480/482500 (42%)]\tLoss: 1.744803\n",
      "Train Epoch: 0 [200640/482500 (42%)]\tLoss: 0.567661\n",
      "Train Epoch: 0 [200800/482500 (42%)]\tLoss: 0.900555\n",
      "Train Epoch: 0 [200960/482500 (42%)]\tLoss: 0.770679\n",
      "Train Epoch: 0 [201120/482500 (42%)]\tLoss: 0.864168\n",
      "Train Epoch: 0 [201280/482500 (42%)]\tLoss: 0.727142\n",
      "Train Epoch: 0 [201440/482500 (42%)]\tLoss: 0.750902\n",
      "Train Epoch: 0 [201600/482500 (42%)]\tLoss: 0.839054\n",
      "Train Epoch: 0 [201760/482500 (42%)]\tLoss: 0.660871\n",
      "Train Epoch: 0 [201920/482500 (42%)]\tLoss: 0.636114\n",
      "Train Epoch: 0 [202080/482500 (42%)]\tLoss: 0.741034\n",
      "Train Epoch: 0 [202240/482500 (42%)]\tLoss: 0.404477\n",
      "Train Epoch: 0 [202400/482500 (42%)]\tLoss: 0.535475\n",
      "Train Epoch: 0 [202560/482500 (42%)]\tLoss: 0.749694\n",
      "Train Epoch: 0 [202720/482500 (42%)]\tLoss: 0.576405\n",
      "Train Epoch: 0 [202880/482500 (42%)]\tLoss: 0.675301\n",
      "Train Epoch: 0 [203040/482500 (42%)]\tLoss: 0.436346\n",
      "Train Epoch: 0 [203200/482500 (42%)]\tLoss: 0.306544\n",
      "Train Epoch: 0 [203360/482500 (42%)]\tLoss: 0.401809\n",
      "Train Epoch: 0 [203520/482500 (42%)]\tLoss: 0.799846\n",
      "Train Epoch: 0 [203680/482500 (42%)]\tLoss: 1.146183\n",
      "Train Epoch: 0 [203840/482500 (42%)]\tLoss: 0.809559\n",
      "Train Epoch: 0 [204000/482500 (42%)]\tLoss: 0.803184\n",
      "Train Epoch: 0 [204160/482500 (42%)]\tLoss: 0.810185\n",
      "Train Epoch: 0 [204320/482500 (42%)]\tLoss: 0.757258\n",
      "Train Epoch: 0 [204480/482500 (42%)]\tLoss: 0.737521\n",
      "Train Epoch: 0 [204640/482500 (42%)]\tLoss: 0.601660\n",
      "Train Epoch: 0 [204800/482500 (42%)]\tLoss: 0.555420\n",
      "Train Epoch: 0 [204960/482500 (42%)]\tLoss: 0.650651\n",
      "Train Epoch: 0 [205120/482500 (43%)]\tLoss: 0.440906\n",
      "Train Epoch: 0 [205280/482500 (43%)]\tLoss: 0.393404\n",
      "Train Epoch: 0 [205440/482500 (43%)]\tLoss: 0.737036\n",
      "Train Epoch: 0 [205600/482500 (43%)]\tLoss: 0.404853\n",
      "Train Epoch: 0 [205760/482500 (43%)]\tLoss: 0.740441\n",
      "Train Epoch: 0 [205920/482500 (43%)]\tLoss: 0.607982\n",
      "Train Epoch: 0 [206080/482500 (43%)]\tLoss: 0.439837\n",
      "Train Epoch: 0 [206240/482500 (43%)]\tLoss: 1.184850\n",
      "Train Epoch: 0 [206400/482500 (43%)]\tLoss: 0.916224\n",
      "Train Epoch: 0 [206560/482500 (43%)]\tLoss: 0.525501\n",
      "Train Epoch: 0 [206720/482500 (43%)]\tLoss: 0.551470\n",
      "Train Epoch: 0 [206880/482500 (43%)]\tLoss: 0.649408\n",
      "Train Epoch: 0 [207040/482500 (43%)]\tLoss: 0.690090\n",
      "Train Epoch: 0 [207200/482500 (43%)]\tLoss: 0.512497\n",
      "Train Epoch: 0 [207360/482500 (43%)]\tLoss: 0.549801\n",
      "Train Epoch: 0 [207520/482500 (43%)]\tLoss: 0.544393\n",
      "Train Epoch: 0 [207680/482500 (43%)]\tLoss: 0.602751\n",
      "Train Epoch: 0 [207840/482500 (43%)]\tLoss: 0.843308\n",
      "Train Epoch: 0 [208000/482500 (43%)]\tLoss: 0.514387\n",
      "Train Epoch: 0 [208160/482500 (43%)]\tLoss: 0.602223\n",
      "Train Epoch: 0 [208320/482500 (43%)]\tLoss: 0.475685\n",
      "Train Epoch: 0 [208480/482500 (43%)]\tLoss: 0.701842\n",
      "Train Epoch: 0 [208640/482500 (43%)]\tLoss: 0.749676\n",
      "Train Epoch: 0 [208800/482500 (43%)]\tLoss: 0.538501\n",
      "Train Epoch: 0 [208960/482500 (43%)]\tLoss: 0.470943\n",
      "Train Epoch: 0 [209120/482500 (43%)]\tLoss: 0.737060\n",
      "Train Epoch: 0 [209280/482500 (43%)]\tLoss: 0.951331\n",
      "Train Epoch: 0 [209440/482500 (43%)]\tLoss: 0.593337\n",
      "Train Epoch: 0 [209600/482500 (43%)]\tLoss: 0.473425\n",
      "Train Epoch: 0 [209760/482500 (43%)]\tLoss: 0.484900\n",
      "Train Epoch: 0 [209920/482500 (44%)]\tLoss: 0.581087\n",
      "Train Epoch: 0 [210080/482500 (44%)]\tLoss: 0.552354\n",
      "Train Epoch: 0 [210240/482500 (44%)]\tLoss: 0.534617\n",
      "Train Epoch: 0 [210400/482500 (44%)]\tLoss: 0.571827\n",
      "Train Epoch: 0 [210560/482500 (44%)]\tLoss: 0.536789\n",
      "Train Epoch: 0 [210720/482500 (44%)]\tLoss: 0.612321\n",
      "Train Epoch: 0 [210880/482500 (44%)]\tLoss: 0.463453\n",
      "Train Epoch: 0 [211040/482500 (44%)]\tLoss: 0.612408\n",
      "Train Epoch: 0 [211200/482500 (44%)]\tLoss: 0.400914\n",
      "Train Epoch: 0 [211360/482500 (44%)]\tLoss: 0.752306\n",
      "Train Epoch: 0 [211520/482500 (44%)]\tLoss: 0.567326\n",
      "Train Epoch: 0 [211680/482500 (44%)]\tLoss: 0.492705\n",
      "Train Epoch: 0 [211840/482500 (44%)]\tLoss: 0.837334\n",
      "Train Epoch: 0 [212000/482500 (44%)]\tLoss: 0.433060\n",
      "Train Epoch: 0 [212160/482500 (44%)]\tLoss: 0.529755\n",
      "Train Epoch: 0 [212320/482500 (44%)]\tLoss: 0.491430\n",
      "Train Epoch: 0 [212480/482500 (44%)]\tLoss: 0.431454\n",
      "Train Epoch: 0 [212640/482500 (44%)]\tLoss: 0.544559\n",
      "Train Epoch: 0 [212800/482500 (44%)]\tLoss: 0.710480\n",
      "Train Epoch: 0 [212960/482500 (44%)]\tLoss: 0.529773\n",
      "Train Epoch: 0 [213120/482500 (44%)]\tLoss: 0.725778\n",
      "Train Epoch: 0 [213280/482500 (44%)]\tLoss: 0.523851\n",
      "Train Epoch: 0 [213440/482500 (44%)]\tLoss: 0.397293\n",
      "Train Epoch: 0 [213600/482500 (44%)]\tLoss: 0.667224\n",
      "Train Epoch: 0 [213760/482500 (44%)]\tLoss: 2.102332\n",
      "Train Epoch: 0 [213920/482500 (44%)]\tLoss: 0.544844\n",
      "Train Epoch: 0 [214080/482500 (44%)]\tLoss: 0.956465\n",
      "Train Epoch: 0 [214240/482500 (44%)]\tLoss: 0.591116\n",
      "Train Epoch: 0 [214400/482500 (44%)]\tLoss: 0.736607\n",
      "Train Epoch: 0 [214560/482500 (44%)]\tLoss: 0.372928\n",
      "Train Epoch: 0 [214720/482500 (45%)]\tLoss: 0.451195\n",
      "Train Epoch: 0 [214880/482500 (45%)]\tLoss: 0.565589\n",
      "Train Epoch: 0 [215040/482500 (45%)]\tLoss: 0.654982\n",
      "Train Epoch: 0 [215200/482500 (45%)]\tLoss: 0.635603\n",
      "Train Epoch: 0 [215360/482500 (45%)]\tLoss: 0.450075\n",
      "Train Epoch: 0 [215520/482500 (45%)]\tLoss: 1.504378\n",
      "Train Epoch: 0 [215680/482500 (45%)]\tLoss: 0.799144\n",
      "Train Epoch: 0 [215840/482500 (45%)]\tLoss: 0.485776\n",
      "Train Epoch: 0 [216000/482500 (45%)]\tLoss: 0.610000\n",
      "Train Epoch: 0 [216160/482500 (45%)]\tLoss: 0.537771\n",
      "Train Epoch: 0 [216320/482500 (45%)]\tLoss: 0.999460\n",
      "Train Epoch: 0 [216480/482500 (45%)]\tLoss: 0.318902\n",
      "Train Epoch: 0 [216640/482500 (45%)]\tLoss: 0.515952\n",
      "Train Epoch: 0 [216800/482500 (45%)]\tLoss: 0.488912\n",
      "Train Epoch: 0 [216960/482500 (45%)]\tLoss: 0.741861\n",
      "Train Epoch: 0 [217120/482500 (45%)]\tLoss: 0.438664\n",
      "Train Epoch: 0 [217280/482500 (45%)]\tLoss: 0.522104\n",
      "Train Epoch: 0 [217440/482500 (45%)]\tLoss: 0.550154\n",
      "Train Epoch: 0 [217600/482500 (45%)]\tLoss: 0.596626\n",
      "Train Epoch: 0 [217760/482500 (45%)]\tLoss: 0.596932\n",
      "Train Epoch: 0 [217920/482500 (45%)]\tLoss: 1.800307\n",
      "Train Epoch: 0 [218080/482500 (45%)]\tLoss: 0.662861\n",
      "Train Epoch: 0 [218240/482500 (45%)]\tLoss: 0.809955\n",
      "Train Epoch: 0 [218400/482500 (45%)]\tLoss: 0.666390\n",
      "Train Epoch: 0 [218560/482500 (45%)]\tLoss: 0.618198\n",
      "Train Epoch: 0 [218720/482500 (45%)]\tLoss: 0.508793\n",
      "Train Epoch: 0 [218880/482500 (45%)]\tLoss: 0.553217\n",
      "Train Epoch: 0 [219040/482500 (45%)]\tLoss: 0.637714\n",
      "Train Epoch: 0 [219200/482500 (45%)]\tLoss: 0.364832\n",
      "Train Epoch: 0 [219360/482500 (45%)]\tLoss: 0.498110\n",
      "Train Epoch: 0 [219520/482500 (45%)]\tLoss: 1.126928\n",
      "Train Epoch: 0 [219680/482500 (46%)]\tLoss: 0.447165\n",
      "Train Epoch: 0 [219840/482500 (46%)]\tLoss: 0.448840\n",
      "Train Epoch: 0 [220000/482500 (46%)]\tLoss: 0.479089\n",
      "Train Epoch: 0 [220160/482500 (46%)]\tLoss: 0.512566\n",
      "Train Epoch: 0 [220320/482500 (46%)]\tLoss: 0.982643\n",
      "Train Epoch: 0 [220480/482500 (46%)]\tLoss: 0.450255\n",
      "Train Epoch: 0 [220640/482500 (46%)]\tLoss: 1.706118\n",
      "Train Epoch: 0 [220800/482500 (46%)]\tLoss: 0.538888\n",
      "Train Epoch: 0 [220960/482500 (46%)]\tLoss: 0.332488\n",
      "Train Epoch: 0 [221120/482500 (46%)]\tLoss: 0.708120\n",
      "Train Epoch: 0 [221280/482500 (46%)]\tLoss: 0.536556\n",
      "Train Epoch: 0 [221440/482500 (46%)]\tLoss: 0.395697\n",
      "Train Epoch: 0 [221600/482500 (46%)]\tLoss: 0.730431\n",
      "Train Epoch: 0 [221760/482500 (46%)]\tLoss: 0.433468\n",
      "Train Epoch: 0 [221920/482500 (46%)]\tLoss: 0.520878\n",
      "Train Epoch: 0 [222080/482500 (46%)]\tLoss: 0.676415\n",
      "Train Epoch: 0 [222240/482500 (46%)]\tLoss: 1.240541\n",
      "Train Epoch: 0 [222400/482500 (46%)]\tLoss: 0.599777\n",
      "Train Epoch: 0 [222560/482500 (46%)]\tLoss: 0.440628\n",
      "Train Epoch: 0 [222720/482500 (46%)]\tLoss: 0.417159\n",
      "Train Epoch: 0 [222880/482500 (46%)]\tLoss: 0.364626\n",
      "Train Epoch: 0 [223040/482500 (46%)]\tLoss: 0.473765\n",
      "Train Epoch: 0 [223200/482500 (46%)]\tLoss: 0.414822\n",
      "Train Epoch: 0 [223360/482500 (46%)]\tLoss: 0.495314\n",
      "Train Epoch: 0 [223520/482500 (46%)]\tLoss: 0.467021\n",
      "Train Epoch: 0 [223680/482500 (46%)]\tLoss: 0.439563\n",
      "Train Epoch: 0 [223840/482500 (46%)]\tLoss: 0.401060\n",
      "Train Epoch: 0 [224000/482500 (46%)]\tLoss: 0.400072\n",
      "Train Epoch: 0 [224160/482500 (46%)]\tLoss: 0.454970\n",
      "Train Epoch: 0 [224320/482500 (46%)]\tLoss: 0.665094\n",
      "Train Epoch: 0 [224480/482500 (47%)]\tLoss: 0.409406\n",
      "Train Epoch: 0 [224640/482500 (47%)]\tLoss: 0.376976\n",
      "Train Epoch: 0 [224800/482500 (47%)]\tLoss: 0.553097\n",
      "Train Epoch: 0 [224960/482500 (47%)]\tLoss: 0.337969\n",
      "Train Epoch: 0 [225120/482500 (47%)]\tLoss: 0.776209\n",
      "Train Epoch: 0 [225280/482500 (47%)]\tLoss: 0.651546\n",
      "Train Epoch: 0 [225440/482500 (47%)]\tLoss: 0.347090\n",
      "Train Epoch: 0 [225600/482500 (47%)]\tLoss: 0.853122\n",
      "Train Epoch: 0 [225760/482500 (47%)]\tLoss: 0.454424\n",
      "Train Epoch: 0 [225920/482500 (47%)]\tLoss: 0.580365\n",
      "Train Epoch: 0 [226080/482500 (47%)]\tLoss: 0.662014\n",
      "Train Epoch: 0 [226240/482500 (47%)]\tLoss: 0.594856\n",
      "Train Epoch: 0 [226400/482500 (47%)]\tLoss: 0.478482\n",
      "Train Epoch: 0 [226560/482500 (47%)]\tLoss: 0.522040\n",
      "Train Epoch: 0 [226720/482500 (47%)]\tLoss: 0.465529\n",
      "Train Epoch: 0 [226880/482500 (47%)]\tLoss: 0.509291\n",
      "Train Epoch: 0 [227040/482500 (47%)]\tLoss: 35.740299\n",
      "Train Epoch: 0 [227200/482500 (47%)]\tLoss: 7.410555\n",
      "Train Epoch: 0 [227360/482500 (47%)]\tLoss: 0.778227\n",
      "Train Epoch: 0 [227520/482500 (47%)]\tLoss: 0.964006\n",
      "Train Epoch: 0 [227680/482500 (47%)]\tLoss: 0.794695\n",
      "Train Epoch: 0 [227840/482500 (47%)]\tLoss: 1.044899\n",
      "Train Epoch: 0 [228000/482500 (47%)]\tLoss: 0.994444\n",
      "Train Epoch: 0 [228160/482500 (47%)]\tLoss: 0.488834\n",
      "Train Epoch: 0 [228320/482500 (47%)]\tLoss: 0.412112\n",
      "Train Epoch: 0 [228480/482500 (47%)]\tLoss: 0.579257\n",
      "Train Epoch: 0 [228640/482500 (47%)]\tLoss: 0.691354\n",
      "Train Epoch: 0 [228800/482500 (47%)]\tLoss: 0.566762\n",
      "Train Epoch: 0 [228960/482500 (47%)]\tLoss: 0.585558\n",
      "Train Epoch: 0 [229120/482500 (47%)]\tLoss: 0.465911\n",
      "Train Epoch: 0 [229280/482500 (48%)]\tLoss: 0.891353\n",
      "Train Epoch: 0 [229440/482500 (48%)]\tLoss: 15.301749\n",
      "Train Epoch: 0 [229600/482500 (48%)]\tLoss: 2.308381\n",
      "Train Epoch: 0 [229760/482500 (48%)]\tLoss: 0.735260\n",
      "Train Epoch: 0 [229920/482500 (48%)]\tLoss: 2.036061\n",
      "Train Epoch: 0 [230080/482500 (48%)]\tLoss: 1.004276\n",
      "Train Epoch: 0 [230240/482500 (48%)]\tLoss: 0.791318\n",
      "Train Epoch: 0 [230400/482500 (48%)]\tLoss: 0.528164\n",
      "Train Epoch: 0 [230560/482500 (48%)]\tLoss: 0.519236\n",
      "Train Epoch: 0 [230720/482500 (48%)]\tLoss: 0.556607\n",
      "Train Epoch: 0 [230880/482500 (48%)]\tLoss: 0.414125\n",
      "Train Epoch: 0 [231040/482500 (48%)]\tLoss: 0.569085\n",
      "Train Epoch: 0 [231200/482500 (48%)]\tLoss: 0.399314\n",
      "Train Epoch: 0 [231360/482500 (48%)]\tLoss: 0.738186\n",
      "Train Epoch: 0 [231520/482500 (48%)]\tLoss: 0.804864\n",
      "Train Epoch: 0 [231680/482500 (48%)]\tLoss: 0.615338\n",
      "Train Epoch: 0 [231840/482500 (48%)]\tLoss: 0.554135\n",
      "Train Epoch: 0 [232000/482500 (48%)]\tLoss: 0.490772\n",
      "Train Epoch: 0 [232160/482500 (48%)]\tLoss: 0.661334\n",
      "Train Epoch: 0 [232320/482500 (48%)]\tLoss: 0.682032\n",
      "Train Epoch: 0 [232480/482500 (48%)]\tLoss: 0.585400\n",
      "Train Epoch: 0 [232640/482500 (48%)]\tLoss: 0.824735\n",
      "Train Epoch: 0 [232800/482500 (48%)]\tLoss: 0.503715\n",
      "Train Epoch: 0 [232960/482500 (48%)]\tLoss: 0.478944\n",
      "Train Epoch: 0 [233120/482500 (48%)]\tLoss: 0.407169\n",
      "Train Epoch: 0 [233280/482500 (48%)]\tLoss: 0.632893\n",
      "Train Epoch: 0 [233440/482500 (48%)]\tLoss: 0.386378\n",
      "Train Epoch: 0 [233600/482500 (48%)]\tLoss: 1.013730\n",
      "Train Epoch: 0 [233760/482500 (48%)]\tLoss: 0.956499\n",
      "Train Epoch: 0 [233920/482500 (48%)]\tLoss: 0.472708\n",
      "Train Epoch: 0 [234080/482500 (49%)]\tLoss: 0.593939\n",
      "Train Epoch: 0 [234240/482500 (49%)]\tLoss: 0.355761\n",
      "Train Epoch: 0 [234400/482500 (49%)]\tLoss: 0.718064\n",
      "Train Epoch: 0 [234560/482500 (49%)]\tLoss: 0.421985\n",
      "Train Epoch: 0 [234720/482500 (49%)]\tLoss: 0.688864\n",
      "Train Epoch: 0 [234880/482500 (49%)]\tLoss: 0.579150\n",
      "Train Epoch: 0 [235040/482500 (49%)]\tLoss: 0.364681\n",
      "Train Epoch: 0 [235200/482500 (49%)]\tLoss: 0.391278\n",
      "Train Epoch: 0 [235360/482500 (49%)]\tLoss: 0.352969\n",
      "Train Epoch: 0 [235520/482500 (49%)]\tLoss: 4.199793\n",
      "Train Epoch: 0 [235680/482500 (49%)]\tLoss: 2.834039\n",
      "Train Epoch: 0 [235840/482500 (49%)]\tLoss: 1.466474\n",
      "Train Epoch: 0 [236000/482500 (49%)]\tLoss: 0.591457\n",
      "Train Epoch: 0 [236160/482500 (49%)]\tLoss: 0.414202\n",
      "Train Epoch: 0 [236320/482500 (49%)]\tLoss: 0.820370\n",
      "Train Epoch: 0 [236480/482500 (49%)]\tLoss: 0.544172\n",
      "Train Epoch: 0 [236640/482500 (49%)]\tLoss: 8.655540\n",
      "Train Epoch: 0 [236800/482500 (49%)]\tLoss: 1.445248\n",
      "Train Epoch: 0 [236960/482500 (49%)]\tLoss: 0.817075\n",
      "Train Epoch: 0 [237120/482500 (49%)]\tLoss: 1.122039\n",
      "Train Epoch: 0 [237280/482500 (49%)]\tLoss: 0.652262\n",
      "Train Epoch: 0 [237440/482500 (49%)]\tLoss: 0.537588\n",
      "Train Epoch: 0 [237600/482500 (49%)]\tLoss: 10.177890\n",
      "Train Epoch: 0 [237760/482500 (49%)]\tLoss: 1.944451\n",
      "Train Epoch: 0 [237920/482500 (49%)]\tLoss: 0.821816\n",
      "Train Epoch: 0 [238080/482500 (49%)]\tLoss: 0.582628\n",
      "Train Epoch: 0 [238240/482500 (49%)]\tLoss: 0.681828\n",
      "Train Epoch: 0 [238400/482500 (49%)]\tLoss: 0.455485\n",
      "Train Epoch: 0 [238560/482500 (49%)]\tLoss: 0.380379\n",
      "Train Epoch: 0 [238720/482500 (49%)]\tLoss: 1.106602\n",
      "Train Epoch: 0 [238880/482500 (50%)]\tLoss: 0.627539\n",
      "Train Epoch: 0 [239040/482500 (50%)]\tLoss: 0.369046\n",
      "Train Epoch: 0 [239200/482500 (50%)]\tLoss: 0.665714\n",
      "Train Epoch: 0 [239360/482500 (50%)]\tLoss: 0.703725\n",
      "Train Epoch: 0 [239520/482500 (50%)]\tLoss: 0.687980\n",
      "Train Epoch: 0 [239680/482500 (50%)]\tLoss: 0.426310\n",
      "Train Epoch: 0 [239840/482500 (50%)]\tLoss: 0.508016\n",
      "Train Epoch: 0 [240000/482500 (50%)]\tLoss: 0.798021\n",
      "Train Epoch: 0 [240160/482500 (50%)]\tLoss: 0.804309\n",
      "Train Epoch: 0 [240320/482500 (50%)]\tLoss: 0.460425\n",
      "Train Epoch: 0 [240480/482500 (50%)]\tLoss: 0.392444\n",
      "Train Epoch: 0 [240640/482500 (50%)]\tLoss: 0.422199\n",
      "Train Epoch: 0 [240800/482500 (50%)]\tLoss: 0.528950\n",
      "Train Epoch: 0 [240960/482500 (50%)]\tLoss: 0.413760\n",
      "Train Epoch: 0 [241120/482500 (50%)]\tLoss: 0.802193\n",
      "Train Epoch: 0 [241280/482500 (50%)]\tLoss: 0.505229\n",
      "Train Epoch: 0 [241440/482500 (50%)]\tLoss: 0.482339\n",
      "Train Epoch: 0 [241600/482500 (50%)]\tLoss: 0.371225\n",
      "Train Epoch: 0 [241760/482500 (50%)]\tLoss: 0.497929\n",
      "Train Epoch: 0 [241920/482500 (50%)]\tLoss: 0.557353\n",
      "Train Epoch: 0 [242080/482500 (50%)]\tLoss: 0.485321\n",
      "Train Epoch: 0 [242240/482500 (50%)]\tLoss: 0.358942\n",
      "Train Epoch: 0 [242400/482500 (50%)]\tLoss: 0.914283\n",
      "Train Epoch: 0 [242560/482500 (50%)]\tLoss: 0.492362\n",
      "Train Epoch: 0 [242720/482500 (50%)]\tLoss: 0.426266\n",
      "Train Epoch: 0 [242880/482500 (50%)]\tLoss: 0.411447\n",
      "Train Epoch: 0 [243040/482500 (50%)]\tLoss: 0.454832\n",
      "Train Epoch: 0 [243200/482500 (50%)]\tLoss: 0.533775\n",
      "Train Epoch: 0 [243360/482500 (50%)]\tLoss: 0.453607\n",
      "Train Epoch: 0 [243520/482500 (50%)]\tLoss: 0.411249\n",
      "Train Epoch: 0 [243680/482500 (51%)]\tLoss: 0.795330\n",
      "Train Epoch: 0 [243840/482500 (51%)]\tLoss: 0.377407\n",
      "Train Epoch: 0 [244000/482500 (51%)]\tLoss: 0.538659\n",
      "Train Epoch: 0 [244160/482500 (51%)]\tLoss: 0.430563\n",
      "Train Epoch: 0 [244320/482500 (51%)]\tLoss: 0.417896\n",
      "Train Epoch: 0 [244480/482500 (51%)]\tLoss: 0.530998\n",
      "Train Epoch: 0 [244640/482500 (51%)]\tLoss: 0.399275\n",
      "Train Epoch: 0 [244800/482500 (51%)]\tLoss: 0.387456\n",
      "Train Epoch: 0 [244960/482500 (51%)]\tLoss: 0.427045\n",
      "Train Epoch: 0 [245120/482500 (51%)]\tLoss: 0.414260\n",
      "Train Epoch: 0 [245280/482500 (51%)]\tLoss: 0.877631\n",
      "Train Epoch: 0 [245440/482500 (51%)]\tLoss: 0.687765\n",
      "Train Epoch: 0 [245600/482500 (51%)]\tLoss: 0.474133\n",
      "Train Epoch: 0 [245760/482500 (51%)]\tLoss: 0.450852\n",
      "Train Epoch: 0 [245920/482500 (51%)]\tLoss: 1.570751\n",
      "Train Epoch: 0 [246080/482500 (51%)]\tLoss: 0.388706\n",
      "Train Epoch: 0 [246240/482500 (51%)]\tLoss: 0.311858\n",
      "Train Epoch: 0 [246400/482500 (51%)]\tLoss: 6.478050\n",
      "Train Epoch: 0 [246560/482500 (51%)]\tLoss: 2.097687\n",
      "Train Epoch: 0 [246720/482500 (51%)]\tLoss: 1.960622\n",
      "Train Epoch: 0 [246880/482500 (51%)]\tLoss: 1.409660\n",
      "Train Epoch: 0 [247040/482500 (51%)]\tLoss: 0.814747\n",
      "Train Epoch: 0 [247200/482500 (51%)]\tLoss: 0.416007\n",
      "Train Epoch: 0 [247360/482500 (51%)]\tLoss: 0.345163\n",
      "Train Epoch: 0 [247520/482500 (51%)]\tLoss: 0.501454\n",
      "Train Epoch: 0 [247680/482500 (51%)]\tLoss: 0.430781\n",
      "Train Epoch: 0 [247840/482500 (51%)]\tLoss: 0.385318\n",
      "Train Epoch: 0 [248000/482500 (51%)]\tLoss: 0.471320\n",
      "Train Epoch: 0 [248160/482500 (51%)]\tLoss: 0.483866\n",
      "Train Epoch: 0 [248320/482500 (51%)]\tLoss: 0.666291\n",
      "Train Epoch: 0 [248480/482500 (51%)]\tLoss: 0.442896\n",
      "Train Epoch: 0 [248640/482500 (52%)]\tLoss: 0.350782\n",
      "Train Epoch: 0 [248800/482500 (52%)]\tLoss: 0.425296\n",
      "Train Epoch: 0 [248960/482500 (52%)]\tLoss: 0.773228\n",
      "Train Epoch: 0 [249120/482500 (52%)]\tLoss: 0.374899\n",
      "Train Epoch: 0 [249280/482500 (52%)]\tLoss: 0.719683\n",
      "Train Epoch: 0 [249440/482500 (52%)]\tLoss: 0.505960\n",
      "Train Epoch: 0 [249600/482500 (52%)]\tLoss: 0.521923\n",
      "Train Epoch: 0 [249760/482500 (52%)]\tLoss: 0.623780\n",
      "Train Epoch: 0 [249920/482500 (52%)]\tLoss: 0.448755\n",
      "Train Epoch: 0 [250080/482500 (52%)]\tLoss: 0.943302\n",
      "Train Epoch: 0 [250240/482500 (52%)]\tLoss: 0.322349\n",
      "Train Epoch: 0 [250400/482500 (52%)]\tLoss: 0.416456\n",
      "Train Epoch: 0 [250560/482500 (52%)]\tLoss: 0.727371\n",
      "Train Epoch: 0 [250720/482500 (52%)]\tLoss: 0.460734\n",
      "Train Epoch: 0 [250880/482500 (52%)]\tLoss: 0.475039\n",
      "Train Epoch: 0 [251040/482500 (52%)]\tLoss: 0.316555\n",
      "Train Epoch: 0 [251200/482500 (52%)]\tLoss: 0.363733\n",
      "Train Epoch: 0 [251360/482500 (52%)]\tLoss: 0.696497\n",
      "Train Epoch: 0 [251520/482500 (52%)]\tLoss: 0.376430\n",
      "Train Epoch: 0 [251680/482500 (52%)]\tLoss: 0.495673\n",
      "Train Epoch: 0 [251840/482500 (52%)]\tLoss: 0.372012\n",
      "Train Epoch: 0 [252000/482500 (52%)]\tLoss: 0.666716\n",
      "Train Epoch: 0 [252160/482500 (52%)]\tLoss: 0.410890\n",
      "Train Epoch: 0 [252320/482500 (52%)]\tLoss: 0.523089\n",
      "Train Epoch: 0 [252480/482500 (52%)]\tLoss: 0.326953\n",
      "Train Epoch: 0 [252640/482500 (52%)]\tLoss: 1.067639\n",
      "Train Epoch: 0 [252800/482500 (52%)]\tLoss: 0.560082\n",
      "Train Epoch: 0 [252960/482500 (52%)]\tLoss: 0.530372\n",
      "Train Epoch: 0 [253120/482500 (52%)]\tLoss: 0.528695\n",
      "Train Epoch: 0 [253280/482500 (52%)]\tLoss: 0.419674\n",
      "Train Epoch: 0 [253440/482500 (53%)]\tLoss: 0.482678\n",
      "Train Epoch: 0 [253600/482500 (53%)]\tLoss: 0.364674\n",
      "Train Epoch: 0 [253760/482500 (53%)]\tLoss: 0.737577\n",
      "Train Epoch: 0 [253920/482500 (53%)]\tLoss: 0.728733\n",
      "Train Epoch: 0 [254080/482500 (53%)]\tLoss: 0.402371\n",
      "Train Epoch: 0 [254240/482500 (53%)]\tLoss: 0.655188\n",
      "Train Epoch: 0 [254400/482500 (53%)]\tLoss: 0.337203\n",
      "Train Epoch: 0 [254560/482500 (53%)]\tLoss: 0.695754\n",
      "Train Epoch: 0 [254720/482500 (53%)]\tLoss: 0.493628\n",
      "Train Epoch: 0 [254880/482500 (53%)]\tLoss: 0.420429\n",
      "Train Epoch: 0 [255040/482500 (53%)]\tLoss: 0.345626\n",
      "Train Epoch: 0 [255200/482500 (53%)]\tLoss: 0.789338\n",
      "Train Epoch: 0 [255360/482500 (53%)]\tLoss: 0.517375\n",
      "Train Epoch: 0 [255520/482500 (53%)]\tLoss: 0.655890\n",
      "Train Epoch: 0 [255680/482500 (53%)]\tLoss: 0.419327\n",
      "Train Epoch: 0 [255840/482500 (53%)]\tLoss: 0.368149\n",
      "Train Epoch: 0 [256000/482500 (53%)]\tLoss: 0.368084\n",
      "Train Epoch: 0 [256160/482500 (53%)]\tLoss: 0.457561\n",
      "Train Epoch: 0 [256320/482500 (53%)]\tLoss: 0.465900\n",
      "Train Epoch: 0 [256480/482500 (53%)]\tLoss: 0.407387\n",
      "Train Epoch: 0 [256640/482500 (53%)]\tLoss: 0.728957\n",
      "Train Epoch: 0 [256800/482500 (53%)]\tLoss: 0.345978\n",
      "Train Epoch: 0 [256960/482500 (53%)]\tLoss: 0.723193\n",
      "Train Epoch: 0 [257120/482500 (53%)]\tLoss: 0.366541\n",
      "Train Epoch: 0 [257280/482500 (53%)]\tLoss: 0.522882\n",
      "Train Epoch: 0 [257440/482500 (53%)]\tLoss: 0.423505\n",
      "Train Epoch: 0 [257600/482500 (53%)]\tLoss: 0.678085\n",
      "Train Epoch: 0 [257760/482500 (53%)]\tLoss: 0.995646\n",
      "Train Epoch: 0 [257920/482500 (53%)]\tLoss: 0.408200\n",
      "Train Epoch: 0 [258080/482500 (53%)]\tLoss: 0.583121\n",
      "Train Epoch: 0 [258240/482500 (54%)]\tLoss: 0.323675\n",
      "Train Epoch: 0 [258400/482500 (54%)]\tLoss: 0.575349\n",
      "Train Epoch: 0 [258560/482500 (54%)]\tLoss: 0.411372\n",
      "Train Epoch: 0 [258720/482500 (54%)]\tLoss: 0.554794\n",
      "Train Epoch: 0 [258880/482500 (54%)]\tLoss: 0.432406\n",
      "Train Epoch: 0 [259040/482500 (54%)]\tLoss: 0.300738\n",
      "Train Epoch: 0 [259200/482500 (54%)]\tLoss: 0.491424\n",
      "Train Epoch: 0 [259360/482500 (54%)]\tLoss: 0.456593\n",
      "Train Epoch: 0 [259520/482500 (54%)]\tLoss: 0.571288\n",
      "Train Epoch: 0 [259680/482500 (54%)]\tLoss: 0.762010\n",
      "Train Epoch: 0 [259840/482500 (54%)]\tLoss: 0.737555\n",
      "Train Epoch: 0 [260000/482500 (54%)]\tLoss: 0.283745\n",
      "Train Epoch: 0 [260160/482500 (54%)]\tLoss: 0.515688\n",
      "Train Epoch: 0 [260320/482500 (54%)]\tLoss: 0.500079\n",
      "Train Epoch: 0 [260480/482500 (54%)]\tLoss: 0.512212\n",
      "Train Epoch: 0 [260640/482500 (54%)]\tLoss: 0.444638\n",
      "Train Epoch: 0 [260800/482500 (54%)]\tLoss: 0.332236\n",
      "Train Epoch: 0 [260960/482500 (54%)]\tLoss: 0.549023\n",
      "Train Epoch: 0 [261120/482500 (54%)]\tLoss: 0.578216\n",
      "Train Epoch: 0 [261280/482500 (54%)]\tLoss: 0.560812\n",
      "Train Epoch: 0 [261440/482500 (54%)]\tLoss: 0.500584\n",
      "Train Epoch: 0 [261600/482500 (54%)]\tLoss: 0.890877\n",
      "Train Epoch: 0 [261760/482500 (54%)]\tLoss: 0.981958\n",
      "Train Epoch: 0 [261920/482500 (54%)]\tLoss: 0.595511\n",
      "Train Epoch: 0 [262080/482500 (54%)]\tLoss: 0.456259\n",
      "Train Epoch: 0 [262240/482500 (54%)]\tLoss: 0.285398\n",
      "Train Epoch: 0 [262400/482500 (54%)]\tLoss: 0.792091\n",
      "Train Epoch: 0 [262560/482500 (54%)]\tLoss: 0.340042\n",
      "Train Epoch: 0 [262720/482500 (54%)]\tLoss: 24.508417\n",
      "Train Epoch: 0 [262880/482500 (54%)]\tLoss: 2.818229\n",
      "Train Epoch: 0 [263040/482500 (55%)]\tLoss: 1.167016\n",
      "Train Epoch: 0 [263200/482500 (55%)]\tLoss: 0.865973\n",
      "Train Epoch: 0 [263360/482500 (55%)]\tLoss: 0.641341\n",
      "Train Epoch: 0 [263520/482500 (55%)]\tLoss: 0.647075\n",
      "Train Epoch: 0 [263680/482500 (55%)]\tLoss: 3.839911\n",
      "Train Epoch: 0 [263840/482500 (55%)]\tLoss: 0.535846\n",
      "Train Epoch: 0 [264000/482500 (55%)]\tLoss: 0.724542\n",
      "Train Epoch: 0 [264160/482500 (55%)]\tLoss: 0.571731\n",
      "Train Epoch: 0 [264320/482500 (55%)]\tLoss: 0.426914\n",
      "Train Epoch: 0 [264480/482500 (55%)]\tLoss: 0.544618\n",
      "Train Epoch: 0 [264640/482500 (55%)]\tLoss: 0.527610\n",
      "Train Epoch: 0 [264800/482500 (55%)]\tLoss: 0.502914\n",
      "Train Epoch: 0 [264960/482500 (55%)]\tLoss: 0.680224\n",
      "Train Epoch: 0 [265120/482500 (55%)]\tLoss: 0.449926\n",
      "Train Epoch: 0 [265280/482500 (55%)]\tLoss: 0.469348\n",
      "Train Epoch: 0 [265440/482500 (55%)]\tLoss: 0.406637\n",
      "Train Epoch: 0 [265600/482500 (55%)]\tLoss: 0.569905\n",
      "Train Epoch: 0 [265760/482500 (55%)]\tLoss: 0.385390\n",
      "Train Epoch: 0 [265920/482500 (55%)]\tLoss: 0.737993\n",
      "Train Epoch: 0 [266080/482500 (55%)]\tLoss: 0.588246\n",
      "Train Epoch: 0 [266240/482500 (55%)]\tLoss: 0.462999\n",
      "Train Epoch: 0 [266400/482500 (55%)]\tLoss: 0.345114\n",
      "Train Epoch: 0 [266560/482500 (55%)]\tLoss: 0.369483\n",
      "Train Epoch: 0 [266720/482500 (55%)]\tLoss: 0.278636\n",
      "Train Epoch: 0 [266880/482500 (55%)]\tLoss: 0.462475\n",
      "Train Epoch: 0 [267040/482500 (55%)]\tLoss: 0.448530\n",
      "Train Epoch: 0 [267200/482500 (55%)]\tLoss: 6.531568\n",
      "Train Epoch: 0 [267360/482500 (55%)]\tLoss: 0.900705\n",
      "Train Epoch: 0 [267520/482500 (55%)]\tLoss: 0.534483\n",
      "Train Epoch: 0 [267680/482500 (55%)]\tLoss: 0.484486\n",
      "Train Epoch: 0 [267840/482500 (56%)]\tLoss: 0.443015\n",
      "Train Epoch: 0 [268000/482500 (56%)]\tLoss: 0.659067\n",
      "Train Epoch: 0 [268160/482500 (56%)]\tLoss: 0.706933\n",
      "Train Epoch: 0 [268320/482500 (56%)]\tLoss: 0.457031\n",
      "Train Epoch: 0 [268480/482500 (56%)]\tLoss: 0.410344\n",
      "Train Epoch: 0 [268640/482500 (56%)]\tLoss: 0.510015\n",
      "Train Epoch: 0 [268800/482500 (56%)]\tLoss: 0.671696\n",
      "Train Epoch: 0 [268960/482500 (56%)]\tLoss: 0.365268\n",
      "Train Epoch: 0 [269120/482500 (56%)]\tLoss: 0.312185\n",
      "Train Epoch: 0 [269280/482500 (56%)]\tLoss: 0.463967\n",
      "Train Epoch: 0 [269440/482500 (56%)]\tLoss: 0.333511\n",
      "Train Epoch: 0 [269600/482500 (56%)]\tLoss: 1.785916\n",
      "Train Epoch: 0 [269760/482500 (56%)]\tLoss: 0.673961\n",
      "Train Epoch: 0 [269920/482500 (56%)]\tLoss: 0.871024\n",
      "Train Epoch: 0 [270080/482500 (56%)]\tLoss: 0.461711\n",
      "Train Epoch: 0 [270240/482500 (56%)]\tLoss: 0.446179\n",
      "Train Epoch: 0 [270400/482500 (56%)]\tLoss: 0.640755\n",
      "Train Epoch: 0 [270560/482500 (56%)]\tLoss: 0.381617\n",
      "Train Epoch: 0 [270720/482500 (56%)]\tLoss: 0.517506\n",
      "Train Epoch: 0 [270880/482500 (56%)]\tLoss: 0.566010\n",
      "Train Epoch: 0 [271040/482500 (56%)]\tLoss: 0.530728\n",
      "Train Epoch: 0 [271200/482500 (56%)]\tLoss: 0.631601\n",
      "Train Epoch: 0 [271360/482500 (56%)]\tLoss: 35.094109\n",
      "Train Epoch: 0 [271520/482500 (56%)]\tLoss: 3.307046\n",
      "Train Epoch: 0 [271680/482500 (56%)]\tLoss: 1.291417\n",
      "Train Epoch: 0 [271840/482500 (56%)]\tLoss: 1.975932\n",
      "Train Epoch: 0 [272000/482500 (56%)]\tLoss: 0.986966\n",
      "Train Epoch: 0 [272160/482500 (56%)]\tLoss: 0.925794\n",
      "Train Epoch: 0 [272320/482500 (56%)]\tLoss: 1.452243\n",
      "Train Epoch: 0 [272480/482500 (56%)]\tLoss: 1.182784\n",
      "Train Epoch: 0 [272640/482500 (57%)]\tLoss: 1.056166\n",
      "Train Epoch: 0 [272800/482500 (57%)]\tLoss: 0.993703\n",
      "Train Epoch: 0 [272960/482500 (57%)]\tLoss: 1.416132\n",
      "Train Epoch: 0 [273120/482500 (57%)]\tLoss: 1.462052\n",
      "Train Epoch: 0 [273280/482500 (57%)]\tLoss: 1.226835\n",
      "Train Epoch: 0 [273440/482500 (57%)]\tLoss: 0.673136\n",
      "Train Epoch: 0 [273600/482500 (57%)]\tLoss: 0.653073\n",
      "Train Epoch: 0 [273760/482500 (57%)]\tLoss: 1.331568\n",
      "Train Epoch: 0 [273920/482500 (57%)]\tLoss: 1.086014\n",
      "Train Epoch: 0 [274080/482500 (57%)]\tLoss: 0.408731\n",
      "Train Epoch: 0 [274240/482500 (57%)]\tLoss: 0.671154\n",
      "Train Epoch: 0 [274400/482500 (57%)]\tLoss: 0.482798\n",
      "Train Epoch: 0 [274560/482500 (57%)]\tLoss: 0.740676\n",
      "Train Epoch: 0 [274720/482500 (57%)]\tLoss: 0.565819\n",
      "Train Epoch: 0 [274880/482500 (57%)]\tLoss: 0.716247\n",
      "Train Epoch: 0 [275040/482500 (57%)]\tLoss: 0.400175\n",
      "Train Epoch: 0 [275200/482500 (57%)]\tLoss: 0.594980\n",
      "Train Epoch: 0 [275360/482500 (57%)]\tLoss: 0.588939\n",
      "Train Epoch: 0 [275520/482500 (57%)]\tLoss: 0.592110\n",
      "Train Epoch: 0 [275680/482500 (57%)]\tLoss: 0.455488\n",
      "Train Epoch: 0 [275840/482500 (57%)]\tLoss: 1.088789\n",
      "Train Epoch: 0 [276000/482500 (57%)]\tLoss: 10.980712\n",
      "Train Epoch: 0 [276160/482500 (57%)]\tLoss: 2.693858\n",
      "Train Epoch: 0 [276320/482500 (57%)]\tLoss: 1.213506\n",
      "Train Epoch: 0 [276480/482500 (57%)]\tLoss: 0.946986\n",
      "Train Epoch: 0 [276640/482500 (57%)]\tLoss: 0.773780\n",
      "Train Epoch: 0 [276800/482500 (57%)]\tLoss: 0.770079\n",
      "Train Epoch: 0 [276960/482500 (57%)]\tLoss: 0.756601\n",
      "Train Epoch: 0 [277120/482500 (57%)]\tLoss: 1.702379\n",
      "Train Epoch: 0 [277280/482500 (57%)]\tLoss: 0.568333\n",
      "Train Epoch: 0 [277440/482500 (58%)]\tLoss: 0.563702\n",
      "Train Epoch: 0 [277600/482500 (58%)]\tLoss: 1.703139\n",
      "Train Epoch: 0 [277760/482500 (58%)]\tLoss: 2.777099\n",
      "Train Epoch: 0 [277920/482500 (58%)]\tLoss: 0.824078\n",
      "Train Epoch: 0 [278080/482500 (58%)]\tLoss: 0.832576\n",
      "Train Epoch: 0 [278240/482500 (58%)]\tLoss: 0.772211\n",
      "Train Epoch: 0 [278400/482500 (58%)]\tLoss: 5.174066\n",
      "Train Epoch: 0 [278560/482500 (58%)]\tLoss: 5.195035\n",
      "Train Epoch: 0 [278720/482500 (58%)]\tLoss: 4.083246\n",
      "Train Epoch: 0 [278880/482500 (58%)]\tLoss: 1.495921\n",
      "Train Epoch: 0 [279040/482500 (58%)]\tLoss: 0.728983\n",
      "Train Epoch: 0 [279200/482500 (58%)]\tLoss: 1.282328\n",
      "Train Epoch: 0 [279360/482500 (58%)]\tLoss: 1.517434\n",
      "Train Epoch: 0 [279520/482500 (58%)]\tLoss: 1.112187\n",
      "Train Epoch: 0 [279680/482500 (58%)]\tLoss: 0.475337\n",
      "Train Epoch: 0 [279840/482500 (58%)]\tLoss: 0.956367\n",
      "Train Epoch: 0 [280000/482500 (58%)]\tLoss: 1.702034\n",
      "Train Epoch: 0 [280160/482500 (58%)]\tLoss: 0.816519\n",
      "Train Epoch: 0 [280320/482500 (58%)]\tLoss: 0.883838\n",
      "Train Epoch: 0 [280480/482500 (58%)]\tLoss: 0.969019\n",
      "Train Epoch: 0 [280640/482500 (58%)]\tLoss: 1.440999\n",
      "Train Epoch: 0 [280800/482500 (58%)]\tLoss: 0.623130\n",
      "Train Epoch: 0 [280960/482500 (58%)]\tLoss: 0.501556\n",
      "Train Epoch: 0 [281120/482500 (58%)]\tLoss: 1.329446\n",
      "Train Epoch: 0 [281280/482500 (58%)]\tLoss: 1.401091\n",
      "Train Epoch: 0 [281440/482500 (58%)]\tLoss: 0.768246\n",
      "Train Epoch: 0 [281600/482500 (58%)]\tLoss: 0.714466\n",
      "Train Epoch: 0 [281760/482500 (58%)]\tLoss: 0.591799\n",
      "Train Epoch: 0 [281920/482500 (58%)]\tLoss: 0.720089\n",
      "Train Epoch: 0 [282080/482500 (58%)]\tLoss: 0.691810\n",
      "Train Epoch: 0 [282240/482500 (58%)]\tLoss: 0.818840\n",
      "Train Epoch: 0 [282400/482500 (59%)]\tLoss: 0.516640\n",
      "Train Epoch: 0 [282560/482500 (59%)]\tLoss: 0.643155\n",
      "Train Epoch: 0 [282720/482500 (59%)]\tLoss: 0.543145\n",
      "Train Epoch: 0 [282880/482500 (59%)]\tLoss: 1.009187\n",
      "Train Epoch: 0 [283040/482500 (59%)]\tLoss: 0.385893\n",
      "Train Epoch: 0 [283200/482500 (59%)]\tLoss: 0.542981\n",
      "Train Epoch: 0 [283360/482500 (59%)]\tLoss: 0.766279\n",
      "Train Epoch: 0 [283520/482500 (59%)]\tLoss: 0.686790\n",
      "Train Epoch: 0 [283680/482500 (59%)]\tLoss: 0.565853\n",
      "Train Epoch: 0 [283840/482500 (59%)]\tLoss: 0.784559\n",
      "Train Epoch: 0 [284000/482500 (59%)]\tLoss: 2.323336\n",
      "Train Epoch: 0 [284160/482500 (59%)]\tLoss: 0.781149\n",
      "Train Epoch: 0 [284320/482500 (59%)]\tLoss: 0.894633\n",
      "Train Epoch: 0 [284480/482500 (59%)]\tLoss: 0.773796\n",
      "Train Epoch: 0 [284640/482500 (59%)]\tLoss: 0.637298\n",
      "Train Epoch: 0 [284800/482500 (59%)]\tLoss: 0.730530\n",
      "Train Epoch: 0 [284960/482500 (59%)]\tLoss: 0.610205\n",
      "Train Epoch: 0 [285120/482500 (59%)]\tLoss: 0.441119\n",
      "Train Epoch: 0 [285280/482500 (59%)]\tLoss: 1.300363\n",
      "Train Epoch: 0 [285440/482500 (59%)]\tLoss: 1.176588\n",
      "Train Epoch: 0 [285600/482500 (59%)]\tLoss: 0.558365\n",
      "Train Epoch: 0 [285760/482500 (59%)]\tLoss: 0.562729\n",
      "Train Epoch: 0 [285920/482500 (59%)]\tLoss: 0.590642\n",
      "Train Epoch: 0 [286080/482500 (59%)]\tLoss: 0.744215\n",
      "Train Epoch: 0 [286240/482500 (59%)]\tLoss: 0.583950\n",
      "Train Epoch: 0 [286400/482500 (59%)]\tLoss: 0.745562\n",
      "Train Epoch: 0 [286560/482500 (59%)]\tLoss: 0.918560\n",
      "Train Epoch: 0 [286720/482500 (59%)]\tLoss: 0.588724\n",
      "Train Epoch: 0 [286880/482500 (59%)]\tLoss: 0.418248\n",
      "Train Epoch: 0 [287040/482500 (59%)]\tLoss: 20.798517\n",
      "Train Epoch: 0 [287200/482500 (60%)]\tLoss: 11.449617\n",
      "Train Epoch: 0 [287360/482500 (60%)]\tLoss: 5.970790\n",
      "Train Epoch: 0 [287520/482500 (60%)]\tLoss: 1.561542\n",
      "Train Epoch: 0 [287680/482500 (60%)]\tLoss: 3.522177\n",
      "Train Epoch: 0 [287840/482500 (60%)]\tLoss: 2.041674\n",
      "Train Epoch: 0 [288000/482500 (60%)]\tLoss: 2.422333\n",
      "Train Epoch: 0 [288160/482500 (60%)]\tLoss: 0.586658\n",
      "Train Epoch: 0 [288320/482500 (60%)]\tLoss: 0.716575\n",
      "Train Epoch: 0 [288480/482500 (60%)]\tLoss: 0.861276\n",
      "Train Epoch: 0 [288640/482500 (60%)]\tLoss: 0.578175\n",
      "Train Epoch: 0 [288800/482500 (60%)]\tLoss: 1.097851\n",
      "Train Epoch: 0 [288960/482500 (60%)]\tLoss: 0.682812\n",
      "Train Epoch: 0 [289120/482500 (60%)]\tLoss: 0.807494\n",
      "Train Epoch: 0 [289280/482500 (60%)]\tLoss: 0.900092\n",
      "Train Epoch: 0 [289440/482500 (60%)]\tLoss: 0.685985\n",
      "Train Epoch: 0 [289600/482500 (60%)]\tLoss: 0.626315\n",
      "Train Epoch: 0 [289760/482500 (60%)]\tLoss: 5.317987\n",
      "Train Epoch: 0 [289920/482500 (60%)]\tLoss: 0.907442\n",
      "Train Epoch: 0 [290080/482500 (60%)]\tLoss: 0.964422\n",
      "Train Epoch: 0 [290240/482500 (60%)]\tLoss: 1.046263\n",
      "Train Epoch: 0 [290400/482500 (60%)]\tLoss: 0.763180\n",
      "Train Epoch: 0 [290560/482500 (60%)]\tLoss: 0.618432\n",
      "Train Epoch: 0 [290720/482500 (60%)]\tLoss: 0.476529\n",
      "Train Epoch: 0 [290880/482500 (60%)]\tLoss: 1.014408\n",
      "Train Epoch: 0 [291040/482500 (60%)]\tLoss: 0.743557\n",
      "Train Epoch: 0 [291200/482500 (60%)]\tLoss: 0.711658\n",
      "Train Epoch: 0 [291360/482500 (60%)]\tLoss: 11.347335\n",
      "Train Epoch: 0 [291520/482500 (60%)]\tLoss: 11.846362\n",
      "Train Epoch: 0 [291680/482500 (60%)]\tLoss: 2.977112\n",
      "Train Epoch: 0 [291840/482500 (60%)]\tLoss: 1.711634\n",
      "Train Epoch: 0 [292000/482500 (61%)]\tLoss: 1.538867\n",
      "Train Epoch: 0 [292160/482500 (61%)]\tLoss: 1.471318\n",
      "Train Epoch: 0 [292320/482500 (61%)]\tLoss: 1.733602\n",
      "Train Epoch: 0 [292480/482500 (61%)]\tLoss: 1.191712\n",
      "Train Epoch: 0 [292640/482500 (61%)]\tLoss: 1.736026\n",
      "Train Epoch: 0 [292800/482500 (61%)]\tLoss: 1.306616\n",
      "Train Epoch: 0 [292960/482500 (61%)]\tLoss: 0.850333\n",
      "Train Epoch: 0 [293120/482500 (61%)]\tLoss: 395.216736\n",
      "Train Epoch: 0 [293280/482500 (61%)]\tLoss: 113.738983\n",
      "Train Epoch: 0 [293440/482500 (61%)]\tLoss: 16.291891\n",
      "Train Epoch: 0 [293600/482500 (61%)]\tLoss: 4.828149\n",
      "Train Epoch: 0 [293760/482500 (61%)]\tLoss: 3.070538\n",
      "Train Epoch: 0 [293920/482500 (61%)]\tLoss: 3.666016\n",
      "Train Epoch: 0 [294080/482500 (61%)]\tLoss: 2.108743\n",
      "Train Epoch: 0 [294240/482500 (61%)]\tLoss: 2.136523\n",
      "Train Epoch: 0 [294400/482500 (61%)]\tLoss: 2.327082\n",
      "Train Epoch: 0 [294560/482500 (61%)]\tLoss: 2.412422\n",
      "Train Epoch: 0 [294720/482500 (61%)]\tLoss: 1.964505\n",
      "Train Epoch: 0 [294880/482500 (61%)]\tLoss: 0.863211\n",
      "Train Epoch: 0 [295040/482500 (61%)]\tLoss: 1.301583\n",
      "Train Epoch: 0 [295200/482500 (61%)]\tLoss: 1.357233\n",
      "Train Epoch: 0 [295360/482500 (61%)]\tLoss: 1.472949\n",
      "Train Epoch: 0 [295520/482500 (61%)]\tLoss: 1.302868\n",
      "Train Epoch: 0 [295680/482500 (61%)]\tLoss: 1.268355\n",
      "Train Epoch: 0 [295840/482500 (61%)]\tLoss: 2.196297\n",
      "Train Epoch: 0 [296000/482500 (61%)]\tLoss: 1.080257\n",
      "Train Epoch: 0 [296160/482500 (61%)]\tLoss: 0.955508\n",
      "Train Epoch: 0 [296320/482500 (61%)]\tLoss: 1.090685\n",
      "Train Epoch: 0 [296480/482500 (61%)]\tLoss: 0.950648\n",
      "Train Epoch: 0 [296640/482500 (61%)]\tLoss: 1.151682\n",
      "Train Epoch: 0 [296800/482500 (62%)]\tLoss: 3.054214\n",
      "Train Epoch: 0 [296960/482500 (62%)]\tLoss: 1.039274\n",
      "Train Epoch: 0 [297120/482500 (62%)]\tLoss: 1.158327\n",
      "Train Epoch: 0 [297280/482500 (62%)]\tLoss: 0.822032\n",
      "Train Epoch: 0 [297440/482500 (62%)]\tLoss: 0.744983\n",
      "Train Epoch: 0 [297600/482500 (62%)]\tLoss: 1.124092\n",
      "Train Epoch: 0 [297760/482500 (62%)]\tLoss: 0.912070\n",
      "Train Epoch: 0 [297920/482500 (62%)]\tLoss: 1.078400\n",
      "Train Epoch: 0 [298080/482500 (62%)]\tLoss: 1.865916\n",
      "Train Epoch: 0 [298240/482500 (62%)]\tLoss: 1.179184\n",
      "Train Epoch: 0 [298400/482500 (62%)]\tLoss: 1.098701\n",
      "Train Epoch: 0 [298560/482500 (62%)]\tLoss: 1.485443\n",
      "Train Epoch: 0 [298720/482500 (62%)]\tLoss: 1.385198\n",
      "Train Epoch: 0 [298880/482500 (62%)]\tLoss: 0.937108\n",
      "Train Epoch: 0 [299040/482500 (62%)]\tLoss: 1.399237\n",
      "Train Epoch: 0 [299200/482500 (62%)]\tLoss: 0.671854\n",
      "Train Epoch: 0 [299360/482500 (62%)]\tLoss: 1.600033\n",
      "Train Epoch: 0 [299520/482500 (62%)]\tLoss: 1.009146\n",
      "Train Epoch: 0 [299680/482500 (62%)]\tLoss: 0.915741\n",
      "Train Epoch: 0 [299840/482500 (62%)]\tLoss: 0.849488\n",
      "Train Epoch: 0 [300000/482500 (62%)]\tLoss: 1.356706\n",
      "Train Epoch: 0 [300160/482500 (62%)]\tLoss: 0.852766\n",
      "Train Epoch: 0 [300320/482500 (62%)]\tLoss: 1.023176\n",
      "Train Epoch: 0 [300480/482500 (62%)]\tLoss: 0.712251\n",
      "Train Epoch: 0 [300640/482500 (62%)]\tLoss: 0.660686\n",
      "Train Epoch: 0 [300800/482500 (62%)]\tLoss: 0.871762\n",
      "Train Epoch: 0 [300960/482500 (62%)]\tLoss: 0.620094\n",
      "Train Epoch: 0 [301120/482500 (62%)]\tLoss: 0.617575\n",
      "Train Epoch: 0 [301280/482500 (62%)]\tLoss: 0.636808\n",
      "Train Epoch: 0 [301440/482500 (62%)]\tLoss: 0.699200\n",
      "Train Epoch: 0 [301600/482500 (63%)]\tLoss: 0.783383\n",
      "Train Epoch: 0 [301760/482500 (63%)]\tLoss: 1.456152\n",
      "Train Epoch: 0 [301920/482500 (63%)]\tLoss: 0.838963\n",
      "Train Epoch: 0 [302080/482500 (63%)]\tLoss: 0.777742\n",
      "Train Epoch: 0 [302240/482500 (63%)]\tLoss: 0.857222\n",
      "Train Epoch: 0 [302400/482500 (63%)]\tLoss: 0.524595\n",
      "Train Epoch: 0 [302560/482500 (63%)]\tLoss: 1.020242\n",
      "Train Epoch: 0 [302720/482500 (63%)]\tLoss: 0.615768\n",
      "Train Epoch: 0 [302880/482500 (63%)]\tLoss: 0.695464\n",
      "Train Epoch: 0 [303040/482500 (63%)]\tLoss: 0.603480\n",
      "Train Epoch: 0 [303200/482500 (63%)]\tLoss: 0.572654\n",
      "Train Epoch: 0 [303360/482500 (63%)]\tLoss: 0.660839\n",
      "Train Epoch: 0 [303520/482500 (63%)]\tLoss: 0.654978\n",
      "Train Epoch: 0 [303680/482500 (63%)]\tLoss: 0.721896\n",
      "Train Epoch: 0 [303840/482500 (63%)]\tLoss: 0.770319\n",
      "Train Epoch: 0 [304000/482500 (63%)]\tLoss: 0.992290\n",
      "Train Epoch: 0 [304160/482500 (63%)]\tLoss: 0.856992\n",
      "Train Epoch: 0 [304320/482500 (63%)]\tLoss: 0.679025\n",
      "Train Epoch: 0 [304480/482500 (63%)]\tLoss: 0.437459\n",
      "Train Epoch: 0 [304640/482500 (63%)]\tLoss: 0.761023\n",
      "Train Epoch: 0 [304800/482500 (63%)]\tLoss: 0.446572\n",
      "Train Epoch: 0 [304960/482500 (63%)]\tLoss: 0.561927\n",
      "Train Epoch: 0 [305120/482500 (63%)]\tLoss: 0.919419\n",
      "Train Epoch: 0 [305280/482500 (63%)]\tLoss: 0.599336\n",
      "Train Epoch: 0 [305440/482500 (63%)]\tLoss: 0.416692\n",
      "Train Epoch: 0 [305600/482500 (63%)]\tLoss: 0.537715\n",
      "Train Epoch: 0 [305760/482500 (63%)]\tLoss: 0.651041\n",
      "Train Epoch: 0 [305920/482500 (63%)]\tLoss: 0.342208\n",
      "Train Epoch: 0 [306080/482500 (63%)]\tLoss: 0.653304\n",
      "Train Epoch: 0 [306240/482500 (63%)]\tLoss: 0.530124\n",
      "Train Epoch: 0 [306400/482500 (64%)]\tLoss: 0.813969\n",
      "Train Epoch: 0 [306560/482500 (64%)]\tLoss: 0.751716\n",
      "Train Epoch: 0 [306720/482500 (64%)]\tLoss: 9.700525\n",
      "Train Epoch: 0 [306880/482500 (64%)]\tLoss: 2.754527\n",
      "Train Epoch: 0 [307040/482500 (64%)]\tLoss: 0.943833\n",
      "Train Epoch: 0 [307200/482500 (64%)]\tLoss: 1.075721\n",
      "Train Epoch: 0 [307360/482500 (64%)]\tLoss: 0.570698\n",
      "Train Epoch: 0 [307520/482500 (64%)]\tLoss: 0.933880\n",
      "Train Epoch: 0 [307680/482500 (64%)]\tLoss: 0.660248\n",
      "Train Epoch: 0 [307840/482500 (64%)]\tLoss: 0.904017\n",
      "Train Epoch: 0 [308000/482500 (64%)]\tLoss: 0.876345\n",
      "Train Epoch: 0 [308160/482500 (64%)]\tLoss: 0.810083\n",
      "Train Epoch: 0 [308320/482500 (64%)]\tLoss: 0.925485\n",
      "Train Epoch: 0 [308480/482500 (64%)]\tLoss: 0.825438\n",
      "Train Epoch: 0 [308640/482500 (64%)]\tLoss: 0.429107\n",
      "Train Epoch: 0 [308800/482500 (64%)]\tLoss: 0.697249\n",
      "Train Epoch: 0 [308960/482500 (64%)]\tLoss: 0.534572\n",
      "Train Epoch: 0 [309120/482500 (64%)]\tLoss: 0.442637\n",
      "Train Epoch: 0 [309280/482500 (64%)]\tLoss: 0.613491\n",
      "Train Epoch: 0 [309440/482500 (64%)]\tLoss: 0.538250\n",
      "Train Epoch: 0 [309600/482500 (64%)]\tLoss: 0.557101\n",
      "Train Epoch: 0 [309760/482500 (64%)]\tLoss: 0.461183\n",
      "Train Epoch: 0 [309920/482500 (64%)]\tLoss: 0.789540\n",
      "Train Epoch: 0 [310080/482500 (64%)]\tLoss: 0.452573\n",
      "Train Epoch: 0 [310240/482500 (64%)]\tLoss: 0.566488\n",
      "Train Epoch: 0 [310400/482500 (64%)]\tLoss: 0.445799\n",
      "Train Epoch: 0 [310560/482500 (64%)]\tLoss: 0.611713\n",
      "Train Epoch: 0 [310720/482500 (64%)]\tLoss: 0.413041\n",
      "Train Epoch: 0 [310880/482500 (64%)]\tLoss: 0.609034\n",
      "Train Epoch: 0 [311040/482500 (64%)]\tLoss: 0.717151\n",
      "Train Epoch: 0 [311200/482500 (64%)]\tLoss: 0.485302\n",
      "Train Epoch: 0 [311360/482500 (65%)]\tLoss: 0.494868\n",
      "Train Epoch: 0 [311520/482500 (65%)]\tLoss: 0.593822\n",
      "Train Epoch: 0 [311680/482500 (65%)]\tLoss: 0.843707\n",
      "Train Epoch: 0 [311840/482500 (65%)]\tLoss: 0.410653\n",
      "Train Epoch: 0 [312000/482500 (65%)]\tLoss: 0.383575\n",
      "Train Epoch: 0 [312160/482500 (65%)]\tLoss: 0.464878\n",
      "Train Epoch: 0 [312320/482500 (65%)]\tLoss: 0.537818\n",
      "Train Epoch: 0 [312480/482500 (65%)]\tLoss: 0.653073\n",
      "Train Epoch: 0 [312640/482500 (65%)]\tLoss: 0.406428\n",
      "Train Epoch: 0 [312800/482500 (65%)]\tLoss: 0.439140\n",
      "Train Epoch: 0 [312960/482500 (65%)]\tLoss: 0.844576\n",
      "Train Epoch: 0 [313120/482500 (65%)]\tLoss: 0.462851\n",
      "Train Epoch: 0 [313280/482500 (65%)]\tLoss: 0.451211\n",
      "Train Epoch: 0 [313440/482500 (65%)]\tLoss: 0.308527\n",
      "Train Epoch: 0 [313600/482500 (65%)]\tLoss: 0.787405\n",
      "Train Epoch: 0 [313760/482500 (65%)]\tLoss: 0.383968\n",
      "Train Epoch: 0 [313920/482500 (65%)]\tLoss: 0.344585\n",
      "Train Epoch: 0 [314080/482500 (65%)]\tLoss: 0.372832\n",
      "Train Epoch: 0 [314240/482500 (65%)]\tLoss: 1.066879\n",
      "Train Epoch: 0 [314400/482500 (65%)]\tLoss: 0.444918\n",
      "Train Epoch: 0 [314560/482500 (65%)]\tLoss: 0.325302\n",
      "Train Epoch: 0 [314720/482500 (65%)]\tLoss: 0.534149\n",
      "Train Epoch: 0 [314880/482500 (65%)]\tLoss: 0.455172\n",
      "Train Epoch: 0 [315040/482500 (65%)]\tLoss: 0.387817\n",
      "Train Epoch: 0 [315200/482500 (65%)]\tLoss: 0.358621\n",
      "Train Epoch: 0 [315360/482500 (65%)]\tLoss: 0.437202\n",
      "Train Epoch: 0 [315520/482500 (65%)]\tLoss: 0.396186\n",
      "Train Epoch: 0 [315680/482500 (65%)]\tLoss: 0.907976\n",
      "Train Epoch: 0 [315840/482500 (65%)]\tLoss: 0.424411\n",
      "Train Epoch: 0 [316000/482500 (65%)]\tLoss: 382.469116\n",
      "Train Epoch: 0 [316160/482500 (66%)]\tLoss: 4.380740\n",
      "Train Epoch: 0 [316320/482500 (66%)]\tLoss: 1.614663\n",
      "Train Epoch: 0 [316480/482500 (66%)]\tLoss: 1.186635\n",
      "Train Epoch: 0 [316640/482500 (66%)]\tLoss: 0.789235\n",
      "Train Epoch: 0 [316800/482500 (66%)]\tLoss: 0.625596\n",
      "Train Epoch: 0 [316960/482500 (66%)]\tLoss: 0.823478\n",
      "Train Epoch: 0 [317120/482500 (66%)]\tLoss: 0.645276\n",
      "Train Epoch: 0 [317280/482500 (66%)]\tLoss: 0.573056\n",
      "Train Epoch: 0 [317440/482500 (66%)]\tLoss: 1.285940\n",
      "Train Epoch: 0 [317600/482500 (66%)]\tLoss: 1.074847\n",
      "Train Epoch: 0 [317760/482500 (66%)]\tLoss: 0.429335\n",
      "Train Epoch: 0 [317920/482500 (66%)]\tLoss: 0.685408\n",
      "Train Epoch: 0 [318080/482500 (66%)]\tLoss: 0.540545\n",
      "Train Epoch: 0 [318240/482500 (66%)]\tLoss: 0.617833\n",
      "Train Epoch: 0 [318400/482500 (66%)]\tLoss: 1.253676\n",
      "Train Epoch: 0 [318560/482500 (66%)]\tLoss: 0.505979\n",
      "Train Epoch: 0 [318720/482500 (66%)]\tLoss: 0.481297\n",
      "Train Epoch: 0 [318880/482500 (66%)]\tLoss: 0.457880\n",
      "Train Epoch: 0 [319040/482500 (66%)]\tLoss: 0.479419\n",
      "Train Epoch: 0 [319200/482500 (66%)]\tLoss: 0.543998\n",
      "Train Epoch: 0 [319360/482500 (66%)]\tLoss: 0.635604\n",
      "Train Epoch: 0 [319520/482500 (66%)]\tLoss: 0.771615\n",
      "Train Epoch: 0 [319680/482500 (66%)]\tLoss: 0.862342\n",
      "Train Epoch: 0 [319840/482500 (66%)]\tLoss: 0.413124\n",
      "Train Epoch: 0 [320000/482500 (66%)]\tLoss: 0.611301\n",
      "Train Epoch: 0 [320160/482500 (66%)]\tLoss: 0.729643\n",
      "Train Epoch: 0 [320320/482500 (66%)]\tLoss: 0.642419\n",
      "Train Epoch: 0 [320480/482500 (66%)]\tLoss: 0.777906\n",
      "Train Epoch: 0 [320640/482500 (66%)]\tLoss: 0.556205\n",
      "Train Epoch: 0 [320800/482500 (66%)]\tLoss: 0.655874\n",
      "Train Epoch: 0 [320960/482500 (67%)]\tLoss: 0.643825\n",
      "Train Epoch: 0 [321120/482500 (67%)]\tLoss: 0.410531\n",
      "Train Epoch: 0 [321280/482500 (67%)]\tLoss: 0.899560\n",
      "Train Epoch: 0 [321440/482500 (67%)]\tLoss: 0.754878\n",
      "Train Epoch: 0 [321600/482500 (67%)]\tLoss: 0.638820\n",
      "Train Epoch: 0 [321760/482500 (67%)]\tLoss: 0.415254\n",
      "Train Epoch: 0 [321920/482500 (67%)]\tLoss: 0.447339\n",
      "Train Epoch: 0 [322080/482500 (67%)]\tLoss: 0.399456\n",
      "Train Epoch: 0 [322240/482500 (67%)]\tLoss: 0.961941\n",
      "Train Epoch: 0 [322400/482500 (67%)]\tLoss: 0.499312\n",
      "Train Epoch: 0 [322560/482500 (67%)]\tLoss: 1.059382\n",
      "Train Epoch: 0 [322720/482500 (67%)]\tLoss: 0.422065\n",
      "Train Epoch: 0 [322880/482500 (67%)]\tLoss: 0.740586\n",
      "Train Epoch: 0 [323040/482500 (67%)]\tLoss: 0.398963\n",
      "Train Epoch: 0 [323200/482500 (67%)]\tLoss: 0.620402\n",
      "Train Epoch: 0 [323360/482500 (67%)]\tLoss: 0.933293\n",
      "Train Epoch: 0 [323520/482500 (67%)]\tLoss: 0.453828\n",
      "Train Epoch: 0 [323680/482500 (67%)]\tLoss: 0.452141\n",
      "Train Epoch: 0 [323840/482500 (67%)]\tLoss: 0.444455\n",
      "Train Epoch: 0 [324000/482500 (67%)]\tLoss: 0.346459\n",
      "Train Epoch: 0 [324160/482500 (67%)]\tLoss: 0.902415\n",
      "Train Epoch: 0 [324320/482500 (67%)]\tLoss: 1.413010\n",
      "Train Epoch: 0 [324480/482500 (67%)]\tLoss: 0.739401\n",
      "Train Epoch: 0 [324640/482500 (67%)]\tLoss: 0.746643\n",
      "Train Epoch: 0 [324800/482500 (67%)]\tLoss: 0.463710\n",
      "Train Epoch: 0 [324960/482500 (67%)]\tLoss: 0.490799\n",
      "Train Epoch: 0 [325120/482500 (67%)]\tLoss: 0.790088\n",
      "Train Epoch: 0 [325280/482500 (67%)]\tLoss: 1.096123\n",
      "Train Epoch: 0 [325440/482500 (67%)]\tLoss: 0.381950\n",
      "Train Epoch: 0 [325600/482500 (67%)]\tLoss: 0.587412\n",
      "Train Epoch: 0 [325760/482500 (68%)]\tLoss: 0.539093\n",
      "Train Epoch: 0 [325920/482500 (68%)]\tLoss: 0.421275\n",
      "Train Epoch: 0 [326080/482500 (68%)]\tLoss: 0.458186\n",
      "Train Epoch: 0 [326240/482500 (68%)]\tLoss: 0.902168\n",
      "Train Epoch: 0 [326400/482500 (68%)]\tLoss: 0.855063\n",
      "Train Epoch: 0 [326560/482500 (68%)]\tLoss: 0.740852\n",
      "Train Epoch: 0 [326720/482500 (68%)]\tLoss: 0.469979\n",
      "Train Epoch: 0 [326880/482500 (68%)]\tLoss: 0.656523\n",
      "Train Epoch: 0 [327040/482500 (68%)]\tLoss: 0.448140\n",
      "Train Epoch: 0 [327200/482500 (68%)]\tLoss: 0.440585\n",
      "Train Epoch: 0 [327360/482500 (68%)]\tLoss: 0.659854\n",
      "Train Epoch: 0 [327520/482500 (68%)]\tLoss: 0.435886\n",
      "Train Epoch: 0 [327680/482500 (68%)]\tLoss: 0.309523\n",
      "Train Epoch: 0 [327840/482500 (68%)]\tLoss: 0.417273\n",
      "Train Epoch: 0 [328000/482500 (68%)]\tLoss: 0.390859\n",
      "Train Epoch: 0 [328160/482500 (68%)]\tLoss: 0.366802\n",
      "Train Epoch: 0 [328320/482500 (68%)]\tLoss: 0.514108\n",
      "Train Epoch: 0 [328480/482500 (68%)]\tLoss: 0.490853\n",
      "Train Epoch: 0 [328640/482500 (68%)]\tLoss: 0.421222\n",
      "Train Epoch: 0 [328800/482500 (68%)]\tLoss: 0.606577\n",
      "Train Epoch: 0 [328960/482500 (68%)]\tLoss: 0.724553\n",
      "Train Epoch: 0 [329120/482500 (68%)]\tLoss: 0.442401\n",
      "Train Epoch: 0 [329280/482500 (68%)]\tLoss: 0.567941\n",
      "Train Epoch: 0 [329440/482500 (68%)]\tLoss: 0.432752\n",
      "Train Epoch: 0 [329600/482500 (68%)]\tLoss: 0.469524\n",
      "Train Epoch: 0 [329760/482500 (68%)]\tLoss: 0.355839\n",
      "Train Epoch: 0 [329920/482500 (68%)]\tLoss: 0.302238\n",
      "Train Epoch: 0 [330080/482500 (68%)]\tLoss: 0.354129\n",
      "Train Epoch: 0 [330240/482500 (68%)]\tLoss: 0.243426\n",
      "Train Epoch: 0 [330400/482500 (68%)]\tLoss: 0.848882\n",
      "Train Epoch: 0 [330560/482500 (69%)]\tLoss: 0.862608\n",
      "Train Epoch: 0 [330720/482500 (69%)]\tLoss: 0.546030\n",
      "Train Epoch: 0 [330880/482500 (69%)]\tLoss: 0.465741\n",
      "Train Epoch: 0 [331040/482500 (69%)]\tLoss: 0.262401\n",
      "Train Epoch: 0 [331200/482500 (69%)]\tLoss: 0.440792\n",
      "Train Epoch: 0 [331360/482500 (69%)]\tLoss: 0.296751\n",
      "Train Epoch: 0 [331520/482500 (69%)]\tLoss: 0.563603\n",
      "Train Epoch: 0 [331680/482500 (69%)]\tLoss: 0.280693\n",
      "Train Epoch: 0 [331840/482500 (69%)]\tLoss: 0.313601\n",
      "Train Epoch: 0 [332000/482500 (69%)]\tLoss: 0.628738\n",
      "Train Epoch: 0 [332160/482500 (69%)]\tLoss: 0.346625\n",
      "Train Epoch: 0 [332320/482500 (69%)]\tLoss: 0.438116\n",
      "Train Epoch: 0 [332480/482500 (69%)]\tLoss: 0.496072\n",
      "Train Epoch: 0 [332640/482500 (69%)]\tLoss: 0.486067\n",
      "Train Epoch: 0 [332800/482500 (69%)]\tLoss: 0.758617\n",
      "Train Epoch: 0 [332960/482500 (69%)]\tLoss: 9.804339\n",
      "Train Epoch: 0 [333120/482500 (69%)]\tLoss: 0.365345\n",
      "Train Epoch: 0 [333280/482500 (69%)]\tLoss: 0.562768\n",
      "Train Epoch: 0 [333440/482500 (69%)]\tLoss: 0.539299\n",
      "Train Epoch: 0 [333600/482500 (69%)]\tLoss: 0.335280\n",
      "Train Epoch: 0 [333760/482500 (69%)]\tLoss: 0.678063\n",
      "Train Epoch: 0 [333920/482500 (69%)]\tLoss: 0.363833\n",
      "Train Epoch: 0 [334080/482500 (69%)]\tLoss: 0.385211\n",
      "Train Epoch: 0 [334240/482500 (69%)]\tLoss: 0.546213\n",
      "Train Epoch: 0 [334400/482500 (69%)]\tLoss: 0.365849\n",
      "Train Epoch: 0 [334560/482500 (69%)]\tLoss: 0.452966\n",
      "Train Epoch: 0 [334720/482500 (69%)]\tLoss: 0.368915\n",
      "Train Epoch: 0 [334880/482500 (69%)]\tLoss: 0.341584\n",
      "Train Epoch: 0 [335040/482500 (69%)]\tLoss: 0.458768\n",
      "Train Epoch: 0 [335200/482500 (69%)]\tLoss: 0.500342\n",
      "Train Epoch: 0 [335360/482500 (70%)]\tLoss: 0.559336\n",
      "Train Epoch: 0 [335520/482500 (70%)]\tLoss: 0.496154\n",
      "Train Epoch: 0 [335680/482500 (70%)]\tLoss: 0.596833\n",
      "Train Epoch: 0 [335840/482500 (70%)]\tLoss: 0.448256\n",
      "Train Epoch: 0 [336000/482500 (70%)]\tLoss: 0.880927\n",
      "Train Epoch: 0 [336160/482500 (70%)]\tLoss: 0.571914\n",
      "Train Epoch: 0 [336320/482500 (70%)]\tLoss: 0.468432\n",
      "Train Epoch: 0 [336480/482500 (70%)]\tLoss: 0.433685\n",
      "Train Epoch: 0 [336640/482500 (70%)]\tLoss: 0.373149\n",
      "Train Epoch: 0 [336800/482500 (70%)]\tLoss: 0.476853\n",
      "Train Epoch: 0 [336960/482500 (70%)]\tLoss: 0.622817\n",
      "Train Epoch: 0 [337120/482500 (70%)]\tLoss: 0.332475\n",
      "Train Epoch: 0 [337280/482500 (70%)]\tLoss: 0.405959\n",
      "Train Epoch: 0 [337440/482500 (70%)]\tLoss: 0.832521\n",
      "Train Epoch: 0 [337600/482500 (70%)]\tLoss: 0.562683\n",
      "Train Epoch: 0 [337760/482500 (70%)]\tLoss: 0.321456\n",
      "Train Epoch: 0 [337920/482500 (70%)]\tLoss: 0.331320\n",
      "Train Epoch: 0 [338080/482500 (70%)]\tLoss: 0.586639\n",
      "Train Epoch: 0 [338240/482500 (70%)]\tLoss: 0.377884\n",
      "Train Epoch: 0 [338400/482500 (70%)]\tLoss: 0.488086\n",
      "Train Epoch: 0 [338560/482500 (70%)]\tLoss: 0.316149\n",
      "Train Epoch: 0 [338720/482500 (70%)]\tLoss: 0.413891\n",
      "Train Epoch: 0 [338880/482500 (70%)]\tLoss: 0.384832\n",
      "Train Epoch: 0 [339040/482500 (70%)]\tLoss: 0.384275\n",
      "Train Epoch: 0 [339200/482500 (70%)]\tLoss: 0.350205\n",
      "Train Epoch: 0 [339360/482500 (70%)]\tLoss: 0.774311\n",
      "Train Epoch: 0 [339520/482500 (70%)]\tLoss: 0.316258\n",
      "Train Epoch: 0 [339680/482500 (70%)]\tLoss: 0.399532\n",
      "Train Epoch: 0 [339840/482500 (70%)]\tLoss: 0.683155\n",
      "Train Epoch: 0 [340000/482500 (70%)]\tLoss: 0.382640\n",
      "Train Epoch: 0 [340160/482500 (71%)]\tLoss: 0.370035\n",
      "Train Epoch: 0 [340320/482500 (71%)]\tLoss: 2.855575\n",
      "Train Epoch: 0 [340480/482500 (71%)]\tLoss: 0.552126\n",
      "Train Epoch: 0 [340640/482500 (71%)]\tLoss: 0.284886\n",
      "Train Epoch: 0 [340800/482500 (71%)]\tLoss: 0.577743\n",
      "Train Epoch: 0 [340960/482500 (71%)]\tLoss: 0.588649\n",
      "Train Epoch: 0 [341120/482500 (71%)]\tLoss: 0.565361\n",
      "Train Epoch: 0 [341280/482500 (71%)]\tLoss: 0.462348\n",
      "Train Epoch: 0 [341440/482500 (71%)]\tLoss: 0.441835\n",
      "Train Epoch: 0 [341600/482500 (71%)]\tLoss: 1.667091\n",
      "Train Epoch: 0 [341760/482500 (71%)]\tLoss: 0.452583\n",
      "Train Epoch: 0 [341920/482500 (71%)]\tLoss: 0.291607\n",
      "Train Epoch: 0 [342080/482500 (71%)]\tLoss: 0.510308\n",
      "Train Epoch: 0 [342240/482500 (71%)]\tLoss: 0.445627\n",
      "Train Epoch: 0 [342400/482500 (71%)]\tLoss: 0.355251\n",
      "Train Epoch: 0 [342560/482500 (71%)]\tLoss: 0.717694\n",
      "Train Epoch: 0 [342720/482500 (71%)]\tLoss: 0.389621\n",
      "Train Epoch: 0 [342880/482500 (71%)]\tLoss: 0.328781\n",
      "Train Epoch: 0 [343040/482500 (71%)]\tLoss: 0.489701\n",
      "Train Epoch: 0 [343200/482500 (71%)]\tLoss: 0.366871\n",
      "Train Epoch: 0 [343360/482500 (71%)]\tLoss: 0.264786\n",
      "Train Epoch: 0 [343520/482500 (71%)]\tLoss: 0.539858\n",
      "Train Epoch: 0 [343680/482500 (71%)]\tLoss: 0.285888\n",
      "Train Epoch: 0 [343840/482500 (71%)]\tLoss: 0.351317\n",
      "Train Epoch: 0 [344000/482500 (71%)]\tLoss: 0.302358\n",
      "Train Epoch: 0 [344160/482500 (71%)]\tLoss: 0.186803\n",
      "Train Epoch: 0 [344320/482500 (71%)]\tLoss: 0.280745\n",
      "Train Epoch: 0 [344480/482500 (71%)]\tLoss: 0.301653\n",
      "Train Epoch: 0 [344640/482500 (71%)]\tLoss: 1.151299\n",
      "Train Epoch: 0 [344800/482500 (71%)]\tLoss: 0.338552\n",
      "Train Epoch: 0 [344960/482500 (71%)]\tLoss: 0.357089\n",
      "Train Epoch: 0 [345120/482500 (72%)]\tLoss: 0.489733\n",
      "Train Epoch: 0 [345280/482500 (72%)]\tLoss: 0.696771\n",
      "Train Epoch: 0 [345440/482500 (72%)]\tLoss: 0.337272\n",
      "Train Epoch: 0 [345600/482500 (72%)]\tLoss: 0.320333\n",
      "Train Epoch: 0 [345760/482500 (72%)]\tLoss: 0.855054\n",
      "Train Epoch: 0 [345920/482500 (72%)]\tLoss: 0.274495\n",
      "Train Epoch: 0 [346080/482500 (72%)]\tLoss: 0.387051\n",
      "Train Epoch: 0 [346240/482500 (72%)]\tLoss: 0.474891\n",
      "Train Epoch: 0 [346400/482500 (72%)]\tLoss: 0.427480\n",
      "Train Epoch: 0 [346560/482500 (72%)]\tLoss: 0.359070\n",
      "Train Epoch: 0 [346720/482500 (72%)]\tLoss: 1.177601\n",
      "Train Epoch: 0 [346880/482500 (72%)]\tLoss: 0.403424\n",
      "Train Epoch: 0 [347040/482500 (72%)]\tLoss: 0.598593\n",
      "Train Epoch: 0 [347200/482500 (72%)]\tLoss: 0.374678\n",
      "Train Epoch: 0 [347360/482500 (72%)]\tLoss: 0.327755\n",
      "Train Epoch: 0 [347520/482500 (72%)]\tLoss: 0.402422\n",
      "Train Epoch: 0 [347680/482500 (72%)]\tLoss: 0.488211\n",
      "Train Epoch: 0 [347840/482500 (72%)]\tLoss: 0.317664\n",
      "Train Epoch: 0 [348000/482500 (72%)]\tLoss: 0.353721\n",
      "Train Epoch: 0 [348160/482500 (72%)]\tLoss: 0.263877\n",
      "Train Epoch: 0 [348320/482500 (72%)]\tLoss: 15.232677\n",
      "Train Epoch: 0 [348480/482500 (72%)]\tLoss: 2.069232\n",
      "Train Epoch: 0 [348640/482500 (72%)]\tLoss: 1.668931\n",
      "Train Epoch: 0 [348800/482500 (72%)]\tLoss: 0.728728\n",
      "Train Epoch: 0 [348960/482500 (72%)]\tLoss: 0.586596\n",
      "Train Epoch: 0 [349120/482500 (72%)]\tLoss: 0.951166\n",
      "Train Epoch: 0 [349280/482500 (72%)]\tLoss: 0.720587\n",
      "Train Epoch: 0 [349440/482500 (72%)]\tLoss: 0.462625\n",
      "Train Epoch: 0 [349600/482500 (72%)]\tLoss: 0.654627\n",
      "Train Epoch: 0 [349760/482500 (72%)]\tLoss: 0.467553\n",
      "Train Epoch: 0 [349920/482500 (73%)]\tLoss: 0.528631\n",
      "Train Epoch: 0 [350080/482500 (73%)]\tLoss: 0.284936\n",
      "Train Epoch: 0 [350240/482500 (73%)]\tLoss: 0.761558\n",
      "Train Epoch: 0 [350400/482500 (73%)]\tLoss: 12.792522\n",
      "Train Epoch: 0 [350560/482500 (73%)]\tLoss: 1.355968\n",
      "Train Epoch: 0 [350720/482500 (73%)]\tLoss: 21.786320\n",
      "Train Epoch: 0 [350880/482500 (73%)]\tLoss: 2.160219\n",
      "Train Epoch: 0 [351040/482500 (73%)]\tLoss: 0.740108\n",
      "Train Epoch: 0 [351200/482500 (73%)]\tLoss: 0.607198\n",
      "Train Epoch: 0 [351360/482500 (73%)]\tLoss: 0.535000\n",
      "Train Epoch: 0 [351520/482500 (73%)]\tLoss: 0.568073\n",
      "Train Epoch: 0 [351680/482500 (73%)]\tLoss: 1.267455\n",
      "Train Epoch: 0 [351840/482500 (73%)]\tLoss: 2.962683\n",
      "Train Epoch: 0 [352000/482500 (73%)]\tLoss: 0.487272\n",
      "Train Epoch: 0 [352160/482500 (73%)]\tLoss: 0.590150\n",
      "Train Epoch: 0 [352320/482500 (73%)]\tLoss: 0.636300\n",
      "Train Epoch: 0 [352480/482500 (73%)]\tLoss: 0.506055\n",
      "Train Epoch: 0 [352640/482500 (73%)]\tLoss: 0.580033\n",
      "Train Epoch: 0 [352800/482500 (73%)]\tLoss: 0.457828\n",
      "Train Epoch: 0 [352960/482500 (73%)]\tLoss: 0.472913\n",
      "Train Epoch: 0 [353120/482500 (73%)]\tLoss: 0.396827\n",
      "Train Epoch: 0 [353280/482500 (73%)]\tLoss: 0.407356\n",
      "Train Epoch: 0 [353440/482500 (73%)]\tLoss: 0.468964\n",
      "Train Epoch: 0 [353600/482500 (73%)]\tLoss: 0.575937\n",
      "Train Epoch: 0 [353760/482500 (73%)]\tLoss: 0.291488\n",
      "Train Epoch: 0 [353920/482500 (73%)]\tLoss: 0.398953\n",
      "Train Epoch: 0 [354080/482500 (73%)]\tLoss: 0.375987\n",
      "Train Epoch: 0 [354240/482500 (73%)]\tLoss: 0.292215\n",
      "Train Epoch: 0 [354400/482500 (73%)]\tLoss: 0.288329\n",
      "Train Epoch: 0 [354560/482500 (73%)]\tLoss: 0.442086\n",
      "Train Epoch: 0 [354720/482500 (74%)]\tLoss: 0.678464\n",
      "Train Epoch: 0 [354880/482500 (74%)]\tLoss: 0.299655\n",
      "Train Epoch: 0 [355040/482500 (74%)]\tLoss: 0.260344\n",
      "Train Epoch: 0 [355200/482500 (74%)]\tLoss: 0.816706\n",
      "Train Epoch: 0 [355360/482500 (74%)]\tLoss: 0.457055\n",
      "Train Epoch: 0 [355520/482500 (74%)]\tLoss: 0.381995\n",
      "Train Epoch: 0 [355680/482500 (74%)]\tLoss: 0.343469\n",
      "Train Epoch: 0 [355840/482500 (74%)]\tLoss: 0.390273\n",
      "Train Epoch: 0 [356000/482500 (74%)]\tLoss: 0.370912\n",
      "Train Epoch: 0 [356160/482500 (74%)]\tLoss: 0.374049\n",
      "Train Epoch: 0 [356320/482500 (74%)]\tLoss: 0.239920\n",
      "Train Epoch: 0 [356480/482500 (74%)]\tLoss: 0.347958\n",
      "Train Epoch: 0 [356640/482500 (74%)]\tLoss: 0.361779\n",
      "Train Epoch: 0 [356800/482500 (74%)]\tLoss: 0.292378\n",
      "Train Epoch: 0 [356960/482500 (74%)]\tLoss: 0.372970\n",
      "Train Epoch: 0 [357120/482500 (74%)]\tLoss: 0.253898\n",
      "Train Epoch: 0 [357280/482500 (74%)]\tLoss: 0.334845\n",
      "Train Epoch: 0 [357440/482500 (74%)]\tLoss: 0.362480\n",
      "Train Epoch: 0 [357600/482500 (74%)]\tLoss: 0.463234\n",
      "Train Epoch: 0 [357760/482500 (74%)]\tLoss: 0.300836\n",
      "Train Epoch: 0 [357920/482500 (74%)]\tLoss: 0.276679\n",
      "Train Epoch: 0 [358080/482500 (74%)]\tLoss: 1.443356\n",
      "Train Epoch: 0 [358240/482500 (74%)]\tLoss: 1.444198\n",
      "Train Epoch: 0 [358400/482500 (74%)]\tLoss: 0.338851\n",
      "Train Epoch: 0 [358560/482500 (74%)]\tLoss: 0.325829\n",
      "Train Epoch: 0 [358720/482500 (74%)]\tLoss: 0.476075\n",
      "Train Epoch: 0 [358880/482500 (74%)]\tLoss: 0.231541\n",
      "Train Epoch: 0 [359040/482500 (74%)]\tLoss: 0.470582\n",
      "Train Epoch: 0 [359200/482500 (74%)]\tLoss: 0.247425\n",
      "Train Epoch: 0 [359360/482500 (74%)]\tLoss: 0.369747\n",
      "Train Epoch: 0 [359520/482500 (75%)]\tLoss: 0.354294\n",
      "Train Epoch: 0 [359680/482500 (75%)]\tLoss: 0.273450\n",
      "Train Epoch: 0 [359840/482500 (75%)]\tLoss: 0.641159\n",
      "Train Epoch: 0 [360000/482500 (75%)]\tLoss: 0.475455\n",
      "Train Epoch: 0 [360160/482500 (75%)]\tLoss: 0.377219\n",
      "Train Epoch: 0 [360320/482500 (75%)]\tLoss: 0.637632\n",
      "Train Epoch: 0 [360480/482500 (75%)]\tLoss: 0.422264\n",
      "Train Epoch: 0 [360640/482500 (75%)]\tLoss: 0.287841\n",
      "Train Epoch: 0 [360800/482500 (75%)]\tLoss: 0.442913\n",
      "Train Epoch: 0 [360960/482500 (75%)]\tLoss: 0.435245\n",
      "Train Epoch: 0 [361120/482500 (75%)]\tLoss: 0.703110\n",
      "Train Epoch: 0 [361280/482500 (75%)]\tLoss: 0.435658\n",
      "Train Epoch: 0 [361440/482500 (75%)]\tLoss: 0.714883\n",
      "Train Epoch: 0 [361600/482500 (75%)]\tLoss: 0.315461\n",
      "Train Epoch: 0 [361760/482500 (75%)]\tLoss: 0.243580\n",
      "Train Epoch: 0 [361920/482500 (75%)]\tLoss: 0.344165\n",
      "Train Epoch: 0 [362080/482500 (75%)]\tLoss: 0.354567\n",
      "Train Epoch: 0 [362240/482500 (75%)]\tLoss: 0.606803\n",
      "Train Epoch: 0 [362400/482500 (75%)]\tLoss: 0.442580\n",
      "Train Epoch: 0 [362560/482500 (75%)]\tLoss: 0.379523\n",
      "Train Epoch: 0 [362720/482500 (75%)]\tLoss: 0.257347\n",
      "Train Epoch: 0 [362880/482500 (75%)]\tLoss: 1.256248\n",
      "Train Epoch: 0 [363040/482500 (75%)]\tLoss: 0.582838\n",
      "Train Epoch: 0 [363200/482500 (75%)]\tLoss: 0.716579\n",
      "Train Epoch: 0 [363360/482500 (75%)]\tLoss: 0.495002\n",
      "Train Epoch: 0 [363520/482500 (75%)]\tLoss: 0.377929\n",
      "Train Epoch: 0 [363680/482500 (75%)]\tLoss: 0.450198\n",
      "Train Epoch: 0 [363840/482500 (75%)]\tLoss: 0.261802\n",
      "Train Epoch: 0 [364000/482500 (75%)]\tLoss: 0.956660\n",
      "Train Epoch: 0 [364160/482500 (75%)]\tLoss: 0.680067\n",
      "Train Epoch: 0 [364320/482500 (76%)]\tLoss: 0.384279\n",
      "Train Epoch: 0 [364480/482500 (76%)]\tLoss: 0.232047\n",
      "Train Epoch: 0 [364640/482500 (76%)]\tLoss: 0.545166\n",
      "Train Epoch: 0 [364800/482500 (76%)]\tLoss: 0.427442\n",
      "Train Epoch: 0 [364960/482500 (76%)]\tLoss: 0.500789\n",
      "Train Epoch: 0 [365120/482500 (76%)]\tLoss: 0.339959\n",
      "Train Epoch: 0 [365280/482500 (76%)]\tLoss: 0.479288\n",
      "Train Epoch: 0 [365440/482500 (76%)]\tLoss: 0.462405\n",
      "Train Epoch: 0 [365600/482500 (76%)]\tLoss: 0.302094\n",
      "Train Epoch: 0 [365760/482500 (76%)]\tLoss: 0.341104\n",
      "Train Epoch: 0 [365920/482500 (76%)]\tLoss: 0.275413\n",
      "Train Epoch: 0 [366080/482500 (76%)]\tLoss: 0.420808\n",
      "Train Epoch: 0 [366240/482500 (76%)]\tLoss: 0.334253\n",
      "Train Epoch: 0 [366400/482500 (76%)]\tLoss: 0.374120\n",
      "Train Epoch: 0 [366560/482500 (76%)]\tLoss: 0.243344\n",
      "Train Epoch: 0 [366720/482500 (76%)]\tLoss: 0.475881\n",
      "Train Epoch: 0 [366880/482500 (76%)]\tLoss: 0.339520\n",
      "Train Epoch: 0 [367040/482500 (76%)]\tLoss: 0.372082\n",
      "Train Epoch: 0 [367200/482500 (76%)]\tLoss: 0.273913\n",
      "Train Epoch: 0 [367360/482500 (76%)]\tLoss: 0.743327\n",
      "Train Epoch: 0 [367520/482500 (76%)]\tLoss: 0.526105\n",
      "Train Epoch: 0 [367680/482500 (76%)]\tLoss: 0.251933\n",
      "Train Epoch: 0 [367840/482500 (76%)]\tLoss: 1.102778\n",
      "Train Epoch: 0 [368000/482500 (76%)]\tLoss: 0.869289\n",
      "Train Epoch: 0 [368160/482500 (76%)]\tLoss: 0.403658\n",
      "Train Epoch: 0 [368320/482500 (76%)]\tLoss: 2.671960\n",
      "Train Epoch: 0 [368480/482500 (76%)]\tLoss: 1.466970\n",
      "Train Epoch: 0 [368640/482500 (76%)]\tLoss: 1.470910\n",
      "Train Epoch: 0 [368800/482500 (76%)]\tLoss: 0.320384\n",
      "Train Epoch: 0 [368960/482500 (76%)]\tLoss: 0.418977\n",
      "Train Epoch: 0 [369120/482500 (77%)]\tLoss: 0.302807\n",
      "Train Epoch: 0 [369280/482500 (77%)]\tLoss: 1.370964\n",
      "Train Epoch: 0 [369440/482500 (77%)]\tLoss: 2.196404\n",
      "Train Epoch: 0 [369600/482500 (77%)]\tLoss: 2.276284\n",
      "Train Epoch: 0 [369760/482500 (77%)]\tLoss: 0.994264\n",
      "Train Epoch: 0 [369920/482500 (77%)]\tLoss: 0.575853\n",
      "Train Epoch: 0 [370080/482500 (77%)]\tLoss: 0.499723\n",
      "Train Epoch: 0 [370240/482500 (77%)]\tLoss: 0.384873\n",
      "Train Epoch: 0 [370400/482500 (77%)]\tLoss: 0.546674\n",
      "Train Epoch: 0 [370560/482500 (77%)]\tLoss: 1.035180\n",
      "Train Epoch: 0 [370720/482500 (77%)]\tLoss: 0.497043\n",
      "Train Epoch: 0 [370880/482500 (77%)]\tLoss: 0.374279\n",
      "Train Epoch: 0 [371040/482500 (77%)]\tLoss: 0.324229\n",
      "Train Epoch: 0 [371200/482500 (77%)]\tLoss: 0.426112\n",
      "Train Epoch: 0 [371360/482500 (77%)]\tLoss: 0.448425\n",
      "Train Epoch: 0 [371520/482500 (77%)]\tLoss: 0.372052\n",
      "Train Epoch: 0 [371680/482500 (77%)]\tLoss: 0.330908\n",
      "Train Epoch: 0 [371840/482500 (77%)]\tLoss: 0.541048\n",
      "Train Epoch: 0 [372000/482500 (77%)]\tLoss: 0.394898\n",
      "Train Epoch: 0 [372160/482500 (77%)]\tLoss: 0.320801\n",
      "Train Epoch: 0 [372320/482500 (77%)]\tLoss: 0.309154\n",
      "Train Epoch: 0 [372480/482500 (77%)]\tLoss: 0.436710\n",
      "Train Epoch: 0 [372640/482500 (77%)]\tLoss: 0.434556\n",
      "Train Epoch: 0 [372800/482500 (77%)]\tLoss: 0.414547\n",
      "Train Epoch: 0 [372960/482500 (77%)]\tLoss: 0.319951\n",
      "Train Epoch: 0 [373120/482500 (77%)]\tLoss: 0.403508\n",
      "Train Epoch: 0 [373280/482500 (77%)]\tLoss: 0.259087\n",
      "Train Epoch: 0 [373440/482500 (77%)]\tLoss: 0.418281\n",
      "Train Epoch: 0 [373600/482500 (77%)]\tLoss: 0.548161\n",
      "Train Epoch: 0 [373760/482500 (77%)]\tLoss: 5.110073\n",
      "Train Epoch: 0 [373920/482500 (77%)]\tLoss: 3.432275\n",
      "Train Epoch: 0 [374080/482500 (78%)]\tLoss: 0.376948\n",
      "Train Epoch: 0 [374240/482500 (78%)]\tLoss: 1.333422\n",
      "Train Epoch: 0 [374400/482500 (78%)]\tLoss: 1.070542\n",
      "Train Epoch: 0 [374560/482500 (78%)]\tLoss: 0.675385\n",
      "Train Epoch: 0 [374720/482500 (78%)]\tLoss: 0.394043\n",
      "Train Epoch: 0 [374880/482500 (78%)]\tLoss: 0.719915\n",
      "Train Epoch: 0 [375040/482500 (78%)]\tLoss: 0.427585\n",
      "Train Epoch: 0 [375200/482500 (78%)]\tLoss: 0.417981\n",
      "Train Epoch: 0 [375360/482500 (78%)]\tLoss: 1.557013\n",
      "Train Epoch: 0 [375520/482500 (78%)]\tLoss: 0.462802\n",
      "Train Epoch: 0 [375680/482500 (78%)]\tLoss: 77.756020\n",
      "Train Epoch: 0 [375840/482500 (78%)]\tLoss: 5.104092\n",
      "Train Epoch: 0 [376000/482500 (78%)]\tLoss: 4.635632\n",
      "Train Epoch: 0 [376160/482500 (78%)]\tLoss: 63.489414\n",
      "Train Epoch: 0 [376320/482500 (78%)]\tLoss: 37.231709\n",
      "Train Epoch: 0 [376480/482500 (78%)]\tLoss: 10.465075\n",
      "Train Epoch: 0 [376640/482500 (78%)]\tLoss: 1.599410\n",
      "Train Epoch: 0 [376800/482500 (78%)]\tLoss: 2.532410\n",
      "Train Epoch: 0 [376960/482500 (78%)]\tLoss: 5.751409\n",
      "Train Epoch: 0 [377120/482500 (78%)]\tLoss: 1.816437\n",
      "Train Epoch: 0 [377280/482500 (78%)]\tLoss: 3.504595\n",
      "Train Epoch: 0 [377440/482500 (78%)]\tLoss: 2.853756\n",
      "Train Epoch: 0 [377600/482500 (78%)]\tLoss: 1.908544\n",
      "Train Epoch: 0 [377760/482500 (78%)]\tLoss: 1.303790\n",
      "Train Epoch: 0 [377920/482500 (78%)]\tLoss: 0.896200\n",
      "Train Epoch: 0 [378080/482500 (78%)]\tLoss: 2.297459\n",
      "Train Epoch: 0 [378240/482500 (78%)]\tLoss: 1.639027\n",
      "Train Epoch: 0 [378400/482500 (78%)]\tLoss: 1.899395\n",
      "Train Epoch: 0 [378560/482500 (78%)]\tLoss: 0.868305\n",
      "Train Epoch: 0 [378720/482500 (78%)]\tLoss: 0.913227\n",
      "Train Epoch: 0 [378880/482500 (79%)]\tLoss: 0.963302\n",
      "Train Epoch: 0 [379040/482500 (79%)]\tLoss: 0.893097\n",
      "Train Epoch: 0 [379200/482500 (79%)]\tLoss: 0.827449\n",
      "Train Epoch: 0 [379360/482500 (79%)]\tLoss: 1.137041\n",
      "Train Epoch: 0 [379520/482500 (79%)]\tLoss: 1.114173\n",
      "Train Epoch: 0 [379680/482500 (79%)]\tLoss: 1.552618\n",
      "Train Epoch: 0 [379840/482500 (79%)]\tLoss: 0.954960\n",
      "Train Epoch: 0 [380000/482500 (79%)]\tLoss: 0.735483\n",
      "Train Epoch: 0 [380160/482500 (79%)]\tLoss: 2.265823\n",
      "Train Epoch: 0 [380320/482500 (79%)]\tLoss: 2.130401\n",
      "Train Epoch: 0 [380480/482500 (79%)]\tLoss: 1.108825\n",
      "Train Epoch: 0 [380640/482500 (79%)]\tLoss: 0.631512\n",
      "Train Epoch: 0 [380800/482500 (79%)]\tLoss: 0.991760\n",
      "Train Epoch: 0 [380960/482500 (79%)]\tLoss: 0.885521\n",
      "Train Epoch: 0 [381120/482500 (79%)]\tLoss: 1.054436\n",
      "Train Epoch: 0 [381280/482500 (79%)]\tLoss: 0.861980\n",
      "Train Epoch: 0 [381440/482500 (79%)]\tLoss: 0.495840\n",
      "Train Epoch: 0 [381600/482500 (79%)]\tLoss: 0.844838\n",
      "Train Epoch: 0 [381760/482500 (79%)]\tLoss: 0.910038\n",
      "Train Epoch: 0 [381920/482500 (79%)]\tLoss: 0.763089\n",
      "Train Epoch: 0 [382080/482500 (79%)]\tLoss: 1.671699\n",
      "Train Epoch: 0 [382240/482500 (79%)]\tLoss: 0.810097\n",
      "Train Epoch: 0 [382400/482500 (79%)]\tLoss: 0.989172\n",
      "Train Epoch: 0 [382560/482500 (79%)]\tLoss: 0.886051\n",
      "Train Epoch: 0 [382720/482500 (79%)]\tLoss: 0.312991\n",
      "Train Epoch: 0 [382880/482500 (79%)]\tLoss: 0.624250\n",
      "Train Epoch: 0 [383040/482500 (79%)]\tLoss: 0.951583\n",
      "Train Epoch: 0 [383200/482500 (79%)]\tLoss: 0.486283\n",
      "Train Epoch: 0 [383360/482500 (79%)]\tLoss: 0.397112\n",
      "Train Epoch: 0 [383520/482500 (79%)]\tLoss: 0.595540\n",
      "Train Epoch: 0 [383680/482500 (80%)]\tLoss: 0.317732\n",
      "Train Epoch: 0 [383840/482500 (80%)]\tLoss: 0.319285\n",
      "Train Epoch: 0 [384000/482500 (80%)]\tLoss: 0.575765\n",
      "Train Epoch: 0 [384160/482500 (80%)]\tLoss: 0.536327\n",
      "Train Epoch: 0 [384320/482500 (80%)]\tLoss: 0.596695\n",
      "Train Epoch: 0 [384480/482500 (80%)]\tLoss: 0.512893\n",
      "Train Epoch: 0 [384640/482500 (80%)]\tLoss: 1.477489\n",
      "Train Epoch: 0 [384800/482500 (80%)]\tLoss: 0.650447\n",
      "Train Epoch: 0 [384960/482500 (80%)]\tLoss: 0.421203\n",
      "Train Epoch: 0 [385120/482500 (80%)]\tLoss: 0.476129\n",
      "Train Epoch: 0 [385280/482500 (80%)]\tLoss: 0.509960\n",
      "Train Epoch: 0 [385440/482500 (80%)]\tLoss: 0.626310\n",
      "Train Epoch: 0 [385600/482500 (80%)]\tLoss: 0.512432\n",
      "Train Epoch: 0 [385760/482500 (80%)]\tLoss: 0.499680\n",
      "Train Epoch: 0 [385920/482500 (80%)]\tLoss: 0.355329\n",
      "Train Epoch: 0 [386080/482500 (80%)]\tLoss: 0.995595\n",
      "Train Epoch: 0 [386240/482500 (80%)]\tLoss: 0.688354\n",
      "Train Epoch: 0 [386400/482500 (80%)]\tLoss: 0.577847\n",
      "Train Epoch: 0 [386560/482500 (80%)]\tLoss: 0.607040\n",
      "Train Epoch: 0 [386720/482500 (80%)]\tLoss: 0.657574\n",
      "Train Epoch: 0 [386880/482500 (80%)]\tLoss: 0.328064\n",
      "Train Epoch: 0 [387040/482500 (80%)]\tLoss: 0.507597\n",
      "Train Epoch: 0 [387200/482500 (80%)]\tLoss: 0.338394\n",
      "Train Epoch: 0 [387360/482500 (80%)]\tLoss: 0.747208\n",
      "Train Epoch: 0 [387520/482500 (80%)]\tLoss: 0.424443\n",
      "Train Epoch: 0 [387680/482500 (80%)]\tLoss: 0.524645\n",
      "Train Epoch: 0 [387840/482500 (80%)]\tLoss: 0.443565\n",
      "Train Epoch: 0 [388000/482500 (80%)]\tLoss: 0.248474\n",
      "Train Epoch: 0 [388160/482500 (80%)]\tLoss: 84.637169\n",
      "Train Epoch: 0 [388320/482500 (80%)]\tLoss: 1.725080\n",
      "Train Epoch: 0 [388480/482500 (81%)]\tLoss: 0.701712\n",
      "Train Epoch: 0 [388640/482500 (81%)]\tLoss: 0.411388\n",
      "Train Epoch: 0 [388800/482500 (81%)]\tLoss: 57.306908\n",
      "Train Epoch: 0 [388960/482500 (81%)]\tLoss: 5.657466\n",
      "Train Epoch: 0 [389120/482500 (81%)]\tLoss: 6.137001\n",
      "Train Epoch: 0 [389280/482500 (81%)]\tLoss: 2.233072\n",
      "Train Epoch: 0 [389440/482500 (81%)]\tLoss: 1.487899\n",
      "Train Epoch: 0 [389600/482500 (81%)]\tLoss: 0.581079\n",
      "Train Epoch: 0 [389760/482500 (81%)]\tLoss: 0.844436\n",
      "Train Epoch: 0 [389920/482500 (81%)]\tLoss: 0.694883\n",
      "Train Epoch: 0 [390080/482500 (81%)]\tLoss: 0.898443\n",
      "Train Epoch: 0 [390240/482500 (81%)]\tLoss: 0.853967\n",
      "Train Epoch: 0 [390400/482500 (81%)]\tLoss: 0.705200\n",
      "Train Epoch: 0 [390560/482500 (81%)]\tLoss: 0.754103\n",
      "Train Epoch: 0 [390720/482500 (81%)]\tLoss: 0.598631\n",
      "Train Epoch: 0 [390880/482500 (81%)]\tLoss: 0.889898\n",
      "Train Epoch: 0 [391040/482500 (81%)]\tLoss: 0.721994\n",
      "Train Epoch: 0 [391200/482500 (81%)]\tLoss: 0.729801\n",
      "Train Epoch: 0 [391360/482500 (81%)]\tLoss: 0.520961\n",
      "Train Epoch: 0 [391520/482500 (81%)]\tLoss: 0.668565\n",
      "Train Epoch: 0 [391680/482500 (81%)]\tLoss: 0.572790\n",
      "Train Epoch: 0 [391840/482500 (81%)]\tLoss: 0.679044\n",
      "Train Epoch: 0 [392000/482500 (81%)]\tLoss: 0.727599\n",
      "Train Epoch: 0 [392160/482500 (81%)]\tLoss: 0.660291\n",
      "Train Epoch: 0 [392320/482500 (81%)]\tLoss: 0.396991\n",
      "Train Epoch: 0 [392480/482500 (81%)]\tLoss: 0.509282\n",
      "Train Epoch: 0 [392640/482500 (81%)]\tLoss: 0.551013\n",
      "Train Epoch: 0 [392800/482500 (81%)]\tLoss: 0.732719\n",
      "Train Epoch: 0 [392960/482500 (81%)]\tLoss: 0.538430\n",
      "Train Epoch: 0 [393120/482500 (81%)]\tLoss: 0.845895\n",
      "Train Epoch: 0 [393280/482500 (82%)]\tLoss: 0.480930\n",
      "Train Epoch: 0 [393440/482500 (82%)]\tLoss: 0.486890\n",
      "Train Epoch: 0 [393600/482500 (82%)]\tLoss: 0.349437\n",
      "Train Epoch: 0 [393760/482500 (82%)]\tLoss: 0.267693\n",
      "Train Epoch: 0 [393920/482500 (82%)]\tLoss: 0.439434\n",
      "Train Epoch: 0 [394080/482500 (82%)]\tLoss: 0.565676\n",
      "Train Epoch: 0 [394240/482500 (82%)]\tLoss: 1.775984\n",
      "Train Epoch: 0 [394400/482500 (82%)]\tLoss: 0.511030\n",
      "Train Epoch: 0 [394560/482500 (82%)]\tLoss: 0.434680\n",
      "Train Epoch: 0 [394720/482500 (82%)]\tLoss: 0.828764\n",
      "Train Epoch: 0 [394880/482500 (82%)]\tLoss: 0.449624\n",
      "Train Epoch: 0 [395040/482500 (82%)]\tLoss: 0.680275\n",
      "Train Epoch: 0 [395200/482500 (82%)]\tLoss: 0.569333\n",
      "Train Epoch: 0 [395360/482500 (82%)]\tLoss: 0.484772\n",
      "Train Epoch: 0 [395520/482500 (82%)]\tLoss: 0.336749\n",
      "Train Epoch: 0 [395680/482500 (82%)]\tLoss: 0.469379\n",
      "Train Epoch: 0 [395840/482500 (82%)]\tLoss: 0.762511\n",
      "Train Epoch: 0 [396000/482500 (82%)]\tLoss: 1.200087\n",
      "Train Epoch: 0 [396160/482500 (82%)]\tLoss: 0.365207\n",
      "Train Epoch: 0 [396320/482500 (82%)]\tLoss: 0.295426\n",
      "Train Epoch: 0 [396480/482500 (82%)]\tLoss: 0.369687\n",
      "Train Epoch: 0 [396640/482500 (82%)]\tLoss: 0.521166\n",
      "Train Epoch: 0 [396800/482500 (82%)]\tLoss: 0.835107\n",
      "Train Epoch: 0 [396960/482500 (82%)]\tLoss: 0.882969\n",
      "Train Epoch: 0 [397120/482500 (82%)]\tLoss: 0.752354\n",
      "Train Epoch: 0 [397280/482500 (82%)]\tLoss: 0.471173\n",
      "Train Epoch: 0 [397440/482500 (82%)]\tLoss: 0.403627\n",
      "Train Epoch: 0 [397600/482500 (82%)]\tLoss: 0.610051\n",
      "Train Epoch: 0 [397760/482500 (82%)]\tLoss: 0.510045\n",
      "Train Epoch: 0 [397920/482500 (82%)]\tLoss: 0.376183\n",
      "Train Epoch: 0 [398080/482500 (83%)]\tLoss: 0.477117\n",
      "Train Epoch: 0 [398240/482500 (83%)]\tLoss: 0.234373\n",
      "Train Epoch: 0 [398400/482500 (83%)]\tLoss: 0.284553\n",
      "Train Epoch: 0 [398560/482500 (83%)]\tLoss: 0.388847\n",
      "Train Epoch: 0 [398720/482500 (83%)]\tLoss: 0.471824\n",
      "Train Epoch: 0 [398880/482500 (83%)]\tLoss: 0.439755\n",
      "Train Epoch: 0 [399040/482500 (83%)]\tLoss: 0.285568\n",
      "Train Epoch: 0 [399200/482500 (83%)]\tLoss: 0.434087\n",
      "Train Epoch: 0 [399360/482500 (83%)]\tLoss: 0.361832\n",
      "Train Epoch: 0 [399520/482500 (83%)]\tLoss: 0.367377\n",
      "Train Epoch: 0 [399680/482500 (83%)]\tLoss: 0.268134\n",
      "Train Epoch: 0 [399840/482500 (83%)]\tLoss: 0.410169\n",
      "Train Epoch: 0 [400000/482500 (83%)]\tLoss: 0.537002\n",
      "Train Epoch: 0 [400160/482500 (83%)]\tLoss: 0.301123\n",
      "Train Epoch: 0 [400320/482500 (83%)]\tLoss: 0.421721\n",
      "Train Epoch: 0 [400480/482500 (83%)]\tLoss: 0.461112\n",
      "Train Epoch: 0 [400640/482500 (83%)]\tLoss: 0.300765\n",
      "Train Epoch: 0 [400800/482500 (83%)]\tLoss: 0.504884\n",
      "Train Epoch: 0 [400960/482500 (83%)]\tLoss: 0.426448\n",
      "Train Epoch: 0 [401120/482500 (83%)]\tLoss: 18.941431\n",
      "Train Epoch: 0 [401280/482500 (83%)]\tLoss: 5.951497\n",
      "Train Epoch: 0 [401440/482500 (83%)]\tLoss: 0.963350\n",
      "Train Epoch: 0 [401600/482500 (83%)]\tLoss: 1.138620\n",
      "Train Epoch: 0 [401760/482500 (83%)]\tLoss: 1.038491\n",
      "Train Epoch: 0 [401920/482500 (83%)]\tLoss: 0.547289\n",
      "Train Epoch: 0 [402080/482500 (83%)]\tLoss: 18.618124\n",
      "Train Epoch: 0 [402240/482500 (83%)]\tLoss: 3.214543\n",
      "Train Epoch: 0 [402400/482500 (83%)]\tLoss: 1.024175\n",
      "Train Epoch: 0 [402560/482500 (83%)]\tLoss: 1.454378\n",
      "Train Epoch: 0 [402720/482500 (83%)]\tLoss: 0.891526\n",
      "Train Epoch: 0 [402880/482500 (83%)]\tLoss: 1.040345\n",
      "Train Epoch: 0 [403040/482500 (84%)]\tLoss: 0.617688\n",
      "Train Epoch: 0 [403200/482500 (84%)]\tLoss: 0.877431\n",
      "Train Epoch: 0 [403360/482500 (84%)]\tLoss: 0.456505\n",
      "Train Epoch: 0 [403520/482500 (84%)]\tLoss: 0.770686\n",
      "Train Epoch: 0 [403680/482500 (84%)]\tLoss: 0.593917\n",
      "Train Epoch: 0 [403840/482500 (84%)]\tLoss: 0.709434\n",
      "Train Epoch: 0 [404000/482500 (84%)]\tLoss: 0.605516\n",
      "Train Epoch: 0 [404160/482500 (84%)]\tLoss: 1.492765\n",
      "Train Epoch: 0 [404320/482500 (84%)]\tLoss: 0.761927\n",
      "Train Epoch: 0 [404480/482500 (84%)]\tLoss: 0.568684\n",
      "Train Epoch: 0 [404640/482500 (84%)]\tLoss: 0.605767\n",
      "Train Epoch: 0 [404800/482500 (84%)]\tLoss: 0.914342\n",
      "Train Epoch: 0 [404960/482500 (84%)]\tLoss: 0.716506\n",
      "Train Epoch: 0 [405120/482500 (84%)]\tLoss: 0.397350\n",
      "Train Epoch: 0 [405280/482500 (84%)]\tLoss: 3.800742\n",
      "Train Epoch: 0 [405440/482500 (84%)]\tLoss: 1.709015\n",
      "Train Epoch: 0 [405600/482500 (84%)]\tLoss: 0.868937\n",
      "Train Epoch: 0 [405760/482500 (84%)]\tLoss: 1.082651\n",
      "Train Epoch: 0 [405920/482500 (84%)]\tLoss: 0.649482\n",
      "Train Epoch: 0 [406080/482500 (84%)]\tLoss: 0.506997\n",
      "Train Epoch: 0 [406240/482500 (84%)]\tLoss: 0.572287\n",
      "Train Epoch: 0 [406400/482500 (84%)]\tLoss: 6.798546\n",
      "Train Epoch: 0 [406560/482500 (84%)]\tLoss: 0.939747\n",
      "Train Epoch: 0 [406720/482500 (84%)]\tLoss: 0.557739\n",
      "Train Epoch: 0 [406880/482500 (84%)]\tLoss: 0.568619\n",
      "Train Epoch: 0 [407040/482500 (84%)]\tLoss: 0.797330\n",
      "Train Epoch: 0 [407200/482500 (84%)]\tLoss: 0.922423\n",
      "Train Epoch: 0 [407360/482500 (84%)]\tLoss: 0.927071\n",
      "Train Epoch: 0 [407520/482500 (84%)]\tLoss: 0.426317\n",
      "Train Epoch: 0 [407680/482500 (84%)]\tLoss: 0.444939\n",
      "Train Epoch: 0 [407840/482500 (85%)]\tLoss: 0.516647\n",
      "Train Epoch: 0 [408000/482500 (85%)]\tLoss: 0.594088\n",
      "Train Epoch: 0 [408160/482500 (85%)]\tLoss: 0.713502\n",
      "Train Epoch: 0 [408320/482500 (85%)]\tLoss: 0.324617\n",
      "Train Epoch: 0 [408480/482500 (85%)]\tLoss: 0.543333\n",
      "Train Epoch: 0 [408640/482500 (85%)]\tLoss: 0.780843\n",
      "Train Epoch: 0 [408800/482500 (85%)]\tLoss: 0.626466\n",
      "Train Epoch: 0 [408960/482500 (85%)]\tLoss: 0.346968\n",
      "Train Epoch: 0 [409120/482500 (85%)]\tLoss: 0.513885\n",
      "Train Epoch: 0 [409280/482500 (85%)]\tLoss: 0.422556\n",
      "Train Epoch: 0 [409440/482500 (85%)]\tLoss: 0.368958\n",
      "Train Epoch: 0 [409600/482500 (85%)]\tLoss: 0.576020\n",
      "Train Epoch: 0 [409760/482500 (85%)]\tLoss: 0.523337\n",
      "Train Epoch: 0 [409920/482500 (85%)]\tLoss: 0.626138\n",
      "Train Epoch: 0 [410080/482500 (85%)]\tLoss: 0.437424\n",
      "Train Epoch: 0 [410240/482500 (85%)]\tLoss: 0.528479\n",
      "Train Epoch: 0 [410400/482500 (85%)]\tLoss: 2.653625\n",
      "Train Epoch: 0 [410560/482500 (85%)]\tLoss: 10.846326\n",
      "Train Epoch: 0 [410720/482500 (85%)]\tLoss: 3.618819\n",
      "Train Epoch: 0 [410880/482500 (85%)]\tLoss: 0.721345\n",
      "Train Epoch: 0 [411040/482500 (85%)]\tLoss: 0.748651\n",
      "Train Epoch: 0 [411200/482500 (85%)]\tLoss: 0.549063\n",
      "Train Epoch: 0 [411360/482500 (85%)]\tLoss: 0.445120\n",
      "Train Epoch: 0 [411520/482500 (85%)]\tLoss: 0.454731\n",
      "Train Epoch: 0 [411680/482500 (85%)]\tLoss: 0.336565\n",
      "Train Epoch: 0 [411840/482500 (85%)]\tLoss: 0.867299\n",
      "Train Epoch: 0 [412000/482500 (85%)]\tLoss: 0.456527\n",
      "Train Epoch: 0 [412160/482500 (85%)]\tLoss: 0.772390\n",
      "Train Epoch: 0 [412320/482500 (85%)]\tLoss: 0.495303\n",
      "Train Epoch: 0 [412480/482500 (85%)]\tLoss: 0.721250\n",
      "Train Epoch: 0 [412640/482500 (86%)]\tLoss: 0.936104\n",
      "Train Epoch: 0 [412800/482500 (86%)]\tLoss: 1.035708\n",
      "Train Epoch: 0 [412960/482500 (86%)]\tLoss: 0.338998\n",
      "Train Epoch: 0 [413120/482500 (86%)]\tLoss: 0.493269\n",
      "Train Epoch: 0 [413280/482500 (86%)]\tLoss: 0.880502\n",
      "Train Epoch: 0 [413440/482500 (86%)]\tLoss: 1.182429\n",
      "Train Epoch: 0 [413600/482500 (86%)]\tLoss: 0.470993\n",
      "Train Epoch: 0 [413760/482500 (86%)]\tLoss: 0.553931\n",
      "Train Epoch: 0 [413920/482500 (86%)]\tLoss: 0.527643\n",
      "Train Epoch: 0 [414080/482500 (86%)]\tLoss: 0.520449\n",
      "Train Epoch: 0 [414240/482500 (86%)]\tLoss: 0.404737\n",
      "Train Epoch: 0 [414400/482500 (86%)]\tLoss: 0.956439\n",
      "Train Epoch: 0 [414560/482500 (86%)]\tLoss: 9.737342\n",
      "Train Epoch: 0 [414720/482500 (86%)]\tLoss: 0.628040\n",
      "Train Epoch: 0 [414880/482500 (86%)]\tLoss: 1.109908\n",
      "Train Epoch: 0 [415040/482500 (86%)]\tLoss: 0.534915\n",
      "Train Epoch: 0 [415200/482500 (86%)]\tLoss: 0.993715\n",
      "Train Epoch: 0 [415360/482500 (86%)]\tLoss: 0.303298\n",
      "Train Epoch: 0 [415520/482500 (86%)]\tLoss: 0.982319\n",
      "Train Epoch: 0 [415680/482500 (86%)]\tLoss: 0.321069\n",
      "Train Epoch: 0 [415840/482500 (86%)]\tLoss: 0.448758\n",
      "Train Epoch: 0 [416000/482500 (86%)]\tLoss: 0.425078\n",
      "Train Epoch: 0 [416160/482500 (86%)]\tLoss: 0.286357\n",
      "Train Epoch: 0 [416320/482500 (86%)]\tLoss: 0.394260\n",
      "Train Epoch: 0 [416480/482500 (86%)]\tLoss: 0.494885\n",
      "Train Epoch: 0 [416640/482500 (86%)]\tLoss: 0.538853\n",
      "Train Epoch: 0 [416800/482500 (86%)]\tLoss: 0.412505\n",
      "Train Epoch: 0 [416960/482500 (86%)]\tLoss: 0.485368\n",
      "Train Epoch: 0 [417120/482500 (86%)]\tLoss: 18.465338\n",
      "Train Epoch: 0 [417280/482500 (86%)]\tLoss: 6.626276\n",
      "Train Epoch: 0 [417440/482500 (87%)]\tLoss: 1.156285\n",
      "Train Epoch: 0 [417600/482500 (87%)]\tLoss: 1.401698\n",
      "Train Epoch: 0 [417760/482500 (87%)]\tLoss: 0.826955\n",
      "Train Epoch: 0 [417920/482500 (87%)]\tLoss: 1.278699\n",
      "Train Epoch: 0 [418080/482500 (87%)]\tLoss: 1.002018\n",
      "Train Epoch: 0 [418240/482500 (87%)]\tLoss: 0.808677\n",
      "Train Epoch: 0 [418400/482500 (87%)]\tLoss: 0.741651\n",
      "Train Epoch: 0 [418560/482500 (87%)]\tLoss: 0.816127\n",
      "Train Epoch: 0 [418720/482500 (87%)]\tLoss: 0.900479\n",
      "Train Epoch: 0 [418880/482500 (87%)]\tLoss: 0.823372\n",
      "Train Epoch: 0 [419040/482500 (87%)]\tLoss: 1.124631\n",
      "Train Epoch: 0 [419200/482500 (87%)]\tLoss: 0.760830\n",
      "Train Epoch: 0 [419360/482500 (87%)]\tLoss: 0.825143\n",
      "Train Epoch: 0 [419520/482500 (87%)]\tLoss: 2.682732\n",
      "Train Epoch: 0 [419680/482500 (87%)]\tLoss: 1.020961\n",
      "Train Epoch: 0 [419840/482500 (87%)]\tLoss: 1.165712\n",
      "Train Epoch: 0 [420000/482500 (87%)]\tLoss: 0.899241\n",
      "Train Epoch: 0 [420160/482500 (87%)]\tLoss: 0.757800\n",
      "Train Epoch: 0 [420320/482500 (87%)]\tLoss: 0.600110\n",
      "Train Epoch: 0 [420480/482500 (87%)]\tLoss: 0.493318\n",
      "Train Epoch: 0 [420640/482500 (87%)]\tLoss: 0.556479\n",
      "Train Epoch: 0 [420800/482500 (87%)]\tLoss: 1.172440\n",
      "Train Epoch: 0 [420960/482500 (87%)]\tLoss: 0.504758\n",
      "Train Epoch: 0 [421120/482500 (87%)]\tLoss: 0.837633\n",
      "Train Epoch: 0 [421280/482500 (87%)]\tLoss: 0.556430\n",
      "Train Epoch: 0 [421440/482500 (87%)]\tLoss: 0.556811\n",
      "Train Epoch: 0 [421600/482500 (87%)]\tLoss: 0.529818\n",
      "Train Epoch: 0 [421760/482500 (87%)]\tLoss: 0.719970\n",
      "Train Epoch: 0 [421920/482500 (87%)]\tLoss: 0.529603\n",
      "Train Epoch: 0 [422080/482500 (87%)]\tLoss: 0.664981\n",
      "Train Epoch: 0 [422240/482500 (88%)]\tLoss: 0.503647\n",
      "Train Epoch: 0 [422400/482500 (88%)]\tLoss: 0.976818\n",
      "Train Epoch: 0 [422560/482500 (88%)]\tLoss: 0.481651\n",
      "Train Epoch: 0 [422720/482500 (88%)]\tLoss: 0.650340\n",
      "Train Epoch: 0 [422880/482500 (88%)]\tLoss: 0.429051\n",
      "Train Epoch: 0 [423040/482500 (88%)]\tLoss: 0.574067\n",
      "Train Epoch: 0 [423200/482500 (88%)]\tLoss: 0.372238\n",
      "Train Epoch: 0 [423360/482500 (88%)]\tLoss: 0.821171\n",
      "Train Epoch: 0 [423520/482500 (88%)]\tLoss: 0.488315\n",
      "Train Epoch: 0 [423680/482500 (88%)]\tLoss: 0.444171\n",
      "Train Epoch: 0 [423840/482500 (88%)]\tLoss: 0.534483\n",
      "Train Epoch: 0 [424000/482500 (88%)]\tLoss: 17.358255\n",
      "Train Epoch: 0 [424160/482500 (88%)]\tLoss: 2.228973\n",
      "Train Epoch: 0 [424320/482500 (88%)]\tLoss: 1.291633\n",
      "Train Epoch: 0 [424480/482500 (88%)]\tLoss: 1.012619\n",
      "Train Epoch: 0 [424640/482500 (88%)]\tLoss: 2.846695\n",
      "Train Epoch: 0 [424800/482500 (88%)]\tLoss: 1.687752\n",
      "Train Epoch: 0 [424960/482500 (88%)]\tLoss: 0.880489\n",
      "Train Epoch: 0 [425120/482500 (88%)]\tLoss: 1.229371\n",
      "Train Epoch: 0 [425280/482500 (88%)]\tLoss: 0.980271\n",
      "Train Epoch: 0 [425440/482500 (88%)]\tLoss: 0.370022\n",
      "Train Epoch: 0 [425600/482500 (88%)]\tLoss: 0.585361\n",
      "Train Epoch: 0 [425760/482500 (88%)]\tLoss: 0.357124\n",
      "Train Epoch: 0 [425920/482500 (88%)]\tLoss: 0.783509\n",
      "Train Epoch: 0 [426080/482500 (88%)]\tLoss: 0.527909\n",
      "Train Epoch: 0 [426240/482500 (88%)]\tLoss: 0.776327\n",
      "Train Epoch: 0 [426400/482500 (88%)]\tLoss: 0.715191\n",
      "Train Epoch: 0 [426560/482500 (88%)]\tLoss: 0.888390\n",
      "Train Epoch: 0 [426720/482500 (88%)]\tLoss: 1.861167\n",
      "Train Epoch: 0 [426880/482500 (88%)]\tLoss: 0.772186\n",
      "Train Epoch: 0 [427040/482500 (89%)]\tLoss: 0.827934\n",
      "Train Epoch: 0 [427200/482500 (89%)]\tLoss: 0.598822\n",
      "Train Epoch: 0 [427360/482500 (89%)]\tLoss: 0.928355\n",
      "Train Epoch: 0 [427520/482500 (89%)]\tLoss: 0.511343\n",
      "Train Epoch: 0 [427680/482500 (89%)]\tLoss: 0.542443\n",
      "Train Epoch: 0 [427840/482500 (89%)]\tLoss: 0.518247\n",
      "Train Epoch: 0 [428000/482500 (89%)]\tLoss: 0.746902\n",
      "Train Epoch: 0 [428160/482500 (89%)]\tLoss: 0.725682\n",
      "Train Epoch: 0 [428320/482500 (89%)]\tLoss: 0.881654\n",
      "Train Epoch: 0 [428480/482500 (89%)]\tLoss: 0.802968\n",
      "Train Epoch: 0 [428640/482500 (89%)]\tLoss: 1.296394\n",
      "Train Epoch: 0 [428800/482500 (89%)]\tLoss: 0.535113\n",
      "Train Epoch: 0 [428960/482500 (89%)]\tLoss: 0.777821\n",
      "Train Epoch: 0 [429120/482500 (89%)]\tLoss: 0.251315\n",
      "Train Epoch: 0 [429280/482500 (89%)]\tLoss: 0.436372\n",
      "Train Epoch: 0 [429440/482500 (89%)]\tLoss: 0.517125\n",
      "Train Epoch: 0 [429600/482500 (89%)]\tLoss: 0.537105\n",
      "Train Epoch: 0 [429760/482500 (89%)]\tLoss: 0.539655\n",
      "Train Epoch: 0 [429920/482500 (89%)]\tLoss: 0.309808\n",
      "Train Epoch: 0 [430080/482500 (89%)]\tLoss: 0.652501\n",
      "Train Epoch: 0 [430240/482500 (89%)]\tLoss: 0.446668\n",
      "Train Epoch: 0 [430400/482500 (89%)]\tLoss: 0.490773\n",
      "Train Epoch: 0 [430560/482500 (89%)]\tLoss: 0.534671\n",
      "Train Epoch: 0 [430720/482500 (89%)]\tLoss: 0.479965\n",
      "Train Epoch: 0 [430880/482500 (89%)]\tLoss: 0.685447\n",
      "Train Epoch: 0 [431040/482500 (89%)]\tLoss: 0.709023\n",
      "Train Epoch: 0 [431200/482500 (89%)]\tLoss: 0.583226\n",
      "Train Epoch: 0 [431360/482500 (89%)]\tLoss: 0.488281\n",
      "Train Epoch: 0 [431520/482500 (89%)]\tLoss: 0.984625\n",
      "Train Epoch: 0 [431680/482500 (89%)]\tLoss: 0.586519\n",
      "Train Epoch: 0 [431840/482500 (90%)]\tLoss: 0.475266\n",
      "Train Epoch: 0 [432000/482500 (90%)]\tLoss: 0.369511\n",
      "Train Epoch: 0 [432160/482500 (90%)]\tLoss: 0.388293\n",
      "Train Epoch: 0 [432320/482500 (90%)]\tLoss: 181.704010\n",
      "Train Epoch: 0 [432480/482500 (90%)]\tLoss: 4.247811\n",
      "Train Epoch: 0 [432640/482500 (90%)]\tLoss: 1.135677\n",
      "Train Epoch: 0 [432800/482500 (90%)]\tLoss: 0.855813\n",
      "Train Epoch: 0 [432960/482500 (90%)]\tLoss: 1.693406\n",
      "Train Epoch: 0 [433120/482500 (90%)]\tLoss: 0.959266\n",
      "Train Epoch: 0 [433280/482500 (90%)]\tLoss: 0.819633\n",
      "Train Epoch: 0 [433440/482500 (90%)]\tLoss: 1.018447\n",
      "Train Epoch: 0 [433600/482500 (90%)]\tLoss: 0.901471\n",
      "Train Epoch: 0 [433760/482500 (90%)]\tLoss: 0.449888\n",
      "Train Epoch: 0 [433920/482500 (90%)]\tLoss: 1.458904\n",
      "Train Epoch: 0 [434080/482500 (90%)]\tLoss: 0.856127\n",
      "Train Epoch: 0 [434240/482500 (90%)]\tLoss: 3.247799\n",
      "Train Epoch: 0 [434400/482500 (90%)]\tLoss: 0.827306\n",
      "Train Epoch: 0 [434560/482500 (90%)]\tLoss: 0.771605\n",
      "Train Epoch: 0 [434720/482500 (90%)]\tLoss: 1.046147\n",
      "Train Epoch: 0 [434880/482500 (90%)]\tLoss: 0.698961\n",
      "Train Epoch: 0 [435040/482500 (90%)]\tLoss: 0.697838\n",
      "Train Epoch: 0 [435200/482500 (90%)]\tLoss: 0.464291\n",
      "Train Epoch: 0 [435360/482500 (90%)]\tLoss: 0.551757\n",
      "Train Epoch: 0 [435520/482500 (90%)]\tLoss: 0.345221\n",
      "Train Epoch: 0 [435680/482500 (90%)]\tLoss: 0.697352\n",
      "Train Epoch: 0 [435840/482500 (90%)]\tLoss: 0.703519\n",
      "Train Epoch: 0 [436000/482500 (90%)]\tLoss: 1.006572\n",
      "Train Epoch: 0 [436160/482500 (90%)]\tLoss: 0.682796\n",
      "Train Epoch: 0 [436320/482500 (90%)]\tLoss: 0.735010\n",
      "Train Epoch: 0 [436480/482500 (90%)]\tLoss: 163.830734\n",
      "Train Epoch: 0 [436640/482500 (90%)]\tLoss: 8.470832\n",
      "Train Epoch: 0 [436800/482500 (91%)]\tLoss: 4.395140\n",
      "Train Epoch: 0 [436960/482500 (91%)]\tLoss: 0.834405\n",
      "Train Epoch: 0 [437120/482500 (91%)]\tLoss: 1.646352\n",
      "Train Epoch: 0 [437280/482500 (91%)]\tLoss: 1.452954\n",
      "Train Epoch: 0 [437440/482500 (91%)]\tLoss: 1.425092\n",
      "Train Epoch: 0 [437600/482500 (91%)]\tLoss: 1.256834\n",
      "Train Epoch: 0 [437760/482500 (91%)]\tLoss: 0.700600\n",
      "Train Epoch: 0 [437920/482500 (91%)]\tLoss: 0.994921\n",
      "Train Epoch: 0 [438080/482500 (91%)]\tLoss: 0.867292\n",
      "Train Epoch: 0 [438240/482500 (91%)]\tLoss: 1.441803\n",
      "Train Epoch: 0 [438400/482500 (91%)]\tLoss: 1.221606\n",
      "Train Epoch: 0 [438560/482500 (91%)]\tLoss: 0.601927\n",
      "Train Epoch: 0 [438720/482500 (91%)]\tLoss: 0.824721\n",
      "Train Epoch: 0 [438880/482500 (91%)]\tLoss: 0.431174\n",
      "Train Epoch: 0 [439040/482500 (91%)]\tLoss: 0.727237\n",
      "Train Epoch: 0 [439200/482500 (91%)]\tLoss: 0.513773\n",
      "Train Epoch: 0 [439360/482500 (91%)]\tLoss: 0.630180\n",
      "Train Epoch: 0 [439520/482500 (91%)]\tLoss: 0.681991\n",
      "Train Epoch: 0 [439680/482500 (91%)]\tLoss: 0.711972\n",
      "Train Epoch: 0 [439840/482500 (91%)]\tLoss: 0.931773\n",
      "Train Epoch: 0 [440000/482500 (91%)]\tLoss: 0.582841\n",
      "Train Epoch: 0 [440160/482500 (91%)]\tLoss: 0.799090\n",
      "Train Epoch: 0 [440320/482500 (91%)]\tLoss: 0.429526\n",
      "Train Epoch: 0 [440480/482500 (91%)]\tLoss: 0.493132\n",
      "Train Epoch: 0 [440640/482500 (91%)]\tLoss: 0.673220\n",
      "Train Epoch: 0 [440800/482500 (91%)]\tLoss: 0.820086\n",
      "Train Epoch: 0 [440960/482500 (91%)]\tLoss: 0.462446\n",
      "Train Epoch: 0 [441120/482500 (91%)]\tLoss: 0.614695\n",
      "Train Epoch: 0 [441280/482500 (91%)]\tLoss: 0.667161\n",
      "Train Epoch: 0 [441440/482500 (91%)]\tLoss: 0.615204\n",
      "Train Epoch: 0 [441600/482500 (92%)]\tLoss: 0.684371\n",
      "Train Epoch: 0 [441760/482500 (92%)]\tLoss: 0.755811\n",
      "Train Epoch: 0 [441920/482500 (92%)]\tLoss: 0.541547\n",
      "Train Epoch: 0 [442080/482500 (92%)]\tLoss: 0.801857\n",
      "Train Epoch: 0 [442240/482500 (92%)]\tLoss: 0.403017\n",
      "Train Epoch: 0 [442400/482500 (92%)]\tLoss: 0.545653\n",
      "Train Epoch: 0 [442560/482500 (92%)]\tLoss: 0.600245\n",
      "Train Epoch: 0 [442720/482500 (92%)]\tLoss: 0.685620\n",
      "Train Epoch: 0 [442880/482500 (92%)]\tLoss: 0.619916\n",
      "Train Epoch: 0 [443040/482500 (92%)]\tLoss: 0.756135\n",
      "Train Epoch: 0 [443200/482500 (92%)]\tLoss: 1.235337\n",
      "Train Epoch: 0 [443360/482500 (92%)]\tLoss: 0.582553\n",
      "Train Epoch: 0 [443520/482500 (92%)]\tLoss: 0.454701\n",
      "Train Epoch: 0 [443680/482500 (92%)]\tLoss: 0.601705\n",
      "Train Epoch: 0 [443840/482500 (92%)]\tLoss: 0.792323\n",
      "Train Epoch: 0 [444000/482500 (92%)]\tLoss: 0.498768\n",
      "Train Epoch: 0 [444160/482500 (92%)]\tLoss: 0.440443\n",
      "Train Epoch: 0 [444320/482500 (92%)]\tLoss: 0.334826\n",
      "Train Epoch: 0 [444480/482500 (92%)]\tLoss: 2.265977\n",
      "Train Epoch: 0 [444640/482500 (92%)]\tLoss: 1.123284\n",
      "Train Epoch: 0 [444800/482500 (92%)]\tLoss: 2.557987\n",
      "Train Epoch: 0 [444960/482500 (92%)]\tLoss: 31.238075\n",
      "Train Epoch: 0 [445120/482500 (92%)]\tLoss: 3.214186\n",
      "Train Epoch: 0 [445280/482500 (92%)]\tLoss: 1.732669\n",
      "Train Epoch: 0 [445440/482500 (92%)]\tLoss: 2.353667\n",
      "Train Epoch: 0 [445600/482500 (92%)]\tLoss: 0.690764\n",
      "Train Epoch: 0 [445760/482500 (92%)]\tLoss: 1.267272\n",
      "Train Epoch: 0 [445920/482500 (92%)]\tLoss: 1.917776\n",
      "Train Epoch: 0 [446080/482500 (92%)]\tLoss: 0.973910\n",
      "Train Epoch: 0 [446240/482500 (92%)]\tLoss: 1.297774\n",
      "Train Epoch: 0 [446400/482500 (93%)]\tLoss: 1.100917\n",
      "Train Epoch: 0 [446560/482500 (93%)]\tLoss: 1.043356\n",
      "Train Epoch: 0 [446720/482500 (93%)]\tLoss: 1.065917\n",
      "Train Epoch: 0 [446880/482500 (93%)]\tLoss: 0.791620\n",
      "Train Epoch: 0 [447040/482500 (93%)]\tLoss: 1.049743\n",
      "Train Epoch: 0 [447200/482500 (93%)]\tLoss: 1.271582\n",
      "Train Epoch: 0 [447360/482500 (93%)]\tLoss: 1.109486\n",
      "Train Epoch: 0 [447520/482500 (93%)]\tLoss: 0.539624\n",
      "Train Epoch: 0 [447680/482500 (93%)]\tLoss: 0.644691\n",
      "Train Epoch: 0 [447840/482500 (93%)]\tLoss: 0.877848\n",
      "Train Epoch: 0 [448000/482500 (93%)]\tLoss: 0.587637\n",
      "Train Epoch: 0 [448160/482500 (93%)]\tLoss: 0.792024\n",
      "Train Epoch: 0 [448320/482500 (93%)]\tLoss: 0.797648\n",
      "Train Epoch: 0 [448480/482500 (93%)]\tLoss: 0.608908\n",
      "Train Epoch: 0 [448640/482500 (93%)]\tLoss: 1.028182\n",
      "Train Epoch: 0 [448800/482500 (93%)]\tLoss: 0.933967\n",
      "Train Epoch: 0 [448960/482500 (93%)]\tLoss: 0.697991\n",
      "Train Epoch: 0 [449120/482500 (93%)]\tLoss: 0.528033\n",
      "Train Epoch: 0 [449280/482500 (93%)]\tLoss: 0.978063\n",
      "Train Epoch: 0 [449440/482500 (93%)]\tLoss: 0.521611\n",
      "Train Epoch: 0 [449600/482500 (93%)]\tLoss: 0.623670\n",
      "Train Epoch: 0 [449760/482500 (93%)]\tLoss: 0.649867\n",
      "Train Epoch: 0 [449920/482500 (93%)]\tLoss: 0.624046\n",
      "Train Epoch: 0 [450080/482500 (93%)]\tLoss: 21.139919\n",
      "Train Epoch: 0 [450240/482500 (93%)]\tLoss: 5.305130\n",
      "Train Epoch: 0 [450400/482500 (93%)]\tLoss: 1.642384\n",
      "Train Epoch: 0 [450560/482500 (93%)]\tLoss: 1.733399\n",
      "Train Epoch: 0 [450720/482500 (93%)]\tLoss: 0.748380\n",
      "Train Epoch: 0 [450880/482500 (93%)]\tLoss: 0.706073\n",
      "Train Epoch: 0 [451040/482500 (93%)]\tLoss: 0.490124\n",
      "Train Epoch: 0 [451200/482500 (94%)]\tLoss: 0.816551\n",
      "Train Epoch: 0 [451360/482500 (94%)]\tLoss: 3.295242\n",
      "Train Epoch: 0 [451520/482500 (94%)]\tLoss: 0.814043\n",
      "Train Epoch: 0 [451680/482500 (94%)]\tLoss: 0.536577\n",
      "Train Epoch: 0 [451840/482500 (94%)]\tLoss: 0.976853\n",
      "Train Epoch: 0 [452000/482500 (94%)]\tLoss: 0.408373\n",
      "Train Epoch: 0 [452160/482500 (94%)]\tLoss: 0.566801\n",
      "Train Epoch: 0 [452320/482500 (94%)]\tLoss: 0.465063\n",
      "Train Epoch: 0 [452480/482500 (94%)]\tLoss: 0.451838\n",
      "Train Epoch: 0 [452640/482500 (94%)]\tLoss: 0.558807\n",
      "Train Epoch: 0 [452800/482500 (94%)]\tLoss: 0.423136\n",
      "Train Epoch: 0 [452960/482500 (94%)]\tLoss: 0.452780\n",
      "Train Epoch: 0 [453120/482500 (94%)]\tLoss: 0.486841\n",
      "Train Epoch: 0 [453280/482500 (94%)]\tLoss: 1.528471\n",
      "Train Epoch: 0 [453440/482500 (94%)]\tLoss: 1.108343\n",
      "Train Epoch: 0 [453600/482500 (94%)]\tLoss: 0.832914\n",
      "Train Epoch: 0 [453760/482500 (94%)]\tLoss: 1.241387\n",
      "Train Epoch: 0 [453920/482500 (94%)]\tLoss: 0.572207\n",
      "Train Epoch: 0 [454080/482500 (94%)]\tLoss: 0.623349\n",
      "Train Epoch: 0 [454240/482500 (94%)]\tLoss: 0.321250\n",
      "Train Epoch: 0 [454400/482500 (94%)]\tLoss: 0.682083\n",
      "Train Epoch: 0 [454560/482500 (94%)]\tLoss: 0.700476\n",
      "Train Epoch: 0 [454720/482500 (94%)]\tLoss: 0.421534\n",
      "Train Epoch: 0 [454880/482500 (94%)]\tLoss: 0.398301\n",
      "Train Epoch: 0 [455040/482500 (94%)]\tLoss: 0.729522\n",
      "Train Epoch: 0 [455200/482500 (94%)]\tLoss: 0.569925\n",
      "Train Epoch: 0 [455360/482500 (94%)]\tLoss: 0.472581\n",
      "Train Epoch: 0 [455520/482500 (94%)]\tLoss: 0.830376\n",
      "Train Epoch: 0 [455680/482500 (94%)]\tLoss: 0.359734\n",
      "Train Epoch: 0 [455840/482500 (94%)]\tLoss: 0.343009\n",
      "Train Epoch: 0 [456000/482500 (95%)]\tLoss: 0.851542\n",
      "Train Epoch: 0 [456160/482500 (95%)]\tLoss: 0.449088\n",
      "Train Epoch: 0 [456320/482500 (95%)]\tLoss: 0.749952\n",
      "Train Epoch: 0 [456480/482500 (95%)]\tLoss: 0.522724\n",
      "Train Epoch: 0 [456640/482500 (95%)]\tLoss: 0.394127\n",
      "Train Epoch: 0 [456800/482500 (95%)]\tLoss: 0.378340\n",
      "Train Epoch: 0 [456960/482500 (95%)]\tLoss: 0.555426\n",
      "Train Epoch: 0 [457120/482500 (95%)]\tLoss: 0.603574\n",
      "Train Epoch: 0 [457280/482500 (95%)]\tLoss: 0.595363\n",
      "Train Epoch: 0 [457440/482500 (95%)]\tLoss: 0.742198\n",
      "Train Epoch: 0 [457600/482500 (95%)]\tLoss: 0.554955\n",
      "Train Epoch: 0 [457760/482500 (95%)]\tLoss: 0.490875\n",
      "Train Epoch: 0 [457920/482500 (95%)]\tLoss: 0.738506\n",
      "Train Epoch: 0 [458080/482500 (95%)]\tLoss: 0.280759\n",
      "Train Epoch: 0 [458240/482500 (95%)]\tLoss: 0.408171\n",
      "Train Epoch: 0 [458400/482500 (95%)]\tLoss: 0.377255\n",
      "Train Epoch: 0 [458560/482500 (95%)]\tLoss: 0.700681\n",
      "Train Epoch: 0 [458720/482500 (95%)]\tLoss: 0.449637\n",
      "Train Epoch: 0 [458880/482500 (95%)]\tLoss: 0.345698\n",
      "Train Epoch: 0 [459040/482500 (95%)]\tLoss: 0.627130\n",
      "Train Epoch: 0 [459200/482500 (95%)]\tLoss: 0.438436\n",
      "Train Epoch: 0 [459360/482500 (95%)]\tLoss: 0.638750\n",
      "Train Epoch: 0 [459520/482500 (95%)]\tLoss: 0.499139\n",
      "Train Epoch: 0 [459680/482500 (95%)]\tLoss: 0.509047\n",
      "Train Epoch: 0 [459840/482500 (95%)]\tLoss: 0.263660\n",
      "Train Epoch: 0 [460000/482500 (95%)]\tLoss: 0.535463\n",
      "Train Epoch: 0 [460160/482500 (95%)]\tLoss: 0.448080\n",
      "Train Epoch: 0 [460320/482500 (95%)]\tLoss: 0.413149\n",
      "Train Epoch: 0 [460480/482500 (95%)]\tLoss: 0.326228\n",
      "Train Epoch: 0 [460640/482500 (95%)]\tLoss: 0.683387\n",
      "Train Epoch: 0 [460800/482500 (96%)]\tLoss: 0.422710\n",
      "Train Epoch: 0 [460960/482500 (96%)]\tLoss: 0.500315\n",
      "Train Epoch: 0 [461120/482500 (96%)]\tLoss: 0.407052\n",
      "Train Epoch: 0 [461280/482500 (96%)]\tLoss: 0.469620\n",
      "Train Epoch: 0 [461440/482500 (96%)]\tLoss: 0.275865\n",
      "Train Epoch: 0 [461600/482500 (96%)]\tLoss: 0.740851\n",
      "Train Epoch: 0 [461760/482500 (96%)]\tLoss: 0.477696\n",
      "Train Epoch: 0 [461920/482500 (96%)]\tLoss: 0.816725\n",
      "Train Epoch: 0 [462080/482500 (96%)]\tLoss: 0.429752\n",
      "Train Epoch: 0 [462240/482500 (96%)]\tLoss: 0.448046\n",
      "Train Epoch: 0 [462400/482500 (96%)]\tLoss: 0.303764\n",
      "Train Epoch: 0 [462560/482500 (96%)]\tLoss: 0.294066\n",
      "Train Epoch: 0 [462720/482500 (96%)]\tLoss: 0.433966\n",
      "Train Epoch: 0 [462880/482500 (96%)]\tLoss: 0.618946\n",
      "Train Epoch: 0 [463040/482500 (96%)]\tLoss: 0.445804\n",
      "Train Epoch: 0 [463200/482500 (96%)]\tLoss: 0.785329\n",
      "Train Epoch: 0 [463360/482500 (96%)]\tLoss: 0.247213\n",
      "Train Epoch: 0 [463520/482500 (96%)]\tLoss: 0.380145\n",
      "Train Epoch: 0 [463680/482500 (96%)]\tLoss: 19.875427\n",
      "Train Epoch: 0 [463840/482500 (96%)]\tLoss: 3.930805\n",
      "Train Epoch: 0 [464000/482500 (96%)]\tLoss: 0.948658\n",
      "Train Epoch: 0 [464160/482500 (96%)]\tLoss: 1.010977\n",
      "Train Epoch: 0 [464320/482500 (96%)]\tLoss: 0.907229\n",
      "Train Epoch: 0 [464480/482500 (96%)]\tLoss: 0.780275\n",
      "Train Epoch: 0 [464640/482500 (96%)]\tLoss: 0.349739\n",
      "Train Epoch: 0 [464800/482500 (96%)]\tLoss: 0.469541\n",
      "Train Epoch: 0 [464960/482500 (96%)]\tLoss: 0.429038\n",
      "Train Epoch: 0 [465120/482500 (96%)]\tLoss: 0.490618\n",
      "Train Epoch: 0 [465280/482500 (96%)]\tLoss: 0.642089\n",
      "Train Epoch: 0 [465440/482500 (96%)]\tLoss: 0.504835\n",
      "Train Epoch: 0 [465600/482500 (96%)]\tLoss: 0.554118\n",
      "Train Epoch: 0 [465760/482500 (97%)]\tLoss: 0.534548\n",
      "Train Epoch: 0 [465920/482500 (97%)]\tLoss: 0.619521\n",
      "Train Epoch: 0 [466080/482500 (97%)]\tLoss: 0.515484\n",
      "Train Epoch: 0 [466240/482500 (97%)]\tLoss: 0.637081\n",
      "Train Epoch: 0 [466400/482500 (97%)]\tLoss: 0.569202\n",
      "Train Epoch: 0 [466560/482500 (97%)]\tLoss: 0.652390\n",
      "Train Epoch: 0 [466720/482500 (97%)]\tLoss: 0.594069\n",
      "Train Epoch: 0 [466880/482500 (97%)]\tLoss: 0.791203\n",
      "Train Epoch: 0 [467040/482500 (97%)]\tLoss: 0.379591\n",
      "Train Epoch: 0 [467200/482500 (97%)]\tLoss: 0.553386\n",
      "Train Epoch: 0 [467360/482500 (97%)]\tLoss: 0.317180\n",
      "Train Epoch: 0 [467520/482500 (97%)]\tLoss: 0.636356\n",
      "Train Epoch: 0 [467680/482500 (97%)]\tLoss: 0.495101\n",
      "Train Epoch: 0 [467840/482500 (97%)]\tLoss: 0.356537\n",
      "Train Epoch: 0 [468000/482500 (97%)]\tLoss: 0.476073\n",
      "Train Epoch: 0 [468160/482500 (97%)]\tLoss: 0.478784\n",
      "Train Epoch: 0 [468320/482500 (97%)]\tLoss: 0.471608\n",
      "Train Epoch: 0 [468480/482500 (97%)]\tLoss: 0.446437\n",
      "Train Epoch: 0 [468640/482500 (97%)]\tLoss: 0.472076\n",
      "Train Epoch: 0 [468800/482500 (97%)]\tLoss: 0.553421\n",
      "Train Epoch: 0 [468960/482500 (97%)]\tLoss: 0.542783\n",
      "Train Epoch: 0 [469120/482500 (97%)]\tLoss: 0.427081\n",
      "Train Epoch: 0 [469280/482500 (97%)]\tLoss: 0.586054\n",
      "Train Epoch: 0 [469440/482500 (97%)]\tLoss: 0.333391\n",
      "Train Epoch: 0 [469600/482500 (97%)]\tLoss: 0.365847\n",
      "Train Epoch: 0 [469760/482500 (97%)]\tLoss: 0.352127\n",
      "Train Epoch: 0 [469920/482500 (97%)]\tLoss: 0.703183\n",
      "Train Epoch: 0 [470080/482500 (97%)]\tLoss: 0.338300\n",
      "Train Epoch: 0 [470240/482500 (97%)]\tLoss: 0.845491\n",
      "Train Epoch: 0 [470400/482500 (97%)]\tLoss: 0.295294\n",
      "Train Epoch: 0 [470560/482500 (98%)]\tLoss: 0.507483\n",
      "Train Epoch: 0 [470720/482500 (98%)]\tLoss: 0.343724\n",
      "Train Epoch: 0 [470880/482500 (98%)]\tLoss: 0.332410\n",
      "Train Epoch: 0 [471040/482500 (98%)]\tLoss: 0.611465\n",
      "Train Epoch: 0 [471200/482500 (98%)]\tLoss: 0.549901\n",
      "Train Epoch: 0 [471360/482500 (98%)]\tLoss: 0.344892\n",
      "Train Epoch: 0 [471520/482500 (98%)]\tLoss: 0.295313\n",
      "Train Epoch: 0 [471680/482500 (98%)]\tLoss: 0.304314\n",
      "Train Epoch: 0 [471840/482500 (98%)]\tLoss: 0.305427\n",
      "Train Epoch: 0 [472000/482500 (98%)]\tLoss: 0.585310\n",
      "Train Epoch: 0 [472160/482500 (98%)]\tLoss: 0.522391\n",
      "Train Epoch: 0 [472320/482500 (98%)]\tLoss: 0.374862\n",
      "Train Epoch: 0 [472480/482500 (98%)]\tLoss: 0.428311\n",
      "Train Epoch: 0 [472640/482500 (98%)]\tLoss: 0.340097\n",
      "Train Epoch: 0 [472800/482500 (98%)]\tLoss: 0.363948\n",
      "Train Epoch: 0 [472960/482500 (98%)]\tLoss: 0.287285\n",
      "Train Epoch: 0 [473120/482500 (98%)]\tLoss: 0.447299\n",
      "Train Epoch: 0 [473280/482500 (98%)]\tLoss: 0.451797\n",
      "Train Epoch: 0 [473440/482500 (98%)]\tLoss: 0.414511\n",
      "Train Epoch: 0 [473600/482500 (98%)]\tLoss: 0.665530\n",
      "Train Epoch: 0 [473760/482500 (98%)]\tLoss: 0.330359\n",
      "Train Epoch: 0 [473920/482500 (98%)]\tLoss: 0.465490\n",
      "Train Epoch: 0 [474080/482500 (98%)]\tLoss: 0.567172\n",
      "Train Epoch: 0 [474240/482500 (98%)]\tLoss: 0.418034\n",
      "Train Epoch: 0 [474400/482500 (98%)]\tLoss: 0.367716\n",
      "Train Epoch: 0 [474560/482500 (98%)]\tLoss: 0.376711\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-af950977bd11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mlog_interval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_mse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#     vis.line(X=torch.ones((1,1)).cpu()*epoch,Y=torch.Tensor([epoch_loss]).unsqueeze(0).cpu(),win=loss_window,update='append',name='loss')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#         vis.line(X=torch.ones((1,1)).cpu()*epoch,Y=torch.Tensor([epoch_mse]).unsqueeze(0).cpu(),win=loss_window,update='append',name='mse_loss')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-20a261bc001c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, writer)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mmse_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epoch, epoch + 200):\n",
    "    log_interval = 20\n",
    "    epoch, epoch_loss, epoch_mse = train(epoch, writer)\n",
    "#     vis.line(X=torch.ones((1,1)).cpu()*epoch,Y=torch.Tensor([epoch_loss]).unsqueeze(0).cpu(),win=loss_window,update='append',name='loss')\n",
    "#         vis.line(X=torch.ones((1,1)).cpu()*epoch,Y=torch.Tensor([epoch_mse]).unsqueeze(0).cpu(),win=loss_window,update='append',name='mse_loss')\n",
    "    writer.add_scalar('Loss/loss', epoch_loss, epoch)\n",
    "    writer.add_scalar('Loss/mse_loss', epoch_mse, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict()\n",
    "            }, 'checkpoints/highway_conv.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda:0\n",
      "convVAE(\n",
      "  (condnn): CondNN(\n",
      "    (cnn): Conv3d(\n",
      "      (adap_pool): AdaptiveAvgPool3d(output_size=(25, 100, 600))\n",
      "      (conv_layer1): Sequential(\n",
      "        (0): Conv3d(1, 16, kernel_size=(2, 3, 3), stride=(1, 1, 1))\n",
      "        (1): LeakyReLU(negative_slope=0.01)\n",
      "        (2): Conv3d(16, 16, kernel_size=(2, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (3): LeakyReLU(negative_slope=0.01)\n",
      "        (4): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (conv_layer2): Sequential(\n",
      "        (0): Conv3d(16, 32, kernel_size=(2, 3, 3), stride=(1, 1, 1))\n",
      "        (1): LeakyReLU(negative_slope=0.01)\n",
      "        (2): Conv3d(32, 32, kernel_size=(2, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (3): LeakyReLU(negative_slope=0.01)\n",
      "        (4): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (conv_layer5): Conv3d(32, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1))\n",
      "      (adap_pool2): AdaptiveAvgPool3d(output_size=(1, 1, 1))\n",
      "      (fc5): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (relu): LeakyReLU(negative_slope=0.01)\n",
      "      (batch0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (drop): Dropout(p=0.15, inplace=False)\n",
      "      (fc6): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (batch1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (fc7): Linear(in_features=64, out_features=300, bias=True)\n",
      "    )\n",
      "    (fc2): Linear(in_features=308, out_features=300, bias=True)\n",
      "  )\n",
      "  (encoder): Encoder(\n",
      "    (sequential): Sequential(\n",
      "      (0): Linear(in_features=304, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=1024, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "    (linear_means): Linear(in_features=512, out_features=50, bias=True)\n",
      "    (linear_log_var): Linear(in_features=512, out_features=50, bias=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (sequential): Sequential(\n",
      "      (0): Linear(in_features=350, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=1024, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (5): ReLU()\n",
      "      (6): Linear(in_features=512, out_features=4, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device\", device)\n",
    "\n",
    "model = convVAE(sample_size = X_dim, \n",
    "                  cnnout_size = cnn_out_size, \n",
    "                  cond_out_size = cond_out_size, \n",
    "                  encoder_layer_sizes = [512,1024,512], \n",
    "                  latent_size = z_dim, \n",
    "                  decoder_layer_sizes = [512,1024,512]).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "checkpoint = torch.load('checkpoints/highway_conv.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "\n",
    "model.eval()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "476184\n"
     ]
    }
   ],
   "source": [
    "test_data = test_loader.dataset\n",
    "viz_idx =   torch.randint(0,len(test_data),[1]).item()  \n",
    "#  idx\n",
    "#  \n",
    "print(viz_idx)\n",
    "\n",
    "batch = test_data[viz_idx]\n",
    "startgoal = torch.from_numpy(batch[\"start_goal\"]).to(device)\n",
    "occ = torch.from_numpy(batch[\"observation\"])\n",
    "occ = occ.unsqueeze(0)\n",
    "occ = occ.unsqueeze(1)\n",
    "adap_pool = nn.AdaptiveAvgPool3d((25,100, 600))\n",
    "occ = adap_pool(occ)\n",
    "occ = occ.to(device)\n",
    "data = torch.from_numpy(batch[\"data\"]).to(device)\n",
    "\n",
    "occ=occ.cpu().detach().numpy()\n",
    "startgoal=startgoal.cpu().detach().numpy()\n",
    "data=data.cpu().detach().numpy()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "plotData(occ, startgoal, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "308958\n"
     ]
    }
   ],
   "source": [
    "test_data = test_loader.dataset\n",
    "viz_idx =   torch.randint(0,len(test_data),[1]).item()  \n",
    "#  idx\n",
    "#  308958 82146 161608\n",
    "viz_idx = 308958\n",
    "print(viz_idx)\n",
    "\n",
    "batch = test_data[viz_idx]\n",
    "startgoal = torch.from_numpy(batch[\"start_goal\"]).to(device)\n",
    "occ = torch.from_numpy(batch[\"observation\"])\n",
    "occ = occ.unsqueeze(0)\n",
    "occ = occ.unsqueeze(1)\n",
    "adap_pool = nn.AdaptiveAvgPool3d((25,100, 600))\n",
    "occ = adap_pool(occ)\n",
    "occ = occ.to(device)\n",
    "\n",
    "data = torch.from_numpy(batch[\"data\"]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    y_viz = torch.randn(1,4).to(device)\n",
    "    for i in range(0, 10):\n",
    "        num_viz = 8\n",
    "        y_viz_p = model.inference(startgoal.expand(num_viz, X_dim * 2).to(device), \n",
    "                                occ.expand(num_viz, 1, -1, -1, -1).to(device), num_viz)\n",
    "        torch.cuda.empty_cache()\n",
    "        y_viz = torch.cat((y_viz_p, y_viz), dim = 0)\n",
    "\n",
    "y_viz=y_viz.cpu().detach().numpy()\n",
    "occ=occ.cpu().detach().numpy()\n",
    "startgoal=startgoal.cpu().detach().numpy()\n",
    "data=data.cpu().detach().numpy()\n",
    "torch.cuda.empty_cache()\n",
    "# from utils.NarrowPassage import plotCondition, plotSample, plotSpeed, plotSampleAttention\n",
    "\n",
    "y_viz=y_viz[:-1]\n",
    "plotData(occ, startgoal, y_viz)\n",
    "plotOrientSpeed(startgoal, y_viz)\n",
    "# # plotCondition(con)\n",
    "# # plotSample(y_viz, con)\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# occ = occ[0].reshape(100,100)\n",
    "# a = alpha[0].reshape(11,11)\n",
    "# a = a.cpu().detach().numpy()\n",
    "\n",
    "# plotSampleAttention(y_viz, con, occ, a)\n",
    "# # plotCondition(con)\n",
    "# # def plotOcc(img):\n",
    "# #     img = torch.from_numpy(img).permute(1,0)\n",
    "# #     # occ = ndimage.rotate(occ[0], degree*60)\n",
    "# #     plt.imshow(img)\n",
    "# #     plt.gca().invert_yaxis()\n",
    "# # plotOcc(occ)\n",
    "# # plotOcc(a)\n",
    "\n",
    "# plotSpeed(y_viz, con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0000000e+00,  0.0000000e+00, -1.8300001e-02,  2.9430000e+01,\n",
       "        1.3418002e+02, -2.2000122e+00, -0.0000000e+00,  2.8770000e+01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "startgoal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.24543388e+02, -2.23520970e+00,  2.84436420e-02,\n",
       "         2.85247135e+01],\n",
       "       [ 1.27335419e+02, -2.13794470e+00,  2.15644315e-02,\n",
       "         2.91948204e+01],\n",
       "       [ 1.26191803e+02, -2.26579809e+00, -3.29630915e-03,\n",
       "         2.84353790e+01],\n",
       "       [ 1.41757248e+02, -2.39534807e+00, -6.34788815e-03,\n",
       "         2.90515232e+01],\n",
       "       [ 1.30280655e+02, -2.26277113e+00,  7.57206138e-03,\n",
       "         2.84413033e+01],\n",
       "       [ 1.28432755e+02, -2.29754496e+00,  1.55592030e-02,\n",
       "         2.90652924e+01],\n",
       "       [ 1.32576416e+02, -2.33535361e+00,  2.76262220e-03,\n",
       "         2.85078144e+01],\n",
       "       [ 1.43552307e+02, -2.39504910e+00, -2.56697461e-02,\n",
       "         2.87375183e+01],\n",
       "       [ 1.24400558e+02, -2.21386480e+00,  2.81393602e-02,\n",
       "         2.85131474e+01],\n",
       "       [ 1.35934860e+02, -2.30486298e+00, -5.95413987e-03,\n",
       "         2.89163666e+01],\n",
       "       [ 1.22298363e+02, -2.07659626e+00,  3.96818575e-03,\n",
       "         2.90542393e+01],\n",
       "       [ 1.41198944e+02, -2.34258270e+00,  2.72554811e-03,\n",
       "         2.84174862e+01],\n",
       "       [ 1.27828072e+02, -2.26439476e+00,  1.49050420e-02,\n",
       "         2.82786064e+01],\n",
       "       [ 1.37660431e+02, -2.28232527e+00,  6.06257375e-03,\n",
       "         2.89516106e+01],\n",
       "       [ 1.41071335e+02, -2.43957138e+00, -2.53313128e-03,\n",
       "         2.82198963e+01],\n",
       "       [ 1.27413040e+02, -2.25695992e+00,  1.79267600e-02,\n",
       "         2.82138729e+01],\n",
       "       [ 1.55900467e+02, -2.82693577e+00,  4.06492408e-03,\n",
       "         2.77780590e+01],\n",
       "       [ 1.24380127e+02, -2.05762863e+00, -3.68886627e-04,\n",
       "         2.91440811e+01],\n",
       "       [ 1.40050415e+02, -2.46346998e+00,  1.27299493e-02,\n",
       "         2.84990005e+01],\n",
       "       [ 1.34095016e+02, -2.30061507e+00, -7.73774926e-03,\n",
       "         2.88335953e+01],\n",
       "       [ 1.36073013e+02, -2.32751226e+00,  3.36891972e-04,\n",
       "         2.84653187e+01],\n",
       "       [ 1.31704269e+02, -2.18070889e+00, -1.16447741e-02,\n",
       "         2.88236771e+01],\n",
       "       [ 1.35936600e+02, -2.44248581e+00,  2.03576759e-02,\n",
       "         2.89521713e+01],\n",
       "       [ 1.23140053e+02, -2.14247561e+00,  3.13468650e-02,\n",
       "         2.83261242e+01],\n",
       "       [ 1.35347672e+02, -2.29045367e+00,  4.05836757e-03,\n",
       "         2.86490784e+01],\n",
       "       [ 1.23649200e+02, -2.12008238e+00,  8.03620275e-03,\n",
       "         2.89729080e+01],\n",
       "       [ 1.38042709e+02, -2.39669800e+00,  1.92150548e-02,\n",
       "         2.79047318e+01],\n",
       "       [ 1.24909691e+02, -2.09908342e+00,  2.20305994e-02,\n",
       "         2.92690277e+01],\n",
       "       [ 1.46161102e+02, -2.54154181e+00, -1.24487216e-02,\n",
       "         2.81672039e+01],\n",
       "       [ 1.14612839e+02, -2.13036227e+00, -2.54672691e-02,\n",
       "         2.89806290e+01],\n",
       "       [ 1.36454498e+02, -2.38408232e+00,  1.49651831e-02,\n",
       "         2.86260738e+01],\n",
       "       [ 1.42664719e+02, -2.56880522e+00, -6.38180412e-04,\n",
       "         2.81057281e+01],\n",
       "       [ 1.21907013e+02, -2.03841925e+00,  3.75158265e-02,\n",
       "         2.84632740e+01],\n",
       "       [ 1.31958954e+02, -2.30834246e+00,  1.32662710e-03,\n",
       "         2.77528572e+01],\n",
       "       [ 1.37656403e+02, -2.30024457e+00,  1.02229184e-02,\n",
       "         2.89597702e+01],\n",
       "       [ 1.35152023e+02, -2.22208142e+00, -3.49699799e-03,\n",
       "         2.82140064e+01],\n",
       "       [ 1.28251816e+02, -2.04179502e+00,  2.20293459e-03,\n",
       "         2.87586899e+01],\n",
       "       [ 1.23785484e+02, -2.01092768e+00,  6.84768613e-03,\n",
       "         2.88177624e+01],\n",
       "       [ 1.46047836e+02, -2.46301746e+00, -6.64060656e-03,\n",
       "         2.86242619e+01],\n",
       "       [ 1.42849854e+02, -2.51013899e+00,  4.70454153e-03,\n",
       "         2.79590092e+01],\n",
       "       [ 1.39221359e+02, -2.44256186e+00,  6.31911214e-03,\n",
       "         2.81979523e+01],\n",
       "       [ 1.42119232e+02, -2.34397960e+00, -9.30338446e-03,\n",
       "         2.86340160e+01],\n",
       "       [ 1.25330627e+02, -2.24632788e+00,  1.76304057e-02,\n",
       "         2.91092396e+01],\n",
       "       [ 1.13465675e+02, -1.86175358e+00,  1.16983121e-02,\n",
       "         2.88532143e+01],\n",
       "       [ 1.24703300e+02, -2.16130280e+00,  1.13751357e-02,\n",
       "         2.88543205e+01],\n",
       "       [ 1.25659897e+02, -2.30018663e+00,  3.13950237e-03,\n",
       "         2.84161625e+01],\n",
       "       [ 1.48738861e+02, -2.62185335e+00,  1.93489250e-03,\n",
       "         2.84261265e+01],\n",
       "       [ 1.44057495e+02, -2.58919382e+00,  8.36778339e-03,\n",
       "         2.76072102e+01],\n",
       "       [ 1.33300537e+02, -2.35304403e+00,  7.02715572e-03,\n",
       "         2.90715923e+01],\n",
       "       [ 1.11495850e+02, -2.03438663e+00, -6.46304432e-03,\n",
       "         2.89324608e+01],\n",
       "       [ 1.36402206e+02, -2.27383375e+00, -1.37862498e-02,\n",
       "         2.90845432e+01],\n",
       "       [ 1.31491684e+02, -2.34634542e+00,  4.87417635e-03,\n",
       "         2.83030128e+01],\n",
       "       [ 1.30088943e+02, -2.24807787e+00,  3.11881956e-03,\n",
       "         2.88369598e+01],\n",
       "       [ 1.57983414e+02, -2.74651909e+00, -2.27415487e-02,\n",
       "         2.83981304e+01],\n",
       "       [ 1.25937668e+02, -2.15588140e+00,  3.13195661e-02,\n",
       "         2.89522114e+01],\n",
       "       [ 1.38582794e+02, -2.32641101e+00, -9.86927189e-04,\n",
       "         2.81276855e+01],\n",
       "       [ 1.25810310e+02, -2.08725643e+00,  3.13331559e-02,\n",
       "         2.84320946e+01],\n",
       "       [ 1.09520027e+02, -1.89787877e+00, -1.46847898e-02,\n",
       "         2.91731777e+01],\n",
       "       [ 1.41687515e+02, -2.38091516e+00, -1.74521133e-02,\n",
       "         2.84994602e+01],\n",
       "       [ 1.41634903e+02, -2.43300629e+00, -1.63446590e-02,\n",
       "         2.84117756e+01],\n",
       "       [ 1.31361404e+02, -2.24531269e+00,  2.75362190e-03,\n",
       "         2.85840607e+01],\n",
       "       [ 1.28831970e+02, -2.18676615e+00,  2.12455466e-02,\n",
       "         2.86629848e+01],\n",
       "       [ 1.33646790e+02, -2.27027941e+00, -8.56190268e-03,\n",
       "         2.79473305e+01],\n",
       "       [ 1.33396240e+02, -2.26389122e+00,  9.87977441e-03,\n",
       "         2.93124218e+01],\n",
       "       [ 1.27074303e+02, -2.22887135e+00, -7.27456156e-03,\n",
       "         2.83180561e+01],\n",
       "       [ 1.39598816e+02, -2.24891710e+00, -3.43137328e-03,\n",
       "         2.88064308e+01],\n",
       "       [ 1.00189896e+02, -1.74915111e+00, -3.19527909e-02,\n",
       "         2.90262775e+01],\n",
       "       [ 1.26048264e+02, -2.29550123e+00,  1.10634631e-02,\n",
       "         2.85779285e+01],\n",
       "       [ 1.23105698e+02, -2.13718653e+00,  2.10008100e-02,\n",
       "         2.88886662e+01],\n",
       "       [ 1.41940887e+02, -2.33877516e+00, -1.30619938e-02,\n",
       "         2.85580215e+01],\n",
       "       [ 1.38416489e+02, -2.40354848e+00, -1.65902898e-02,\n",
       "         2.88151817e+01],\n",
       "       [ 1.29728119e+02, -2.14347363e+00, -3.43375839e-04,\n",
       "         2.93838940e+01],\n",
       "       [ 1.27743988e+02, -2.30703235e+00,  9.26095899e-03,\n",
       "         2.89234142e+01],\n",
       "       [ 1.36097473e+02, -2.24936414e+00, -2.38566324e-02,\n",
       "         2.88427334e+01],\n",
       "       [ 1.21674500e+02, -2.05664825e+00,  3.16292699e-03,\n",
       "         2.90167065e+01],\n",
       "       [ 1.56979050e+02, -2.73192716e+00, -1.11029679e-02,\n",
       "         2.81951904e+01],\n",
       "       [ 1.41294128e+02, -2.36330628e+00,  1.56312063e-02,\n",
       "         2.86810246e+01],\n",
       "       [ 1.35597961e+02, -2.27815390e+00, -1.23517448e-02,\n",
       "         2.87342052e+01],\n",
       "       [ 1.31537216e+02, -2.27309561e+00, -1.39309103e-02,\n",
       "         2.88338223e+01],\n",
       "       [ 1.60988449e+02, -2.72305107e+00,  9.02814325e-03,\n",
       "         2.84708557e+01]], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib\n",
    "\n",
    "startx = int(600*(3/8))\n",
    "starty = 50\n",
    "\n",
    "def position2imagep(data, startgoal):\n",
    "    datax = (data[0])*(3/2) + startx\n",
    "    datay = (data[1])*(3/2) + starty\n",
    "    return datax, datay\n",
    "\n",
    "def plotCondition(occ, startgoal):\n",
    "    fig = plt.figure()\n",
    "    occ=occ.squeeze(0)\n",
    "    occ=occ.squeeze(0)\n",
    "    myobj = plt.imshow(occ[0, :, :])\n",
    "    goalx, goaly = position2imagep(startgoal[4:], startgoal)\n",
    "    plt.scatter(startx, starty)\n",
    "    plt.scatter(goalx, goaly)\n",
    "    for frame in occ:\n",
    "        myobj.set_data(frame)\n",
    "        plt.draw()\n",
    "        plt.pause(0.1)\n",
    "\n",
    "def plotData(occ, startgoal,data):\n",
    "    fig = plt.figure()\n",
    "    occ=occ.squeeze(0)\n",
    "    occ=occ.squeeze(0)\n",
    "    myobj = plt.imshow(occ[0, :, :])\n",
    "    goalx, goaly = position2imagep(startgoal[4:], startgoal)\n",
    "    plt.scatter(startx, starty)\n",
    "    plt.scatter(goalx, goaly)\n",
    "    if len(data.shape) > 1:\n",
    "        for row in data:\n",
    "            datax, datay = position2imagep(row, startgoal)\n",
    "            plt.scatter(datax, datay, marker='*', c='#d62728')\n",
    "    else:\n",
    "        datax, datay = position2imagep(data, startgoal)\n",
    "        plt.scatter(datax, datay, marker='*', c='#d62728')\n",
    "    for frame in occ:\n",
    "        myobj.set_data(frame)\n",
    "        plt.draw()\n",
    "        plt.pause(0.1)\n",
    "\n",
    "def plotOrientSpeed(startgoal, data):\n",
    "    fig = plt.figure()\n",
    "    goalx, goaly = position2imagep(startgoal[4:], startgoal)\n",
    "    plt.scatter(data[:, 2], data[:,3])\n",
    "    plt.scatter(startgoal[2], startgoal[3])\n",
    "    plt.scatter(startgoal[6], startgoal[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
