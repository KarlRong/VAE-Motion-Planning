{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda:0\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import sqlite3\n",
    "import torch\n",
    "import io\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device\", device)\n",
    "\n",
    "def adapt_array(arr):\n",
    "    out = io.BytesIO()\n",
    "    np.save(out, arr)\n",
    "    out.seek(0)\n",
    "    return sqlite3.Binary(out.read())\n",
    "\n",
    "def convert_array(text):\n",
    "    out = io.BytesIO(text)\n",
    "    out.seek(0)\n",
    "    return np.load(out)\n",
    "\n",
    "def position_reformed(data, startgoal):\n",
    "    datax = (data[0]-startgoal[0])*(3/2) + startx\n",
    "    datay = (data[1]-startgoal[1])*(3/2) + starty\n",
    "    return datax, datay\n",
    "\n",
    "# Converts np.array to TEXT when inserting\n",
    "sqlite3.register_adapter(np.ndarray, adapt_array)\n",
    "# Converts TEXT to np.array when selecting\n",
    "sqlite3.register_converter(\"array\", convert_array)\n",
    "\n",
    "class TrafficDataset(Dataset):\n",
    "    def __init__(self, dbpath, train=True, ratio_test=0.8, num_sce=100, num_data=25):\n",
    "        self.con = sqlite3.connect(dbpath, detect_types=sqlite3.PARSE_DECLTYPES)\n",
    "        self.path = dbpath\n",
    "        self.cur = self.con.cursor()\n",
    "        self.cur.execute(\"select id from highway\")\n",
    "        self.idlist = self.cur.fetchall()\n",
    "        self.cur.execute(\"select data from highway where id = \" +  str(1))\n",
    "        self.dataperow = len(self.cur.fetchone()[0])\n",
    "        numTrain = int(ratio_test * len(self.idlist))\n",
    "        if (train):\n",
    "            self.filelist = self.idlist[:numTrain]\n",
    "        else:\n",
    "            self.filelist = self.idlist[numTrain:]\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.idlist) * self.dataperow\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        rowid = idx // self.dataperow\n",
    "        dataid = idx % self.dataperow\n",
    "        self.cur.execute(\"select id, startgoal, occ, data from highway where id = \" +  str(rowid+1))\n",
    "        results = self.cur.fetchone()\n",
    "        start_goal = results[1].astype(np.single)\n",
    "        observed = results[2].astype(np.single)\n",
    "        data = results[3][dataid].astype(np.single)\n",
    "        \n",
    "        start_goal[4] = start_goal[4] - start_goal[0]\n",
    "        start_goal[5] = start_goal[5] - start_goal[1]\n",
    "        \n",
    "        data[0] = data[0] - start_goal[0]\n",
    "        data[1] = data[1] - start_goal[1]\n",
    "        \n",
    "        start_goal[0] = 0\n",
    "        start_goal[1] = 0\n",
    "        \n",
    "         \n",
    "        sample = {'start_goal': start_goal,\n",
    "                             'observation': observed,\n",
    "                            'data': data}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "dbpath = '/home/rong/disk/database/highway.db'\n",
    "bs = 8\n",
    "train_loader = DataLoader(TrafficDataset(dbpath = dbpath,\n",
    "                            train = True),\n",
    "                         batch_size = bs, shuffle=True, drop_last = True)\n",
    "test_loader = DataLoader(TrafficDataset(dbpath = dbpath,\n",
    "                            train = False),\n",
    "                          batch_size = bs, shuffle=True, drop_last = True)\n",
    "\n",
    "# batch = next(iter(train_loader))\n",
    "# batch['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "\n",
    "class convVAE(nn.Module):\n",
    "    def __init__(self, sample_size, cnnout_size, cond_out_size, encoder_layer_sizes, latent_size, decoder_layer_sizes):\n",
    "        super(convVAE, self).__init__()\n",
    "\n",
    "        assert type(encoder_layer_sizes) == list\n",
    "        assert type(latent_size) == int\n",
    "        assert type(decoder_layer_sizes) == list\n",
    "        \n",
    "        self.latent_size = latent_size\n",
    "        self.condnn = CondNN(sample_size, cnnout_size, cond_out_size)\n",
    "        self.encoder = Encoder(sample_size + cond_out_size, encoder_layer_sizes, latent_size)\n",
    "        self.decoder = Decoder(latent_size +cond_out_size, decoder_layer_sizes, sample_size)\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def decode(self, x):\n",
    "        return self.decoder(x)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x, startend, occ):\n",
    "        c = self.condnn(startend, occ)\n",
    "        mu, logvar = self.encode(torch.cat((x, c), dim=-1))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(torch.cat((z, c), dim=-1)), mu, logvar\n",
    "    \n",
    "    def inference(self, startend, occ, num_viz):\n",
    "        c = self.condnn(startend, occ)\n",
    "        z = torch.randn(num_viz, self.latent_size, device = c.device)\n",
    "        return self.decode(torch.cat((z, c), dim=-1))\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, layer_sizes, latent_size):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        layer_sizes = [input_size] + layer_sizes\n",
    "        modules = []\n",
    "        for i, (in_size, out_size) in enumerate(zip(layer_sizes[:-1], layer_sizes[1:])):\n",
    "            modules.append(nn.Linear(in_size, out_size))\n",
    "            modules.append(nn.ReLU())\n",
    "#             modules.append(nn.Dropout(p=0.5))\n",
    "\n",
    "        self.sequential = nn.Sequential(*modules)\n",
    "        self.linear_means = nn.Linear(layer_sizes[-1], latent_size)\n",
    "        self.linear_log_var = nn.Linear(layer_sizes[-1], latent_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sequential(x)\n",
    "        means = self.linear_means(x)\n",
    "        log_vars = self.linear_log_var(x)\n",
    "        return means, log_vars\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, layer_sizes, sample_size):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        layer_sizes = [input_size] + layer_sizes\n",
    "        modules = []\n",
    "        for i, (in_size, out_size) in enumerate(zip(layer_sizes[:-1], layer_sizes[1:])):\n",
    "            modules.append(nn.Linear(in_size, out_size))\n",
    "            modules.append(nn.ReLU())\n",
    "#             modules.append(nn.Dropout(p=0.5))\n",
    "        modules.append(nn.Linear(layer_sizes[-1], sample_size))\n",
    "\n",
    "        self.sequential = nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sequential(x)\n",
    "\n",
    "\n",
    "class CondNN(nn.Module):\n",
    "    def __init__(self, sampleSize,  cnn_out_size, outSize):\n",
    "        super(CondNN, self).__init__()\n",
    "        self.sampleSize = sampleSize\n",
    "        self.cnn = Conv3d(cnn_out_size)\n",
    "        self.fc2 = nn.Linear(cnn_out_size + sampleSize * 2, outSize)\n",
    "\n",
    "    def forward(self, startend, occ):\n",
    "        occ = self.cnn(occ)\n",
    "        x = torch.cat((occ, startend), dim=-1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "class Conv3d(nn.Module):\n",
    "    def __init__(self, cnn_out_size):\n",
    "        super(Conv3d, self).__init__()\n",
    "\n",
    "        self.adap_pool = nn.AdaptiveAvgPool3d((25, 100, 600))\n",
    "        self.conv_layer1 = self._make_conv_layer(1, 16)\n",
    "        self.conv_layer2 = self._make_conv_layer(16, 32)\n",
    "#         self.conv_layer3 = self._make_conv_layer(64, 124)\n",
    "#         self.conv_layer4 = self._make_conv_layer(124, 256)\n",
    "        self.conv_layer5=nn.Conv3d(32, 64, kernel_size=(1, 3, 3), padding=0)\n",
    "        \n",
    "        self.adap_pool2 = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "        self.fc5 = nn.Linear(64, 64)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.batch0=nn.BatchNorm1d(64)\n",
    "        self.drop=nn.Dropout(p=0.15)        \n",
    "        self.fc6 = nn.Linear(64, 64)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.batch1=nn.BatchNorm1d(64)\n",
    "        \n",
    "        self.drop=nn.Dropout(p=0.15)\n",
    "        self.fc7 = nn.Linear(64, cnn_out_size)\n",
    "\n",
    "    def _make_conv_layer(self, in_c, out_c):\n",
    "        conv_layer = nn.Sequential(\n",
    "        nn.Conv3d(in_c, out_c, kernel_size=(2, 3, 3), padding=0),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Conv3d(out_c, out_c, kernel_size=(2, 3, 3), padding=1),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.MaxPool3d((2, 2, 2)),\n",
    "        )\n",
    "        return conv_layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.adap_pool(x)\n",
    "#         print(x.size())\n",
    "        x = self.conv_layer1(x)\n",
    "#         print(x.size())\n",
    "        x = self.conv_layer2(x)\n",
    "#         print(x.size())\n",
    "#         x = self.conv_layer3(x)\n",
    "#         print(x.size())\n",
    "#         x = self.conv_layer4(x)\n",
    "#         print(x.size())\n",
    "        x=self.conv_layer5(x)\n",
    "#         print(x.size())\n",
    "        x = self.adap_pool2(x)\n",
    "#         print(x.size())\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc5(x)\n",
    "#         print(x.size())\n",
    "        \n",
    "        x = self.relu(x)\n",
    "        x = self.batch0(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc6(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batch1(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc7(x)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda:0\n",
      "convVAE(\n",
      "  (condnn): CondNN(\n",
      "    (cnn): Conv3d(\n",
      "      (adap_pool): AdaptiveAvgPool3d(output_size=(25, 100, 600))\n",
      "      (conv_layer1): Sequential(\n",
      "        (0): Conv3d(1, 16, kernel_size=(2, 3, 3), stride=(1, 1, 1))\n",
      "        (1): LeakyReLU(negative_slope=0.01)\n",
      "        (2): Conv3d(16, 16, kernel_size=(2, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (3): LeakyReLU(negative_slope=0.01)\n",
      "        (4): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (conv_layer2): Sequential(\n",
      "        (0): Conv3d(16, 32, kernel_size=(2, 3, 3), stride=(1, 1, 1))\n",
      "        (1): LeakyReLU(negative_slope=0.01)\n",
      "        (2): Conv3d(32, 32, kernel_size=(2, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (3): LeakyReLU(negative_slope=0.01)\n",
      "        (4): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (conv_layer5): Conv3d(32, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1))\n",
      "      (adap_pool2): AdaptiveAvgPool3d(output_size=(1, 1, 1))\n",
      "      (fc5): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (relu): LeakyReLU(negative_slope=0.01)\n",
      "      (batch0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (drop): Dropout(p=0.15, inplace=False)\n",
      "      (fc6): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (batch1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (fc7): Linear(in_features=64, out_features=300, bias=True)\n",
      "    )\n",
      "    (fc2): Linear(in_features=308, out_features=300, bias=True)\n",
      "  )\n",
      "  (encoder): Encoder(\n",
      "    (sequential): Sequential(\n",
      "      (0): Linear(in_features=304, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=1024, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "    (linear_means): Linear(in_features=512, out_features=50, bias=True)\n",
      "    (linear_log_var): Linear(in_features=512, out_features=50, bias=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (sequential): Sequential(\n",
      "      (0): Linear(in_features=350, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=1024, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (5): ReLU()\n",
      "      (6): Linear(in_features=512, out_features=4, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "X_dim = 4\n",
    "z_dim = 50\n",
    "cnn_out_size = 300\n",
    "cond_out_size = 300\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device\", device)\n",
    "\n",
    "model = convVAE(sample_size = X_dim, \n",
    "                  cnnout_size = cnn_out_size, \n",
    "                  cond_out_size = cond_out_size, \n",
    "                  encoder_layer_sizes = [512,1024,512], \n",
    "                  latent_size = z_dim, \n",
    "                  decoder_layer_sizes = [512,1024,512]).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(recon_x, x, w, mean, log_var):\n",
    "    MSE = torch.mean((w.expand_as(x) * (recon_x-x)**2))\n",
    "    KLD = - 0.002 * torch.mean(torch.sum(1 + log_var - mean.pow(2) - log_var.exp(), 1))\n",
    "    return MSE + KLD, MSE\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, writer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    mse_loss = 0\n",
    "    w = torch.tensor([5, 10, 1, 3], dtype=torch.float).to(device)\n",
    "    adap_pool = nn.AdaptiveAvgPool3d((25,100, 600))\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        startgoal = batch[\"start_goal\"].to(device)\n",
    "        occ = batch[\"observation\"]\n",
    "        occ = adap_pool(occ)\n",
    "        occ = occ.to(device)\n",
    "        occ = occ.unsqueeze(1)\n",
    "        data = batch[\"data\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data, startgoal, occ)\n",
    "        loss, mse= loss_fn(recon_batch, data, w, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        mse_loss += mse.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item()))\n",
    "        \n",
    "            writer.add_scalar('BatchLoss/loss', loss.item(), batch_idx)\n",
    "            writer.add_scalar('BatchLoss/mse_loss', mse.item(), batch_idx)\n",
    "\n",
    "    epoch_loss = train_loss * len(data) / len(train_loader.dataset)\n",
    "    epoch_mse = mse_loss * len(data) / len(train_loader.dataset)\n",
    "    print('====> Epoch: {} Average loss: {:.7f}'.format(\n",
    "          epoch, epoch_loss))\n",
    "    return epoch, epoch_loss, epoch_mse\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    mse_loss = 0\n",
    "    w = torch.tensor([1, 1, 1, 0.5], dtype=torch.float).to(device)\n",
    "    for batch_idx, batch in enumerate(test_loader):\n",
    "        startgoal = batch[\"start_goal\"].to(device)\n",
    "        occ = batch[\"observation\"].to(device)\n",
    "        occ = occ.unsqueeze(1)\n",
    "        data = batch[\"data\"].to(device)\n",
    "        \n",
    "        recon_batch, mu, logvar = model(sample, startend, occ)\n",
    "        loss, mse= loss_fn(recon_batch, data, w, mu, logvar)\n",
    "        test_loss += loss.item()\n",
    "        mse_loss += mse.item()\n",
    "\n",
    "    epoch_loss = test_loss * len(data) / len(test_loader.dataset)\n",
    "    epoch_mse = mse_loss * len(data) / len(test_loader.dataset)\n",
    "    print('====> Epoch: {} Average test loss: {:.7f}'.format(\n",
    "          epoch, epoch_loss))\n",
    "    return epoch, epoch_loss, epoch_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Writer will output to ./runs/ directory by default\n",
    "writer = SummaryWriter('runs/highway_conv_05')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/482500 (0%)]\tLoss: 5775.336914\n",
      "Train Epoch: 0 [160/482500 (0%)]\tLoss: 1889.641235\n",
      "Train Epoch: 0 [320/482500 (0%)]\tLoss: 132411.921875\n",
      "Train Epoch: 0 [480/482500 (0%)]\tLoss: 1605.928955\n",
      "Train Epoch: 0 [640/482500 (0%)]\tLoss: 477.630768\n",
      "Train Epoch: 0 [800/482500 (0%)]\tLoss: 104.786652\n",
      "Train Epoch: 0 [960/482500 (0%)]\tLoss: 71.976715\n",
      "Train Epoch: 0 [1120/482500 (0%)]\tLoss: 71.411873\n",
      "Train Epoch: 0 [1280/482500 (0%)]\tLoss: 33.475891\n",
      "Train Epoch: 0 [1440/482500 (0%)]\tLoss: 8.792051\n",
      "Train Epoch: 0 [1600/482500 (0%)]\tLoss: 3.689662\n",
      "Train Epoch: 0 [1760/482500 (0%)]\tLoss: 9.076292\n",
      "Train Epoch: 0 [1920/482500 (0%)]\tLoss: 6.189453\n",
      "Train Epoch: 0 [2080/482500 (0%)]\tLoss: 7.029680\n",
      "Train Epoch: 0 [2240/482500 (0%)]\tLoss: 5.613835\n",
      "Train Epoch: 0 [2400/482500 (0%)]\tLoss: 5.237060\n",
      "Train Epoch: 0 [2560/482500 (1%)]\tLoss: 6.430727\n",
      "Train Epoch: 0 [2720/482500 (1%)]\tLoss: 3.583141\n",
      "Train Epoch: 0 [2880/482500 (1%)]\tLoss: 6.077054\n",
      "Train Epoch: 0 [3040/482500 (1%)]\tLoss: 4.002203\n",
      "Train Epoch: 0 [3200/482500 (1%)]\tLoss: 2.946280\n",
      "Train Epoch: 0 [3360/482500 (1%)]\tLoss: 2.730072\n",
      "Train Epoch: 0 [3520/482500 (1%)]\tLoss: 6.209569\n",
      "Train Epoch: 0 [3680/482500 (1%)]\tLoss: 6.829014\n",
      "Train Epoch: 0 [3840/482500 (1%)]\tLoss: 2.507500\n",
      "Train Epoch: 0 [4000/482500 (1%)]\tLoss: 3.079047\n",
      "Train Epoch: 0 [4160/482500 (1%)]\tLoss: 6.293044\n",
      "Train Epoch: 0 [4320/482500 (1%)]\tLoss: 3.773857\n",
      "Train Epoch: 0 [4480/482500 (1%)]\tLoss: 3.724286\n",
      "Train Epoch: 0 [4640/482500 (1%)]\tLoss: 3.928445\n",
      "Train Epoch: 0 [4800/482500 (1%)]\tLoss: 7.563559\n",
      "Train Epoch: 0 [4960/482500 (1%)]\tLoss: 1.948052\n",
      "Train Epoch: 0 [5120/482500 (1%)]\tLoss: 4.214717\n",
      "Train Epoch: 0 [5280/482500 (1%)]\tLoss: 8.201138\n",
      "Train Epoch: 0 [5440/482500 (1%)]\tLoss: 3.293778\n",
      "Train Epoch: 0 [5600/482500 (1%)]\tLoss: 8.390935\n",
      "Train Epoch: 0 [5760/482500 (1%)]\tLoss: 40.616051\n",
      "Train Epoch: 0 [5920/482500 (1%)]\tLoss: 11.728858\n",
      "Train Epoch: 0 [6080/482500 (1%)]\tLoss: 58.471771\n",
      "Train Epoch: 0 [6240/482500 (1%)]\tLoss: 31.814873\n",
      "Train Epoch: 0 [6400/482500 (1%)]\tLoss: 9.119798\n",
      "Train Epoch: 0 [6560/482500 (1%)]\tLoss: 3.052482\n",
      "Train Epoch: 0 [6720/482500 (1%)]\tLoss: 2.691463\n",
      "Train Epoch: 0 [6880/482500 (1%)]\tLoss: 2.450567\n",
      "Train Epoch: 0 [7040/482500 (1%)]\tLoss: 6.985124\n",
      "Train Epoch: 0 [7200/482500 (1%)]\tLoss: 2.406615\n",
      "Train Epoch: 0 [7360/482500 (2%)]\tLoss: 2.682793\n",
      "Train Epoch: 0 [7520/482500 (2%)]\tLoss: 2.921236\n",
      "Train Epoch: 0 [7680/482500 (2%)]\tLoss: 5.462255\n",
      "Train Epoch: 0 [7840/482500 (2%)]\tLoss: 5.452061\n",
      "Train Epoch: 0 [8000/482500 (2%)]\tLoss: 2.695617\n",
      "Train Epoch: 0 [8160/482500 (2%)]\tLoss: 3.978054\n",
      "Train Epoch: 0 [8320/482500 (2%)]\tLoss: 3.349117\n",
      "Train Epoch: 0 [8480/482500 (2%)]\tLoss: 2.415529\n",
      "Train Epoch: 0 [8640/482500 (2%)]\tLoss: 1.063651\n",
      "Train Epoch: 0 [8800/482500 (2%)]\tLoss: 1.920762\n",
      "Train Epoch: 0 [8960/482500 (2%)]\tLoss: 34.618519\n",
      "Train Epoch: 0 [9120/482500 (2%)]\tLoss: 10.138664\n",
      "Train Epoch: 0 [9280/482500 (2%)]\tLoss: 3.826285\n",
      "Train Epoch: 0 [9440/482500 (2%)]\tLoss: 3.455117\n",
      "Train Epoch: 0 [9600/482500 (2%)]\tLoss: 5.938538\n",
      "Train Epoch: 0 [9760/482500 (2%)]\tLoss: 2.460551\n",
      "Train Epoch: 0 [9920/482500 (2%)]\tLoss: 2.530036\n",
      "Train Epoch: 0 [10080/482500 (2%)]\tLoss: 3.568414\n",
      "Train Epoch: 0 [10240/482500 (2%)]\tLoss: 38.827042\n",
      "Train Epoch: 0 [10400/482500 (2%)]\tLoss: 11.889724\n",
      "Train Epoch: 0 [10560/482500 (2%)]\tLoss: 4.783106\n",
      "Train Epoch: 0 [10720/482500 (2%)]\tLoss: 3.239360\n",
      "Train Epoch: 0 [10880/482500 (2%)]\tLoss: 1.819397\n",
      "Train Epoch: 0 [11040/482500 (2%)]\tLoss: 3.009114\n",
      "Train Epoch: 0 [11200/482500 (2%)]\tLoss: 2.116323\n",
      "Train Epoch: 0 [11360/482500 (2%)]\tLoss: 4.310039\n",
      "Train Epoch: 0 [11520/482500 (2%)]\tLoss: 1.190300\n",
      "Train Epoch: 0 [11680/482500 (2%)]\tLoss: 2.604610\n",
      "Train Epoch: 0 [11840/482500 (2%)]\tLoss: 2.196665\n",
      "Train Epoch: 0 [12000/482500 (2%)]\tLoss: 2.380281\n",
      "Train Epoch: 0 [12160/482500 (3%)]\tLoss: 2.400263\n",
      "Train Epoch: 0 [12320/482500 (3%)]\tLoss: 2.700425\n",
      "Train Epoch: 0 [12480/482500 (3%)]\tLoss: 1.905219\n",
      "Train Epoch: 0 [12640/482500 (3%)]\tLoss: 3.714046\n",
      "Train Epoch: 0 [12800/482500 (3%)]\tLoss: 4.587293\n",
      "Train Epoch: 0 [12960/482500 (3%)]\tLoss: 3.050252\n",
      "Train Epoch: 0 [13120/482500 (3%)]\tLoss: 1.974837\n",
      "Train Epoch: 0 [13280/482500 (3%)]\tLoss: 2.200003\n",
      "Train Epoch: 0 [13440/482500 (3%)]\tLoss: 7.584292\n",
      "Train Epoch: 0 [13600/482500 (3%)]\tLoss: 1.417610\n",
      "Train Epoch: 0 [13760/482500 (3%)]\tLoss: 1.822629\n",
      "Train Epoch: 0 [13920/482500 (3%)]\tLoss: 18.555342\n",
      "Train Epoch: 0 [14080/482500 (3%)]\tLoss: 6.524225\n",
      "Train Epoch: 0 [14240/482500 (3%)]\tLoss: 3.818696\n",
      "Train Epoch: 0 [14400/482500 (3%)]\tLoss: 5.560686\n",
      "Train Epoch: 0 [14560/482500 (3%)]\tLoss: 2.702402\n",
      "Train Epoch: 0 [14720/482500 (3%)]\tLoss: 4.091696\n",
      "Train Epoch: 0 [14880/482500 (3%)]\tLoss: 1.445402\n",
      "Train Epoch: 0 [15040/482500 (3%)]\tLoss: 2.216321\n",
      "Train Epoch: 0 [15200/482500 (3%)]\tLoss: 1.728169\n",
      "Train Epoch: 0 [15360/482500 (3%)]\tLoss: 75.490753\n",
      "Train Epoch: 0 [15520/482500 (3%)]\tLoss: 18.574219\n",
      "Train Epoch: 0 [15680/482500 (3%)]\tLoss: 7.550075\n",
      "Train Epoch: 0 [15840/482500 (3%)]\tLoss: 3.183805\n",
      "Train Epoch: 0 [16000/482500 (3%)]\tLoss: 2.695195\n",
      "Train Epoch: 0 [16160/482500 (3%)]\tLoss: 3.296174\n",
      "Train Epoch: 0 [16320/482500 (3%)]\tLoss: 5.685050\n",
      "Train Epoch: 0 [16480/482500 (3%)]\tLoss: 3.853953\n",
      "Train Epoch: 0 [16640/482500 (3%)]\tLoss: 7.467309\n",
      "Train Epoch: 0 [16800/482500 (3%)]\tLoss: 2.894490\n",
      "Train Epoch: 0 [16960/482500 (4%)]\tLoss: 1.762332\n",
      "Train Epoch: 0 [17120/482500 (4%)]\tLoss: 1.540627\n",
      "Train Epoch: 0 [17280/482500 (4%)]\tLoss: 3.008618\n",
      "Train Epoch: 0 [17440/482500 (4%)]\tLoss: 2.761744\n",
      "Train Epoch: 0 [17600/482500 (4%)]\tLoss: 3.884767\n",
      "Train Epoch: 0 [17760/482500 (4%)]\tLoss: 2.503039\n",
      "Train Epoch: 0 [17920/482500 (4%)]\tLoss: 2.376081\n",
      "Train Epoch: 0 [18080/482500 (4%)]\tLoss: 5.501546\n",
      "Train Epoch: 0 [18240/482500 (4%)]\tLoss: 3.224516\n",
      "Train Epoch: 0 [18400/482500 (4%)]\tLoss: 1.679610\n",
      "Train Epoch: 0 [18560/482500 (4%)]\tLoss: 4.231180\n",
      "Train Epoch: 0 [18720/482500 (4%)]\tLoss: 0.934609\n",
      "Train Epoch: 0 [18880/482500 (4%)]\tLoss: 2.569669\n",
      "Train Epoch: 0 [19040/482500 (4%)]\tLoss: 2.159008\n",
      "Train Epoch: 0 [19200/482500 (4%)]\tLoss: 22.190134\n",
      "Train Epoch: 0 [19360/482500 (4%)]\tLoss: 17.495255\n",
      "Train Epoch: 0 [19520/482500 (4%)]\tLoss: 3.805131\n",
      "Train Epoch: 0 [19680/482500 (4%)]\tLoss: 3.544657\n",
      "Train Epoch: 0 [19840/482500 (4%)]\tLoss: 2.585798\n",
      "Train Epoch: 0 [20000/482500 (4%)]\tLoss: 2.467945\n",
      "Train Epoch: 0 [20160/482500 (4%)]\tLoss: 1.510492\n",
      "Train Epoch: 0 [20320/482500 (4%)]\tLoss: 1.715170\n",
      "Train Epoch: 0 [20480/482500 (4%)]\tLoss: 1.189733\n",
      "Train Epoch: 0 [20640/482500 (4%)]\tLoss: 1.287517\n",
      "Train Epoch: 0 [20800/482500 (4%)]\tLoss: 1.900800\n",
      "Train Epoch: 0 [20960/482500 (4%)]\tLoss: 1.672297\n",
      "Train Epoch: 0 [21120/482500 (4%)]\tLoss: 3.898952\n",
      "Train Epoch: 0 [21280/482500 (4%)]\tLoss: 9.627156\n",
      "Train Epoch: 0 [21440/482500 (4%)]\tLoss: 12.415352\n",
      "Train Epoch: 0 [21600/482500 (4%)]\tLoss: 7.276277\n",
      "Train Epoch: 0 [21760/482500 (5%)]\tLoss: 4.465241\n",
      "Train Epoch: 0 [21920/482500 (5%)]\tLoss: 2.821211\n",
      "Train Epoch: 0 [22080/482500 (5%)]\tLoss: 4.532896\n",
      "Train Epoch: 0 [22240/482500 (5%)]\tLoss: 6.778409\n",
      "Train Epoch: 0 [22400/482500 (5%)]\tLoss: 2.186667\n",
      "Train Epoch: 0 [22560/482500 (5%)]\tLoss: 3.145953\n",
      "Train Epoch: 0 [22720/482500 (5%)]\tLoss: 3.656053\n",
      "Train Epoch: 0 [22880/482500 (5%)]\tLoss: 2.031078\n",
      "Train Epoch: 0 [23040/482500 (5%)]\tLoss: 3.543834\n",
      "Train Epoch: 0 [23200/482500 (5%)]\tLoss: 2.541588\n",
      "Train Epoch: 0 [23360/482500 (5%)]\tLoss: 41.578972\n",
      "Train Epoch: 0 [23520/482500 (5%)]\tLoss: 6.509239\n",
      "Train Epoch: 0 [23680/482500 (5%)]\tLoss: 6.119191\n",
      "Train Epoch: 0 [23840/482500 (5%)]\tLoss: 2.984290\n",
      "Train Epoch: 0 [24000/482500 (5%)]\tLoss: 1.017948\n",
      "Train Epoch: 0 [24160/482500 (5%)]\tLoss: 1.687116\n",
      "Train Epoch: 0 [24320/482500 (5%)]\tLoss: 2.218697\n",
      "Train Epoch: 0 [24480/482500 (5%)]\tLoss: 2.124396\n",
      "Train Epoch: 0 [24640/482500 (5%)]\tLoss: 1.934033\n",
      "Train Epoch: 0 [24800/482500 (5%)]\tLoss: 1.814143\n",
      "Train Epoch: 0 [24960/482500 (5%)]\tLoss: 1.303267\n",
      "Train Epoch: 0 [25120/482500 (5%)]\tLoss: 2.554131\n",
      "Train Epoch: 0 [25280/482500 (5%)]\tLoss: 2.763272\n",
      "Train Epoch: 0 [25440/482500 (5%)]\tLoss: 2.822682\n",
      "Train Epoch: 0 [25600/482500 (5%)]\tLoss: 1.091120\n",
      "Train Epoch: 0 [25760/482500 (5%)]\tLoss: 0.871386\n",
      "Train Epoch: 0 [25920/482500 (5%)]\tLoss: 8.823183\n",
      "Train Epoch: 0 [26080/482500 (5%)]\tLoss: 1.722889\n",
      "Train Epoch: 0 [26240/482500 (5%)]\tLoss: 1.594902\n",
      "Train Epoch: 0 [26400/482500 (5%)]\tLoss: 3.055228\n",
      "Train Epoch: 0 [26560/482500 (6%)]\tLoss: 0.961012\n",
      "Train Epoch: 0 [26720/482500 (6%)]\tLoss: 2.808432\n",
      "Train Epoch: 0 [26880/482500 (6%)]\tLoss: 4.452859\n",
      "Train Epoch: 0 [27040/482500 (6%)]\tLoss: 2.433155\n",
      "Train Epoch: 0 [27200/482500 (6%)]\tLoss: 2.283186\n",
      "Train Epoch: 0 [27360/482500 (6%)]\tLoss: 12.778039\n",
      "Train Epoch: 0 [27520/482500 (6%)]\tLoss: 5.158872\n",
      "Train Epoch: 0 [27680/482500 (6%)]\tLoss: 12.976431\n",
      "Train Epoch: 0 [27840/482500 (6%)]\tLoss: 4.311819\n",
      "Train Epoch: 0 [28000/482500 (6%)]\tLoss: 2.713864\n",
      "Train Epoch: 0 [28160/482500 (6%)]\tLoss: 2.247736\n",
      "Train Epoch: 0 [28320/482500 (6%)]\tLoss: 2.299374\n",
      "Train Epoch: 0 [28480/482500 (6%)]\tLoss: 1.748705\n",
      "Train Epoch: 0 [28640/482500 (6%)]\tLoss: 4.917881\n",
      "Train Epoch: 0 [28800/482500 (6%)]\tLoss: 3.182895\n",
      "Train Epoch: 0 [28960/482500 (6%)]\tLoss: 1.379647\n",
      "Train Epoch: 0 [29120/482500 (6%)]\tLoss: 3.427447\n",
      "Train Epoch: 0 [29280/482500 (6%)]\tLoss: 1.456542\n",
      "Train Epoch: 0 [29440/482500 (6%)]\tLoss: 1.464329\n",
      "Train Epoch: 0 [29600/482500 (6%)]\tLoss: 1.906083\n",
      "Train Epoch: 0 [29760/482500 (6%)]\tLoss: 2.536911\n",
      "Train Epoch: 0 [29920/482500 (6%)]\tLoss: 1.501919\n",
      "Train Epoch: 0 [30080/482500 (6%)]\tLoss: 47.457016\n",
      "Train Epoch: 0 [30240/482500 (6%)]\tLoss: 8.888689\n",
      "Train Epoch: 0 [30400/482500 (6%)]\tLoss: 2.528662\n",
      "Train Epoch: 0 [30560/482500 (6%)]\tLoss: 1.451043\n",
      "Train Epoch: 0 [30720/482500 (6%)]\tLoss: 4.301203\n",
      "Train Epoch: 0 [30880/482500 (6%)]\tLoss: 3.754286\n",
      "Train Epoch: 0 [31040/482500 (6%)]\tLoss: 3.340668\n",
      "Train Epoch: 0 [31200/482500 (6%)]\tLoss: 1.874157\n",
      "Train Epoch: 0 [31360/482500 (6%)]\tLoss: 5.140060\n",
      "Train Epoch: 0 [31520/482500 (7%)]\tLoss: 1.690456\n",
      "Train Epoch: 0 [31680/482500 (7%)]\tLoss: 1.823262\n",
      "Train Epoch: 0 [31840/482500 (7%)]\tLoss: 4.754262\n",
      "Train Epoch: 0 [32000/482500 (7%)]\tLoss: 3.891954\n",
      "Train Epoch: 0 [32160/482500 (7%)]\tLoss: 1.347833\n",
      "Train Epoch: 0 [32320/482500 (7%)]\tLoss: 1.294920\n",
      "Train Epoch: 0 [32480/482500 (7%)]\tLoss: 3.332075\n",
      "Train Epoch: 0 [32640/482500 (7%)]\tLoss: 1.664297\n",
      "Train Epoch: 0 [32800/482500 (7%)]\tLoss: 1.147571\n",
      "Train Epoch: 0 [32960/482500 (7%)]\tLoss: 1.629695\n",
      "Train Epoch: 0 [33120/482500 (7%)]\tLoss: 2.461825\n",
      "Train Epoch: 0 [33280/482500 (7%)]\tLoss: 1.446721\n",
      "Train Epoch: 0 [33440/482500 (7%)]\tLoss: 29.453768\n",
      "Train Epoch: 0 [33600/482500 (7%)]\tLoss: 4.641564\n",
      "Train Epoch: 0 [33760/482500 (7%)]\tLoss: 2.144805\n",
      "Train Epoch: 0 [33920/482500 (7%)]\tLoss: 4.051529\n",
      "Train Epoch: 0 [34080/482500 (7%)]\tLoss: 2.359919\n",
      "Train Epoch: 0 [34240/482500 (7%)]\tLoss: 1.482303\n",
      "Train Epoch: 0 [34400/482500 (7%)]\tLoss: 6.063626\n",
      "Train Epoch: 0 [34560/482500 (7%)]\tLoss: 1.690126\n",
      "Train Epoch: 0 [34720/482500 (7%)]\tLoss: 1.863439\n",
      "Train Epoch: 0 [34880/482500 (7%)]\tLoss: 4.200334\n",
      "Train Epoch: 0 [35040/482500 (7%)]\tLoss: 5.612059\n",
      "Train Epoch: 0 [35200/482500 (7%)]\tLoss: 9.569618\n",
      "Train Epoch: 0 [35360/482500 (7%)]\tLoss: 1.812542\n",
      "Train Epoch: 0 [35520/482500 (7%)]\tLoss: 3.723435\n",
      "Train Epoch: 0 [35680/482500 (7%)]\tLoss: 6.852939\n",
      "Train Epoch: 0 [35840/482500 (7%)]\tLoss: 3.483421\n",
      "Train Epoch: 0 [36000/482500 (7%)]\tLoss: 3.388517\n",
      "Train Epoch: 0 [36160/482500 (7%)]\tLoss: 6.148330\n",
      "Train Epoch: 0 [36320/482500 (8%)]\tLoss: 3.321059\n",
      "Train Epoch: 0 [36480/482500 (8%)]\tLoss: 2.823606\n",
      "Train Epoch: 0 [36640/482500 (8%)]\tLoss: 1.868638\n",
      "Train Epoch: 0 [36800/482500 (8%)]\tLoss: 1.949314\n",
      "Train Epoch: 0 [36960/482500 (8%)]\tLoss: 3.190686\n",
      "Train Epoch: 0 [37120/482500 (8%)]\tLoss: 1.276602\n",
      "Train Epoch: 0 [37280/482500 (8%)]\tLoss: 3.483997\n",
      "Train Epoch: 0 [37440/482500 (8%)]\tLoss: 1.464302\n",
      "Train Epoch: 0 [37600/482500 (8%)]\tLoss: 3.083725\n",
      "Train Epoch: 0 [37760/482500 (8%)]\tLoss: 2.327412\n",
      "Train Epoch: 0 [37920/482500 (8%)]\tLoss: 4.405145\n",
      "Train Epoch: 0 [38080/482500 (8%)]\tLoss: 3.108717\n",
      "Train Epoch: 0 [38240/482500 (8%)]\tLoss: 7.113560\n",
      "Train Epoch: 0 [38400/482500 (8%)]\tLoss: 5.220265\n",
      "Train Epoch: 0 [38560/482500 (8%)]\tLoss: 2.066996\n",
      "Train Epoch: 0 [38720/482500 (8%)]\tLoss: 6.235669\n",
      "Train Epoch: 0 [38880/482500 (8%)]\tLoss: 5.088530\n",
      "Train Epoch: 0 [39040/482500 (8%)]\tLoss: 2.987240\n",
      "Train Epoch: 0 [39200/482500 (8%)]\tLoss: 6.341039\n",
      "Train Epoch: 0 [39360/482500 (8%)]\tLoss: 3.623924\n",
      "Train Epoch: 0 [39520/482500 (8%)]\tLoss: 2.460298\n",
      "Train Epoch: 0 [39680/482500 (8%)]\tLoss: 1.778620\n",
      "Train Epoch: 0 [39840/482500 (8%)]\tLoss: 1.609105\n",
      "Train Epoch: 0 [40000/482500 (8%)]\tLoss: 2.746420\n",
      "Train Epoch: 0 [40160/482500 (8%)]\tLoss: 4.631787\n",
      "Train Epoch: 0 [40320/482500 (8%)]\tLoss: 2.386810\n",
      "Train Epoch: 0 [40480/482500 (8%)]\tLoss: 1.981569\n",
      "Train Epoch: 0 [40640/482500 (8%)]\tLoss: 1.839509\n",
      "Train Epoch: 0 [40800/482500 (8%)]\tLoss: 2.009352\n",
      "Train Epoch: 0 [40960/482500 (8%)]\tLoss: 93.932091\n",
      "Train Epoch: 0 [41120/482500 (9%)]\tLoss: 26.872192\n",
      "Train Epoch: 0 [41280/482500 (9%)]\tLoss: 2.532220\n",
      "Train Epoch: 0 [41440/482500 (9%)]\tLoss: 5.899649\n",
      "Train Epoch: 0 [41600/482500 (9%)]\tLoss: 3.548485\n",
      "Train Epoch: 0 [41760/482500 (9%)]\tLoss: 1.829621\n",
      "Train Epoch: 0 [41920/482500 (9%)]\tLoss: 3.117624\n",
      "Train Epoch: 0 [42080/482500 (9%)]\tLoss: 2.602623\n",
      "Train Epoch: 0 [42240/482500 (9%)]\tLoss: 3.374845\n",
      "Train Epoch: 0 [42400/482500 (9%)]\tLoss: 1.481475\n",
      "Train Epoch: 0 [42560/482500 (9%)]\tLoss: 1.183156\n",
      "Train Epoch: 0 [42720/482500 (9%)]\tLoss: 1.629294\n",
      "Train Epoch: 0 [42880/482500 (9%)]\tLoss: 0.997911\n",
      "Train Epoch: 0 [43040/482500 (9%)]\tLoss: 2.765364\n",
      "Train Epoch: 0 [43200/482500 (9%)]\tLoss: 4.436845\n",
      "Train Epoch: 0 [43360/482500 (9%)]\tLoss: 1.483846\n",
      "Train Epoch: 0 [43520/482500 (9%)]\tLoss: 4.009910\n",
      "Train Epoch: 0 [43680/482500 (9%)]\tLoss: 3.758430\n",
      "Train Epoch: 0 [43840/482500 (9%)]\tLoss: 2.755101\n",
      "Train Epoch: 0 [44000/482500 (9%)]\tLoss: 4.423033\n",
      "Train Epoch: 0 [44160/482500 (9%)]\tLoss: 3.408486\n",
      "Train Epoch: 0 [44320/482500 (9%)]\tLoss: 2.981499\n",
      "Train Epoch: 0 [44480/482500 (9%)]\tLoss: 2.294255\n",
      "Train Epoch: 0 [44640/482500 (9%)]\tLoss: 1.265616\n",
      "Train Epoch: 0 [44800/482500 (9%)]\tLoss: 1.492831\n",
      "Train Epoch: 0 [44960/482500 (9%)]\tLoss: 2.382065\n",
      "Train Epoch: 0 [45120/482500 (9%)]\tLoss: 1.919983\n",
      "Train Epoch: 0 [45280/482500 (9%)]\tLoss: 2.215410\n",
      "Train Epoch: 0 [45440/482500 (9%)]\tLoss: 3.022402\n",
      "Train Epoch: 0 [45600/482500 (9%)]\tLoss: 1.337597\n",
      "Train Epoch: 0 [45760/482500 (9%)]\tLoss: 2.649770\n",
      "Train Epoch: 0 [45920/482500 (10%)]\tLoss: 2.830668\n",
      "Train Epoch: 0 [46080/482500 (10%)]\tLoss: 2.076655\n",
      "Train Epoch: 0 [46240/482500 (10%)]\tLoss: 2.488046\n",
      "Train Epoch: 0 [46400/482500 (10%)]\tLoss: 2.468307\n",
      "Train Epoch: 0 [46560/482500 (10%)]\tLoss: 1.359481\n",
      "Train Epoch: 0 [46720/482500 (10%)]\tLoss: 1.155060\n",
      "Train Epoch: 0 [46880/482500 (10%)]\tLoss: 1.304837\n",
      "Train Epoch: 0 [47040/482500 (10%)]\tLoss: 3.010625\n",
      "Train Epoch: 0 [47200/482500 (10%)]\tLoss: 2.145261\n",
      "Train Epoch: 0 [47360/482500 (10%)]\tLoss: 0.852871\n",
      "Train Epoch: 0 [47520/482500 (10%)]\tLoss: 2.526006\n",
      "Train Epoch: 0 [47680/482500 (10%)]\tLoss: 1.293674\n",
      "Train Epoch: 0 [47840/482500 (10%)]\tLoss: 2.063802\n",
      "Train Epoch: 0 [48000/482500 (10%)]\tLoss: 2.281364\n",
      "Train Epoch: 0 [48160/482500 (10%)]\tLoss: 2.489643\n",
      "Train Epoch: 0 [48320/482500 (10%)]\tLoss: 1.391051\n",
      "Train Epoch: 0 [48480/482500 (10%)]\tLoss: 4.197190\n",
      "Train Epoch: 0 [48640/482500 (10%)]\tLoss: 3.135839\n",
      "Train Epoch: 0 [48800/482500 (10%)]\tLoss: 1.397640\n",
      "Train Epoch: 0 [48960/482500 (10%)]\tLoss: 2.412136\n",
      "Train Epoch: 0 [49120/482500 (10%)]\tLoss: 2.841593\n",
      "Train Epoch: 0 [49280/482500 (10%)]\tLoss: 2.793100\n",
      "Train Epoch: 0 [49440/482500 (10%)]\tLoss: 1.920483\n",
      "Train Epoch: 0 [49600/482500 (10%)]\tLoss: 1.347405\n",
      "Train Epoch: 0 [49760/482500 (10%)]\tLoss: 4.183336\n",
      "Train Epoch: 0 [49920/482500 (10%)]\tLoss: 1.438479\n",
      "Train Epoch: 0 [50080/482500 (10%)]\tLoss: 3.003395\n",
      "Train Epoch: 0 [50240/482500 (10%)]\tLoss: 5.664129\n",
      "Train Epoch: 0 [50400/482500 (10%)]\tLoss: 1.919491\n",
      "Train Epoch: 0 [50560/482500 (10%)]\tLoss: 2.388149\n",
      "Train Epoch: 0 [50720/482500 (11%)]\tLoss: 2.761825\n",
      "Train Epoch: 0 [50880/482500 (11%)]\tLoss: 1.866298\n",
      "Train Epoch: 0 [51040/482500 (11%)]\tLoss: 4.587191\n",
      "Train Epoch: 0 [51200/482500 (11%)]\tLoss: 2.211089\n",
      "Train Epoch: 0 [51360/482500 (11%)]\tLoss: 1.620961\n",
      "Train Epoch: 0 [51520/482500 (11%)]\tLoss: 3.541191\n",
      "Train Epoch: 0 [51680/482500 (11%)]\tLoss: 1.583231\n",
      "Train Epoch: 0 [51840/482500 (11%)]\tLoss: 3.075864\n",
      "Train Epoch: 0 [52000/482500 (11%)]\tLoss: 1.001202\n",
      "Train Epoch: 0 [52160/482500 (11%)]\tLoss: 2.102222\n",
      "Train Epoch: 0 [52320/482500 (11%)]\tLoss: 1.196834\n",
      "Train Epoch: 0 [52480/482500 (11%)]\tLoss: 1.557038\n",
      "Train Epoch: 0 [52640/482500 (11%)]\tLoss: 22.528194\n",
      "Train Epoch: 0 [52800/482500 (11%)]\tLoss: 57.551071\n",
      "Train Epoch: 0 [52960/482500 (11%)]\tLoss: 14.070007\n",
      "Train Epoch: 0 [53120/482500 (11%)]\tLoss: 11.594809\n",
      "Train Epoch: 0 [53280/482500 (11%)]\tLoss: 18.594435\n",
      "Train Epoch: 0 [53440/482500 (11%)]\tLoss: 51.563183\n",
      "Train Epoch: 0 [53600/482500 (11%)]\tLoss: 5.929056\n",
      "Train Epoch: 0 [53760/482500 (11%)]\tLoss: 3.379537\n",
      "Train Epoch: 0 [53920/482500 (11%)]\tLoss: 13.559753\n",
      "Train Epoch: 0 [54080/482500 (11%)]\tLoss: 12.312016\n",
      "Train Epoch: 0 [54240/482500 (11%)]\tLoss: 15.698277\n",
      "Train Epoch: 0 [54400/482500 (11%)]\tLoss: 2.194180\n",
      "Train Epoch: 0 [54560/482500 (11%)]\tLoss: 2.309412\n",
      "Train Epoch: 0 [54720/482500 (11%)]\tLoss: 3.978774\n",
      "Train Epoch: 0 [54880/482500 (11%)]\tLoss: 4.874506\n",
      "Train Epoch: 0 [55040/482500 (11%)]\tLoss: 2.665998\n",
      "Train Epoch: 0 [55200/482500 (11%)]\tLoss: 2.223387\n",
      "Train Epoch: 0 [55360/482500 (11%)]\tLoss: 4.559739\n",
      "Train Epoch: 0 [55520/482500 (12%)]\tLoss: 9.855583\n",
      "Train Epoch: 0 [55680/482500 (12%)]\tLoss: 1.583468\n",
      "Train Epoch: 0 [55840/482500 (12%)]\tLoss: 3.425564\n",
      "Train Epoch: 0 [56000/482500 (12%)]\tLoss: 2.286652\n",
      "Train Epoch: 0 [56160/482500 (12%)]\tLoss: 2.318133\n",
      "Train Epoch: 0 [56320/482500 (12%)]\tLoss: 1.736232\n",
      "Train Epoch: 0 [56480/482500 (12%)]\tLoss: 5.034410\n",
      "Train Epoch: 0 [56640/482500 (12%)]\tLoss: 2.204170\n",
      "Train Epoch: 0 [56800/482500 (12%)]\tLoss: 1.318988\n",
      "Train Epoch: 0 [56960/482500 (12%)]\tLoss: 2.274281\n",
      "Train Epoch: 0 [57120/482500 (12%)]\tLoss: 2.273875\n",
      "Train Epoch: 0 [57280/482500 (12%)]\tLoss: 1.993980\n",
      "Train Epoch: 0 [57440/482500 (12%)]\tLoss: 2.052915\n",
      "Train Epoch: 0 [57600/482500 (12%)]\tLoss: 1.472997\n",
      "Train Epoch: 0 [57760/482500 (12%)]\tLoss: 1.394133\n",
      "Train Epoch: 0 [57920/482500 (12%)]\tLoss: 1.320674\n",
      "Train Epoch: 0 [58080/482500 (12%)]\tLoss: 2.389116\n",
      "Train Epoch: 0 [58240/482500 (12%)]\tLoss: 2.084218\n",
      "Train Epoch: 0 [58400/482500 (12%)]\tLoss: 1.522231\n",
      "Train Epoch: 0 [58560/482500 (12%)]\tLoss: 1.843516\n",
      "Train Epoch: 0 [58720/482500 (12%)]\tLoss: 4.446597\n",
      "Train Epoch: 0 [58880/482500 (12%)]\tLoss: 2.465865\n",
      "Train Epoch: 0 [59040/482500 (12%)]\tLoss: 1.885905\n",
      "Train Epoch: 0 [59200/482500 (12%)]\tLoss: 1.597975\n",
      "Train Epoch: 0 [59360/482500 (12%)]\tLoss: 1.274806\n",
      "Train Epoch: 0 [59520/482500 (12%)]\tLoss: 22.493782\n",
      "Train Epoch: 0 [59680/482500 (12%)]\tLoss: 3.203457\n",
      "Train Epoch: 0 [59840/482500 (12%)]\tLoss: 3.356240\n",
      "Train Epoch: 0 [60000/482500 (12%)]\tLoss: 10.763148\n",
      "Train Epoch: 0 [60160/482500 (12%)]\tLoss: 3.226763\n",
      "Train Epoch: 0 [60320/482500 (13%)]\tLoss: 1.321279\n",
      "Train Epoch: 0 [60480/482500 (13%)]\tLoss: 13.162055\n",
      "Train Epoch: 0 [60640/482500 (13%)]\tLoss: 4.902671\n",
      "Train Epoch: 0 [60800/482500 (13%)]\tLoss: 7.254212\n",
      "Train Epoch: 0 [60960/482500 (13%)]\tLoss: 4.755942\n",
      "Train Epoch: 0 [61120/482500 (13%)]\tLoss: 1.160541\n",
      "Train Epoch: 0 [61280/482500 (13%)]\tLoss: 2.468860\n",
      "Train Epoch: 0 [61440/482500 (13%)]\tLoss: 2.903417\n",
      "Train Epoch: 0 [61600/482500 (13%)]\tLoss: 1.054886\n",
      "Train Epoch: 0 [61760/482500 (13%)]\tLoss: 6.922896\n",
      "Train Epoch: 0 [61920/482500 (13%)]\tLoss: 1.636895\n",
      "Train Epoch: 0 [62080/482500 (13%)]\tLoss: 1.086966\n",
      "Train Epoch: 0 [62240/482500 (13%)]\tLoss: 1.813505\n",
      "Train Epoch: 0 [62400/482500 (13%)]\tLoss: 1.950254\n",
      "Train Epoch: 0 [62560/482500 (13%)]\tLoss: 2.062743\n",
      "Train Epoch: 0 [62720/482500 (13%)]\tLoss: 1.172303\n",
      "Train Epoch: 0 [62880/482500 (13%)]\tLoss: 1.430028\n",
      "Train Epoch: 0 [63040/482500 (13%)]\tLoss: 1.134251\n",
      "Train Epoch: 0 [63200/482500 (13%)]\tLoss: 2.052454\n",
      "Train Epoch: 0 [63360/482500 (13%)]\tLoss: 3.259211\n",
      "Train Epoch: 0 [63520/482500 (13%)]\tLoss: 1.873589\n",
      "Train Epoch: 0 [63680/482500 (13%)]\tLoss: 1.370826\n",
      "Train Epoch: 0 [63840/482500 (13%)]\tLoss: 3.050569\n",
      "Train Epoch: 0 [64000/482500 (13%)]\tLoss: 20.270315\n",
      "Train Epoch: 0 [64160/482500 (13%)]\tLoss: 5.608239\n",
      "Train Epoch: 0 [64320/482500 (13%)]\tLoss: 2.727676\n",
      "Train Epoch: 0 [64480/482500 (13%)]\tLoss: 2.495074\n",
      "Train Epoch: 0 [64640/482500 (13%)]\tLoss: 1.768022\n",
      "Train Epoch: 0 [64800/482500 (13%)]\tLoss: 1.748036\n",
      "Train Epoch: 0 [64960/482500 (13%)]\tLoss: 2.627061\n",
      "Train Epoch: 0 [65120/482500 (13%)]\tLoss: 1.566835\n",
      "Train Epoch: 0 [65280/482500 (14%)]\tLoss: 16.669405\n",
      "Train Epoch: 0 [65440/482500 (14%)]\tLoss: 4.215049\n",
      "Train Epoch: 0 [65600/482500 (14%)]\tLoss: 6.663282\n",
      "Train Epoch: 0 [65760/482500 (14%)]\tLoss: 2.846576\n",
      "Train Epoch: 0 [65920/482500 (14%)]\tLoss: 1.928241\n",
      "Train Epoch: 0 [66080/482500 (14%)]\tLoss: 20.330402\n",
      "Train Epoch: 0 [66240/482500 (14%)]\tLoss: 3.331795\n",
      "Train Epoch: 0 [66400/482500 (14%)]\tLoss: 0.900558\n",
      "Train Epoch: 0 [66560/482500 (14%)]\tLoss: 1.241797\n",
      "Train Epoch: 0 [66720/482500 (14%)]\tLoss: 2.879488\n",
      "Train Epoch: 0 [66880/482500 (14%)]\tLoss: 3.186474\n",
      "Train Epoch: 0 [67040/482500 (14%)]\tLoss: 1.132444\n",
      "Train Epoch: 0 [67200/482500 (14%)]\tLoss: 2.593821\n",
      "Train Epoch: 0 [67360/482500 (14%)]\tLoss: 1.472666\n",
      "Train Epoch: 0 [67520/482500 (14%)]\tLoss: 2.872107\n",
      "Train Epoch: 0 [67680/482500 (14%)]\tLoss: 1.411836\n",
      "Train Epoch: 0 [67840/482500 (14%)]\tLoss: 1.021169\n",
      "Train Epoch: 0 [68000/482500 (14%)]\tLoss: 1.191155\n",
      "Train Epoch: 0 [68160/482500 (14%)]\tLoss: 3.490898\n",
      "Train Epoch: 0 [68320/482500 (14%)]\tLoss: 1.763746\n",
      "Train Epoch: 0 [68480/482500 (14%)]\tLoss: 1.339749\n",
      "Train Epoch: 0 [68640/482500 (14%)]\tLoss: 1.657600\n",
      "Train Epoch: 0 [68800/482500 (14%)]\tLoss: 1.213436\n",
      "Train Epoch: 0 [68960/482500 (14%)]\tLoss: 1.343984\n",
      "Train Epoch: 0 [69120/482500 (14%)]\tLoss: 2.284747\n",
      "Train Epoch: 0 [69280/482500 (14%)]\tLoss: 0.837803\n",
      "Train Epoch: 0 [69440/482500 (14%)]\tLoss: 1.416163\n",
      "Train Epoch: 0 [69600/482500 (14%)]\tLoss: 0.590097\n",
      "Train Epoch: 0 [69760/482500 (14%)]\tLoss: 2.230604\n",
      "Train Epoch: 0 [69920/482500 (14%)]\tLoss: 1.961601\n",
      "Train Epoch: 0 [70080/482500 (15%)]\tLoss: 1.346329\n",
      "Train Epoch: 0 [70240/482500 (15%)]\tLoss: 2.586655\n",
      "Train Epoch: 0 [70400/482500 (15%)]\tLoss: 1.647620\n",
      "Train Epoch: 0 [70560/482500 (15%)]\tLoss: 1.507818\n",
      "Train Epoch: 0 [70720/482500 (15%)]\tLoss: 1.308459\n",
      "Train Epoch: 0 [70880/482500 (15%)]\tLoss: 1.840471\n",
      "Train Epoch: 0 [71040/482500 (15%)]\tLoss: 3.171128\n",
      "Train Epoch: 0 [71200/482500 (15%)]\tLoss: 1.777602\n",
      "Train Epoch: 0 [71360/482500 (15%)]\tLoss: 1.536389\n",
      "Train Epoch: 0 [71520/482500 (15%)]\tLoss: 1.444523\n",
      "Train Epoch: 0 [71680/482500 (15%)]\tLoss: 274.334686\n",
      "Train Epoch: 0 [71840/482500 (15%)]\tLoss: 56.925156\n",
      "Train Epoch: 0 [72000/482500 (15%)]\tLoss: 90.966385\n",
      "Train Epoch: 0 [72160/482500 (15%)]\tLoss: 16.717684\n",
      "Train Epoch: 0 [72320/482500 (15%)]\tLoss: 4.074510\n",
      "Train Epoch: 0 [72480/482500 (15%)]\tLoss: 4.730968\n",
      "Train Epoch: 0 [72640/482500 (15%)]\tLoss: 2.111847\n",
      "Train Epoch: 0 [72800/482500 (15%)]\tLoss: 2.298188\n",
      "Train Epoch: 0 [72960/482500 (15%)]\tLoss: 1.866621\n",
      "Train Epoch: 0 [73120/482500 (15%)]\tLoss: 32.787842\n",
      "Train Epoch: 0 [73280/482500 (15%)]\tLoss: 15.977505\n",
      "Train Epoch: 0 [73440/482500 (15%)]\tLoss: 4.453670\n",
      "Train Epoch: 0 [73600/482500 (15%)]\tLoss: 5.861423\n",
      "Train Epoch: 0 [73760/482500 (15%)]\tLoss: 1.595924\n",
      "Train Epoch: 0 [73920/482500 (15%)]\tLoss: 2.436017\n",
      "Train Epoch: 0 [74080/482500 (15%)]\tLoss: 4.209503\n",
      "Train Epoch: 0 [74240/482500 (15%)]\tLoss: 1.776952\n",
      "Train Epoch: 0 [74400/482500 (15%)]\tLoss: 1.676587\n",
      "Train Epoch: 0 [74560/482500 (15%)]\tLoss: 2.366933\n",
      "Train Epoch: 0 [74720/482500 (15%)]\tLoss: 5.607609\n",
      "Train Epoch: 0 [74880/482500 (16%)]\tLoss: 3.617143\n",
      "Train Epoch: 0 [75040/482500 (16%)]\tLoss: 1.012657\n",
      "Train Epoch: 0 [75200/482500 (16%)]\tLoss: 1.270568\n",
      "Train Epoch: 0 [75360/482500 (16%)]\tLoss: 2.193394\n",
      "Train Epoch: 0 [75520/482500 (16%)]\tLoss: 1.131945\n",
      "Train Epoch: 0 [75680/482500 (16%)]\tLoss: 0.947534\n",
      "Train Epoch: 0 [75840/482500 (16%)]\tLoss: 31.255119\n",
      "Train Epoch: 0 [76000/482500 (16%)]\tLoss: 51.732708\n",
      "Train Epoch: 0 [76160/482500 (16%)]\tLoss: 3.021359\n",
      "Train Epoch: 0 [76320/482500 (16%)]\tLoss: 5.403607\n",
      "Train Epoch: 0 [76480/482500 (16%)]\tLoss: 3.423259\n",
      "Train Epoch: 0 [76640/482500 (16%)]\tLoss: 5.557812\n",
      "Train Epoch: 0 [76800/482500 (16%)]\tLoss: 2.771420\n",
      "Train Epoch: 0 [76960/482500 (16%)]\tLoss: 1.671862\n",
      "Train Epoch: 0 [77120/482500 (16%)]\tLoss: 1.040123\n",
      "Train Epoch: 0 [77280/482500 (16%)]\tLoss: 1.818471\n",
      "Train Epoch: 0 [77440/482500 (16%)]\tLoss: 1.103084\n",
      "Train Epoch: 0 [77600/482500 (16%)]\tLoss: 4.195480\n",
      "Train Epoch: 0 [77760/482500 (16%)]\tLoss: 3.102467\n",
      "Train Epoch: 0 [77920/482500 (16%)]\tLoss: 3.092902\n",
      "Train Epoch: 0 [78080/482500 (16%)]\tLoss: 2.096277\n",
      "Train Epoch: 0 [78240/482500 (16%)]\tLoss: 3.238545\n",
      "Train Epoch: 0 [78400/482500 (16%)]\tLoss: 2.721988\n",
      "Train Epoch: 0 [78560/482500 (16%)]\tLoss: 1.551935\n",
      "Train Epoch: 0 [78720/482500 (16%)]\tLoss: 75.857864\n",
      "Train Epoch: 0 [78880/482500 (16%)]\tLoss: 11.561753\n",
      "Train Epoch: 0 [79040/482500 (16%)]\tLoss: 3.205456\n",
      "Train Epoch: 0 [79200/482500 (16%)]\tLoss: 1.708311\n",
      "Train Epoch: 0 [79360/482500 (16%)]\tLoss: 1.711170\n",
      "Train Epoch: 0 [79520/482500 (16%)]\tLoss: 1.251964\n",
      "Train Epoch: 0 [79680/482500 (17%)]\tLoss: 1.266799\n",
      "Train Epoch: 0 [79840/482500 (17%)]\tLoss: 1.847525\n",
      "Train Epoch: 0 [80000/482500 (17%)]\tLoss: 2.212471\n",
      "Train Epoch: 0 [80160/482500 (17%)]\tLoss: 3.133758\n",
      "Train Epoch: 0 [80320/482500 (17%)]\tLoss: 6.387784\n",
      "Train Epoch: 0 [80480/482500 (17%)]\tLoss: 3.193560\n",
      "Train Epoch: 0 [80640/482500 (17%)]\tLoss: 2.279317\n",
      "Train Epoch: 0 [80800/482500 (17%)]\tLoss: 8682.579102\n",
      "Train Epoch: 0 [80960/482500 (17%)]\tLoss: 275.940277\n",
      "Train Epoch: 0 [81120/482500 (17%)]\tLoss: 126.855049\n",
      "Train Epoch: 0 [81280/482500 (17%)]\tLoss: 15.655863\n",
      "Train Epoch: 0 [81440/482500 (17%)]\tLoss: 6.396394\n",
      "Train Epoch: 0 [81600/482500 (17%)]\tLoss: 4.228380\n",
      "Train Epoch: 0 [81760/482500 (17%)]\tLoss: 2.839635\n",
      "Train Epoch: 0 [81920/482500 (17%)]\tLoss: 1.360781\n",
      "Train Epoch: 0 [82080/482500 (17%)]\tLoss: 1.718538\n",
      "Train Epoch: 0 [82240/482500 (17%)]\tLoss: 1.997313\n",
      "Train Epoch: 0 [82400/482500 (17%)]\tLoss: 3.792567\n",
      "Train Epoch: 0 [82560/482500 (17%)]\tLoss: 6.819186\n",
      "Train Epoch: 0 [82720/482500 (17%)]\tLoss: 3.915361\n",
      "Train Epoch: 0 [82880/482500 (17%)]\tLoss: 1.651435\n",
      "Train Epoch: 0 [83040/482500 (17%)]\tLoss: 2.540932\n",
      "Train Epoch: 0 [83200/482500 (17%)]\tLoss: 3.758443\n",
      "Train Epoch: 0 [83360/482500 (17%)]\tLoss: 1.568793\n",
      "Train Epoch: 0 [83520/482500 (17%)]\tLoss: 828.624390\n",
      "Train Epoch: 0 [83680/482500 (17%)]\tLoss: 217.038589\n",
      "Train Epoch: 0 [83840/482500 (17%)]\tLoss: 51.053978\n",
      "Train Epoch: 0 [84000/482500 (17%)]\tLoss: 13.432312\n",
      "Train Epoch: 0 [84160/482500 (17%)]\tLoss: 4.053874\n",
      "Train Epoch: 0 [84320/482500 (17%)]\tLoss: 5.136147\n",
      "Train Epoch: 0 [84480/482500 (18%)]\tLoss: 3.322587\n",
      "Train Epoch: 0 [84640/482500 (18%)]\tLoss: 4.908006\n",
      "Train Epoch: 0 [84800/482500 (18%)]\tLoss: 2.796893\n",
      "Train Epoch: 0 [84960/482500 (18%)]\tLoss: 3.629170\n",
      "Train Epoch: 0 [85120/482500 (18%)]\tLoss: 1.378849\n",
      "Train Epoch: 0 [85280/482500 (18%)]\tLoss: 1.982410\n",
      "Train Epoch: 0 [85440/482500 (18%)]\tLoss: 2.242344\n",
      "Train Epoch: 0 [85600/482500 (18%)]\tLoss: 6.910434\n",
      "Train Epoch: 0 [85760/482500 (18%)]\tLoss: 2.225374\n",
      "Train Epoch: 0 [85920/482500 (18%)]\tLoss: 3.052007\n",
      "Train Epoch: 0 [86080/482500 (18%)]\tLoss: 1.054919\n",
      "Train Epoch: 0 [86240/482500 (18%)]\tLoss: 1.664267\n",
      "Train Epoch: 0 [86400/482500 (18%)]\tLoss: 1.638547\n",
      "Train Epoch: 0 [86560/482500 (18%)]\tLoss: 3.239388\n",
      "Train Epoch: 0 [86720/482500 (18%)]\tLoss: 1.569634\n",
      "Train Epoch: 0 [86880/482500 (18%)]\tLoss: 2.103130\n",
      "Train Epoch: 0 [87040/482500 (18%)]\tLoss: 2.081719\n",
      "Train Epoch: 0 [87200/482500 (18%)]\tLoss: 22.315022\n",
      "Train Epoch: 0 [87360/482500 (18%)]\tLoss: 7.764758\n",
      "Train Epoch: 0 [87520/482500 (18%)]\tLoss: 36.389374\n",
      "Train Epoch: 0 [87680/482500 (18%)]\tLoss: 8.863874\n",
      "Train Epoch: 0 [87840/482500 (18%)]\tLoss: 5.294659\n",
      "Train Epoch: 0 [88000/482500 (18%)]\tLoss: 3.329531\n",
      "Train Epoch: 0 [88160/482500 (18%)]\tLoss: 2.181620\n",
      "Train Epoch: 0 [88320/482500 (18%)]\tLoss: 2.952776\n",
      "Train Epoch: 0 [88480/482500 (18%)]\tLoss: 2.481348\n",
      "Train Epoch: 0 [88640/482500 (18%)]\tLoss: 2.242367\n",
      "Train Epoch: 0 [88800/482500 (18%)]\tLoss: 2.541084\n",
      "Train Epoch: 0 [88960/482500 (18%)]\tLoss: 3.203257\n",
      "Train Epoch: 0 [89120/482500 (18%)]\tLoss: 3.981471\n",
      "Train Epoch: 0 [89280/482500 (19%)]\tLoss: 1.664338\n",
      "Train Epoch: 0 [89440/482500 (19%)]\tLoss: 2.366440\n",
      "Train Epoch: 0 [89600/482500 (19%)]\tLoss: 3.250697\n",
      "Train Epoch: 0 [89760/482500 (19%)]\tLoss: 1.842176\n",
      "Train Epoch: 0 [89920/482500 (19%)]\tLoss: 2.216058\n",
      "Train Epoch: 0 [90080/482500 (19%)]\tLoss: 2.985357\n",
      "Train Epoch: 0 [90240/482500 (19%)]\tLoss: 2.203264\n",
      "Train Epoch: 0 [90400/482500 (19%)]\tLoss: 1.739103\n",
      "Train Epoch: 0 [90560/482500 (19%)]\tLoss: 3.241138\n",
      "Train Epoch: 0 [90720/482500 (19%)]\tLoss: 1.868460\n",
      "Train Epoch: 0 [90880/482500 (19%)]\tLoss: 3.745369\n",
      "Train Epoch: 0 [91040/482500 (19%)]\tLoss: 4.784842\n",
      "Train Epoch: 0 [91200/482500 (19%)]\tLoss: 2.022721\n",
      "Train Epoch: 0 [91360/482500 (19%)]\tLoss: 2.362717\n",
      "Train Epoch: 0 [91520/482500 (19%)]\tLoss: 1.846498\n",
      "Train Epoch: 0 [91680/482500 (19%)]\tLoss: 1.780200\n",
      "Train Epoch: 0 [91840/482500 (19%)]\tLoss: 2.208653\n",
      "Train Epoch: 0 [92000/482500 (19%)]\tLoss: 3.214841\n",
      "Train Epoch: 0 [92160/482500 (19%)]\tLoss: 3.554856\n",
      "Train Epoch: 0 [92320/482500 (19%)]\tLoss: 3.045376\n",
      "Train Epoch: 0 [92480/482500 (19%)]\tLoss: 2.362209\n",
      "Train Epoch: 0 [92640/482500 (19%)]\tLoss: 3.412516\n",
      "Train Epoch: 0 [92800/482500 (19%)]\tLoss: 5.243253\n",
      "Train Epoch: 0 [92960/482500 (19%)]\tLoss: 1.462228\n",
      "Train Epoch: 0 [93120/482500 (19%)]\tLoss: 1.415665\n",
      "Train Epoch: 0 [93280/482500 (19%)]\tLoss: 3.938395\n",
      "Train Epoch: 0 [93440/482500 (19%)]\tLoss: 1.772302\n",
      "Train Epoch: 0 [93600/482500 (19%)]\tLoss: 1.765616\n",
      "Train Epoch: 0 [93760/482500 (19%)]\tLoss: 2.384554\n",
      "Train Epoch: 0 [93920/482500 (19%)]\tLoss: 2.932386\n",
      "Train Epoch: 0 [94080/482500 (19%)]\tLoss: 0.741646\n",
      "Train Epoch: 0 [94240/482500 (20%)]\tLoss: 3.274816\n",
      "Train Epoch: 0 [94400/482500 (20%)]\tLoss: 8.260172\n",
      "Train Epoch: 0 [94560/482500 (20%)]\tLoss: 1.027022\n",
      "Train Epoch: 0 [94720/482500 (20%)]\tLoss: 29.470463\n",
      "Train Epoch: 0 [94880/482500 (20%)]\tLoss: 60.601303\n",
      "Train Epoch: 0 [95040/482500 (20%)]\tLoss: 3.737689\n",
      "Train Epoch: 0 [95200/482500 (20%)]\tLoss: 4.527400\n",
      "Train Epoch: 0 [95360/482500 (20%)]\tLoss: 1.875535\n",
      "Train Epoch: 0 [95520/482500 (20%)]\tLoss: 1.198661\n",
      "Train Epoch: 0 [95680/482500 (20%)]\tLoss: 1.682497\n",
      "Train Epoch: 0 [95840/482500 (20%)]\tLoss: 1.930663\n",
      "Train Epoch: 0 [96000/482500 (20%)]\tLoss: 2.735822\n",
      "Train Epoch: 0 [96160/482500 (20%)]\tLoss: 0.875181\n",
      "Train Epoch: 0 [96320/482500 (20%)]\tLoss: 2.491154\n",
      "Train Epoch: 0 [96480/482500 (20%)]\tLoss: 2.178622\n",
      "Train Epoch: 0 [96640/482500 (20%)]\tLoss: 1.439967\n",
      "Train Epoch: 0 [96800/482500 (20%)]\tLoss: 1.825918\n",
      "Train Epoch: 0 [96960/482500 (20%)]\tLoss: 1.050834\n",
      "Train Epoch: 0 [97120/482500 (20%)]\tLoss: 1.714639\n",
      "Train Epoch: 0 [97280/482500 (20%)]\tLoss: 2.297686\n",
      "Train Epoch: 0 [97440/482500 (20%)]\tLoss: 1.321856\n",
      "Train Epoch: 0 [97600/482500 (20%)]\tLoss: 2.272067\n",
      "Train Epoch: 0 [97760/482500 (20%)]\tLoss: 2.271379\n",
      "Train Epoch: 0 [97920/482500 (20%)]\tLoss: 4.370805\n",
      "Train Epoch: 0 [98080/482500 (20%)]\tLoss: 3.750163\n",
      "Train Epoch: 0 [98240/482500 (20%)]\tLoss: 1.201362\n",
      "Train Epoch: 0 [98400/482500 (20%)]\tLoss: 1.636465\n",
      "Train Epoch: 0 [98560/482500 (20%)]\tLoss: 1.547943\n",
      "Train Epoch: 0 [98720/482500 (20%)]\tLoss: 2.033894\n",
      "Train Epoch: 0 [98880/482500 (20%)]\tLoss: 2.177018\n",
      "Train Epoch: 0 [99040/482500 (21%)]\tLoss: 1.700858\n",
      "Train Epoch: 0 [99200/482500 (21%)]\tLoss: 3.272793\n",
      "Train Epoch: 0 [99360/482500 (21%)]\tLoss: 1.428831\n",
      "Train Epoch: 0 [99520/482500 (21%)]\tLoss: 1.785995\n",
      "Train Epoch: 0 [99680/482500 (21%)]\tLoss: 0.774619\n",
      "Train Epoch: 0 [99840/482500 (21%)]\tLoss: 1.477718\n",
      "Train Epoch: 0 [100000/482500 (21%)]\tLoss: 1.320946\n",
      "Train Epoch: 0 [100160/482500 (21%)]\tLoss: 1.681897\n",
      "Train Epoch: 0 [100320/482500 (21%)]\tLoss: 2.173033\n",
      "Train Epoch: 0 [100480/482500 (21%)]\tLoss: 1.730464\n",
      "Train Epoch: 0 [100640/482500 (21%)]\tLoss: 2.985334\n",
      "Train Epoch: 0 [100800/482500 (21%)]\tLoss: 1.348602\n",
      "Train Epoch: 0 [100960/482500 (21%)]\tLoss: 1.156163\n",
      "Train Epoch: 0 [101120/482500 (21%)]\tLoss: 0.861035\n",
      "Train Epoch: 0 [101280/482500 (21%)]\tLoss: 516.804626\n",
      "Train Epoch: 0 [101440/482500 (21%)]\tLoss: 22.866575\n",
      "Train Epoch: 0 [101600/482500 (21%)]\tLoss: 34.362572\n",
      "Train Epoch: 0 [101760/482500 (21%)]\tLoss: 6.935786\n",
      "Train Epoch: 0 [101920/482500 (21%)]\tLoss: 5.160228\n",
      "Train Epoch: 0 [102080/482500 (21%)]\tLoss: 2.216879\n",
      "Train Epoch: 0 [102240/482500 (21%)]\tLoss: 3.822224\n",
      "Train Epoch: 0 [102400/482500 (21%)]\tLoss: 3.255270\n",
      "Train Epoch: 0 [102560/482500 (21%)]\tLoss: 6.202816\n",
      "Train Epoch: 0 [102720/482500 (21%)]\tLoss: 7.556950\n",
      "Train Epoch: 0 [102880/482500 (21%)]\tLoss: 2.820069\n",
      "Train Epoch: 0 [103040/482500 (21%)]\tLoss: 1.920682\n",
      "Train Epoch: 0 [103200/482500 (21%)]\tLoss: 3.013172\n",
      "Train Epoch: 0 [103360/482500 (21%)]\tLoss: 2.466450\n",
      "Train Epoch: 0 [103520/482500 (21%)]\tLoss: 1.243220\n",
      "Train Epoch: 0 [103680/482500 (21%)]\tLoss: 1.557938\n",
      "Train Epoch: 0 [103840/482500 (22%)]\tLoss: 3.061225\n",
      "Train Epoch: 0 [104000/482500 (22%)]\tLoss: 2.593318\n",
      "Train Epoch: 0 [104160/482500 (22%)]\tLoss: 2.227269\n",
      "Train Epoch: 0 [104320/482500 (22%)]\tLoss: 1.279938\n",
      "Train Epoch: 0 [104480/482500 (22%)]\tLoss: 2.189456\n",
      "Train Epoch: 0 [104640/482500 (22%)]\tLoss: 1.599980\n",
      "Train Epoch: 0 [104800/482500 (22%)]\tLoss: 2.160257\n",
      "Train Epoch: 0 [104960/482500 (22%)]\tLoss: 1.121679\n",
      "Train Epoch: 0 [105120/482500 (22%)]\tLoss: 1.877901\n",
      "Train Epoch: 0 [105280/482500 (22%)]\tLoss: 0.821608\n",
      "Train Epoch: 0 [105440/482500 (22%)]\tLoss: 1.354750\n",
      "Train Epoch: 0 [105600/482500 (22%)]\tLoss: 1.099445\n",
      "Train Epoch: 0 [105760/482500 (22%)]\tLoss: 0.784167\n",
      "Train Epoch: 0 [105920/482500 (22%)]\tLoss: 0.950425\n",
      "Train Epoch: 0 [106080/482500 (22%)]\tLoss: 1.913134\n",
      "Train Epoch: 0 [106240/482500 (22%)]\tLoss: 2.161716\n",
      "Train Epoch: 0 [106400/482500 (22%)]\tLoss: 0.952156\n",
      "Train Epoch: 0 [106560/482500 (22%)]\tLoss: 1.392854\n",
      "Train Epoch: 0 [106720/482500 (22%)]\tLoss: 1.388017\n",
      "Train Epoch: 0 [106880/482500 (22%)]\tLoss: 1.444545\n",
      "Train Epoch: 0 [107040/482500 (22%)]\tLoss: 1.811197\n",
      "Train Epoch: 0 [107200/482500 (22%)]\tLoss: 2.418158\n",
      "Train Epoch: 0 [107360/482500 (22%)]\tLoss: 1.517413\n",
      "Train Epoch: 0 [107520/482500 (22%)]\tLoss: 2.637093\n",
      "Train Epoch: 0 [107680/482500 (22%)]\tLoss: 3.357294\n",
      "Train Epoch: 0 [107840/482500 (22%)]\tLoss: 2.090109\n",
      "Train Epoch: 0 [108000/482500 (22%)]\tLoss: 1.460503\n",
      "Train Epoch: 0 [108160/482500 (22%)]\tLoss: 1.077127\n",
      "Train Epoch: 0 [108320/482500 (22%)]\tLoss: 2.689080\n",
      "Train Epoch: 0 [108480/482500 (22%)]\tLoss: 869.403564\n",
      "Train Epoch: 0 [108640/482500 (23%)]\tLoss: 55.740627\n",
      "Train Epoch: 0 [108800/482500 (23%)]\tLoss: 11.574953\n",
      "Train Epoch: 0 [108960/482500 (23%)]\tLoss: 4.363790\n",
      "Train Epoch: 0 [109120/482500 (23%)]\tLoss: 2.419937\n",
      "Train Epoch: 0 [109280/482500 (23%)]\tLoss: 4.395747\n",
      "Train Epoch: 0 [109440/482500 (23%)]\tLoss: 4.104379\n",
      "Train Epoch: 0 [109600/482500 (23%)]\tLoss: 3.062517\n",
      "Train Epoch: 0 [109760/482500 (23%)]\tLoss: 4.448361\n",
      "Train Epoch: 0 [109920/482500 (23%)]\tLoss: 2.667376\n",
      "Train Epoch: 0 [110080/482500 (23%)]\tLoss: 1.754100\n",
      "Train Epoch: 0 [110240/482500 (23%)]\tLoss: 16.289551\n",
      "Train Epoch: 0 [110400/482500 (23%)]\tLoss: 3.668210\n",
      "Train Epoch: 0 [110560/482500 (23%)]\tLoss: 16.341179\n",
      "Train Epoch: 0 [110720/482500 (23%)]\tLoss: 3.304561\n",
      "Train Epoch: 0 [110880/482500 (23%)]\tLoss: 3.143235\n",
      "Train Epoch: 0 [111040/482500 (23%)]\tLoss: 3.933283\n",
      "Train Epoch: 0 [111200/482500 (23%)]\tLoss: 5.138394\n",
      "Train Epoch: 0 [111360/482500 (23%)]\tLoss: 8.217991\n",
      "Train Epoch: 0 [111520/482500 (23%)]\tLoss: 3.574157\n",
      "Train Epoch: 0 [111680/482500 (23%)]\tLoss: 2.288968\n",
      "Train Epoch: 0 [111840/482500 (23%)]\tLoss: 1.955308\n",
      "Train Epoch: 0 [112000/482500 (23%)]\tLoss: 2.502476\n",
      "Train Epoch: 0 [112160/482500 (23%)]\tLoss: 2.445816\n",
      "Train Epoch: 0 [112320/482500 (23%)]\tLoss: 2.511187\n",
      "Train Epoch: 0 [112480/482500 (23%)]\tLoss: 5.937054\n",
      "Train Epoch: 0 [112640/482500 (23%)]\tLoss: 3.691169\n",
      "Train Epoch: 0 [112800/482500 (23%)]\tLoss: 2.474558\n",
      "Train Epoch: 0 [112960/482500 (23%)]\tLoss: 23.145124\n",
      "Train Epoch: 0 [113120/482500 (23%)]\tLoss: 7.446577\n",
      "Train Epoch: 0 [113280/482500 (23%)]\tLoss: 5.678916\n",
      "Train Epoch: 0 [113440/482500 (24%)]\tLoss: 2.808624\n",
      "Train Epoch: 0 [113600/482500 (24%)]\tLoss: 2.402677\n",
      "Train Epoch: 0 [113760/482500 (24%)]\tLoss: 3.675765\n",
      "Train Epoch: 0 [113920/482500 (24%)]\tLoss: 2.010525\n",
      "Train Epoch: 0 [114080/482500 (24%)]\tLoss: 1.759838\n",
      "Train Epoch: 0 [114240/482500 (24%)]\tLoss: 1.084763\n",
      "Train Epoch: 0 [114400/482500 (24%)]\tLoss: 1.682138\n",
      "Train Epoch: 0 [114560/482500 (24%)]\tLoss: 1.746437\n",
      "Train Epoch: 0 [114720/482500 (24%)]\tLoss: 2.208290\n",
      "Train Epoch: 0 [114880/482500 (24%)]\tLoss: 3.991349\n",
      "Train Epoch: 0 [115040/482500 (24%)]\tLoss: 2.045225\n",
      "Train Epoch: 0 [115200/482500 (24%)]\tLoss: 1.544169\n",
      "Train Epoch: 0 [115360/482500 (24%)]\tLoss: 1.993697\n",
      "Train Epoch: 0 [115520/482500 (24%)]\tLoss: 1.656770\n",
      "Train Epoch: 0 [115680/482500 (24%)]\tLoss: 1.118969\n",
      "Train Epoch: 0 [115840/482500 (24%)]\tLoss: 1.604498\n",
      "Train Epoch: 0 [116000/482500 (24%)]\tLoss: 2.193798\n",
      "Train Epoch: 0 [116160/482500 (24%)]\tLoss: 2.782470\n",
      "Train Epoch: 0 [116320/482500 (24%)]\tLoss: 2.216905\n",
      "Train Epoch: 0 [116480/482500 (24%)]\tLoss: 3.815139\n",
      "Train Epoch: 0 [116640/482500 (24%)]\tLoss: 1.722537\n",
      "Train Epoch: 0 [116800/482500 (24%)]\tLoss: 1.155645\n",
      "Train Epoch: 0 [116960/482500 (24%)]\tLoss: 1.871158\n",
      "Train Epoch: 0 [117120/482500 (24%)]\tLoss: 1.045072\n",
      "Train Epoch: 0 [117280/482500 (24%)]\tLoss: 1.216851\n",
      "Train Epoch: 0 [117440/482500 (24%)]\tLoss: 1.144139\n",
      "Train Epoch: 0 [117600/482500 (24%)]\tLoss: 2.437554\n",
      "Train Epoch: 0 [117760/482500 (24%)]\tLoss: 2.134497\n",
      "Train Epoch: 0 [117920/482500 (24%)]\tLoss: 0.955896\n",
      "Train Epoch: 0 [118080/482500 (24%)]\tLoss: 1.695271\n",
      "Train Epoch: 0 [118240/482500 (25%)]\tLoss: 1.853142\n",
      "Train Epoch: 0 [118400/482500 (25%)]\tLoss: 1.253244\n",
      "Train Epoch: 0 [118560/482500 (25%)]\tLoss: 2.401920\n",
      "Train Epoch: 0 [118720/482500 (25%)]\tLoss: 2.986497\n",
      "Train Epoch: 0 [118880/482500 (25%)]\tLoss: 2.296547\n",
      "Train Epoch: 0 [119040/482500 (25%)]\tLoss: 1.544143\n",
      "Train Epoch: 0 [119200/482500 (25%)]\tLoss: 5.580137\n",
      "Train Epoch: 0 [119360/482500 (25%)]\tLoss: 6.096800\n",
      "Train Epoch: 0 [119520/482500 (25%)]\tLoss: 608.515259\n",
      "Train Epoch: 0 [119680/482500 (25%)]\tLoss: 66.358749\n",
      "Train Epoch: 0 [119840/482500 (25%)]\tLoss: 12.175568\n",
      "Train Epoch: 0 [120000/482500 (25%)]\tLoss: 5.712114\n",
      "Train Epoch: 0 [120160/482500 (25%)]\tLoss: 3.311969\n",
      "Train Epoch: 0 [120320/482500 (25%)]\tLoss: 1.728032\n",
      "Train Epoch: 0 [120480/482500 (25%)]\tLoss: 1.308280\n",
      "Train Epoch: 0 [120640/482500 (25%)]\tLoss: 1.602131\n",
      "Train Epoch: 0 [120800/482500 (25%)]\tLoss: 2.020318\n",
      "Train Epoch: 0 [120960/482500 (25%)]\tLoss: 2.496004\n",
      "Train Epoch: 0 [121120/482500 (25%)]\tLoss: 3.271710\n",
      "Train Epoch: 0 [121280/482500 (25%)]\tLoss: 1.429459\n",
      "Train Epoch: 0 [121440/482500 (25%)]\tLoss: 1.875603\n",
      "Train Epoch: 0 [121600/482500 (25%)]\tLoss: 1.038065\n",
      "Train Epoch: 0 [121760/482500 (25%)]\tLoss: 3.341559\n",
      "Train Epoch: 0 [121920/482500 (25%)]\tLoss: 3.229568\n",
      "Train Epoch: 0 [122080/482500 (25%)]\tLoss: 2.193133\n",
      "Train Epoch: 0 [122240/482500 (25%)]\tLoss: 1.565695\n",
      "Train Epoch: 0 [122400/482500 (25%)]\tLoss: 2.192000\n",
      "Train Epoch: 0 [122560/482500 (25%)]\tLoss: 1.390647\n",
      "Train Epoch: 0 [122720/482500 (25%)]\tLoss: 1.941257\n",
      "Train Epoch: 0 [122880/482500 (25%)]\tLoss: 1.275212\n",
      "Train Epoch: 0 [123040/482500 (26%)]\tLoss: 3.820137\n",
      "Train Epoch: 0 [123200/482500 (26%)]\tLoss: 1.857187\n",
      "Train Epoch: 0 [123360/482500 (26%)]\tLoss: 1.443259\n",
      "Train Epoch: 0 [123520/482500 (26%)]\tLoss: 1.295155\n",
      "Train Epoch: 0 [123680/482500 (26%)]\tLoss: 1.734274\n",
      "Train Epoch: 0 [123840/482500 (26%)]\tLoss: 1.096490\n",
      "Train Epoch: 0 [124000/482500 (26%)]\tLoss: 2.429682\n",
      "Train Epoch: 0 [124160/482500 (26%)]\tLoss: 1.758818\n",
      "Train Epoch: 0 [124320/482500 (26%)]\tLoss: 1.290824\n",
      "Train Epoch: 0 [124480/482500 (26%)]\tLoss: 2.056125\n",
      "Train Epoch: 0 [124640/482500 (26%)]\tLoss: 0.836879\n",
      "Train Epoch: 0 [124800/482500 (26%)]\tLoss: 1.209038\n",
      "Train Epoch: 0 [124960/482500 (26%)]\tLoss: 1.890834\n",
      "Train Epoch: 0 [125120/482500 (26%)]\tLoss: 1.369597\n",
      "Train Epoch: 0 [125280/482500 (26%)]\tLoss: 2.088227\n",
      "Train Epoch: 0 [125440/482500 (26%)]\tLoss: 2.547023\n",
      "Train Epoch: 0 [125600/482500 (26%)]\tLoss: 1.002325\n",
      "Train Epoch: 0 [125760/482500 (26%)]\tLoss: 5.090366\n",
      "Train Epoch: 0 [125920/482500 (26%)]\tLoss: 1.196800\n",
      "Train Epoch: 0 [126080/482500 (26%)]\tLoss: 1.117708\n",
      "Train Epoch: 0 [126240/482500 (26%)]\tLoss: 1.134113\n",
      "Train Epoch: 0 [126400/482500 (26%)]\tLoss: 1.587960\n",
      "Train Epoch: 0 [126560/482500 (26%)]\tLoss: 0.928335\n",
      "Train Epoch: 0 [126720/482500 (26%)]\tLoss: 1.296121\n",
      "Train Epoch: 0 [126880/482500 (26%)]\tLoss: 1.354861\n",
      "Train Epoch: 0 [127040/482500 (26%)]\tLoss: 3.106241\n",
      "Train Epoch: 0 [127200/482500 (26%)]\tLoss: 1.235609\n",
      "Train Epoch: 0 [127360/482500 (26%)]\tLoss: 14.429846\n",
      "Train Epoch: 0 [127520/482500 (26%)]\tLoss: 5.546237\n",
      "Train Epoch: 0 [127680/482500 (26%)]\tLoss: 3.382427\n",
      "Train Epoch: 0 [127840/482500 (26%)]\tLoss: 2.402466\n",
      "Train Epoch: 0 [128000/482500 (27%)]\tLoss: 1.654311\n",
      "Train Epoch: 0 [128160/482500 (27%)]\tLoss: 0.827425\n",
      "Train Epoch: 0 [128320/482500 (27%)]\tLoss: 1782.266235\n",
      "Train Epoch: 0 [128480/482500 (27%)]\tLoss: 183.509293\n",
      "Train Epoch: 0 [128640/482500 (27%)]\tLoss: 21.750690\n",
      "Train Epoch: 0 [128800/482500 (27%)]\tLoss: 4.569202\n",
      "Train Epoch: 0 [128960/482500 (27%)]\tLoss: 5.145142\n",
      "Train Epoch: 0 [129120/482500 (27%)]\tLoss: 17.724302\n",
      "Train Epoch: 0 [129280/482500 (27%)]\tLoss: 8.209001\n",
      "Train Epoch: 0 [129440/482500 (27%)]\tLoss: 2.582559\n",
      "Train Epoch: 0 [129600/482500 (27%)]\tLoss: 2.390316\n",
      "Train Epoch: 0 [129760/482500 (27%)]\tLoss: 3.760811\n",
      "Train Epoch: 0 [129920/482500 (27%)]\tLoss: 3.715407\n",
      "Train Epoch: 0 [130080/482500 (27%)]\tLoss: 2.635552\n",
      "Train Epoch: 0 [130240/482500 (27%)]\tLoss: 2.859818\n",
      "Train Epoch: 0 [130400/482500 (27%)]\tLoss: 5.539231\n",
      "Train Epoch: 0 [130560/482500 (27%)]\tLoss: 2.613596\n",
      "Train Epoch: 0 [130720/482500 (27%)]\tLoss: 3.756221\n",
      "Train Epoch: 0 [130880/482500 (27%)]\tLoss: 1.642910\n",
      "Train Epoch: 0 [131040/482500 (27%)]\tLoss: 3.231740\n",
      "Train Epoch: 0 [131200/482500 (27%)]\tLoss: 4.017990\n",
      "Train Epoch: 0 [131360/482500 (27%)]\tLoss: 2.134831\n",
      "Train Epoch: 0 [131520/482500 (27%)]\tLoss: 2.145054\n",
      "Train Epoch: 0 [131680/482500 (27%)]\tLoss: 1.832462\n",
      "Train Epoch: 0 [131840/482500 (27%)]\tLoss: 3.831203\n",
      "Train Epoch: 0 [132000/482500 (27%)]\tLoss: 2.672763\n",
      "Train Epoch: 0 [132160/482500 (27%)]\tLoss: 3.585788\n",
      "Train Epoch: 0 [132320/482500 (27%)]\tLoss: 2.263461\n",
      "Train Epoch: 0 [132480/482500 (27%)]\tLoss: 2.323159\n",
      "Train Epoch: 0 [132640/482500 (27%)]\tLoss: 3.154800\n",
      "Train Epoch: 0 [132800/482500 (28%)]\tLoss: 4.502373\n",
      "Train Epoch: 0 [132960/482500 (28%)]\tLoss: 2.772035\n",
      "Train Epoch: 0 [133120/482500 (28%)]\tLoss: 2.613378\n",
      "Train Epoch: 0 [133280/482500 (28%)]\tLoss: 4.398189\n",
      "Train Epoch: 0 [133440/482500 (28%)]\tLoss: 3.064212\n",
      "Train Epoch: 0 [133600/482500 (28%)]\tLoss: 1.584740\n",
      "Train Epoch: 0 [133760/482500 (28%)]\tLoss: 5.743009\n",
      "Train Epoch: 0 [133920/482500 (28%)]\tLoss: 1.762392\n",
      "Train Epoch: 0 [134080/482500 (28%)]\tLoss: 1.868942\n",
      "Train Epoch: 0 [134240/482500 (28%)]\tLoss: 1.784381\n",
      "Train Epoch: 0 [134400/482500 (28%)]\tLoss: 2.391779\n",
      "Train Epoch: 0 [134560/482500 (28%)]\tLoss: 1.252751\n",
      "Train Epoch: 0 [134720/482500 (28%)]\tLoss: 1.828997\n",
      "Train Epoch: 0 [134880/482500 (28%)]\tLoss: 2.590073\n",
      "Train Epoch: 0 [135040/482500 (28%)]\tLoss: 2.542633\n",
      "Train Epoch: 0 [135200/482500 (28%)]\tLoss: 2.299340\n",
      "Train Epoch: 0 [135360/482500 (28%)]\tLoss: 1.991364\n",
      "Train Epoch: 0 [135520/482500 (28%)]\tLoss: 2.702683\n",
      "Train Epoch: 0 [135680/482500 (28%)]\tLoss: 2.623795\n",
      "Train Epoch: 0 [135840/482500 (28%)]\tLoss: 1.911404\n",
      "Train Epoch: 0 [136000/482500 (28%)]\tLoss: 1.134722\n",
      "Train Epoch: 0 [136160/482500 (28%)]\tLoss: 3.218096\n",
      "Train Epoch: 0 [136320/482500 (28%)]\tLoss: 1.574510\n",
      "Train Epoch: 0 [136480/482500 (28%)]\tLoss: 1.988212\n",
      "Train Epoch: 0 [136640/482500 (28%)]\tLoss: 1.214157\n",
      "Train Epoch: 0 [136800/482500 (28%)]\tLoss: 2.012628\n",
      "Train Epoch: 0 [136960/482500 (28%)]\tLoss: 1.344471\n",
      "Train Epoch: 0 [137120/482500 (28%)]\tLoss: 2.861307\n",
      "Train Epoch: 0 [137280/482500 (28%)]\tLoss: 1.369513\n",
      "Train Epoch: 0 [137440/482500 (28%)]\tLoss: 2.508925\n",
      "Train Epoch: 0 [137600/482500 (29%)]\tLoss: 3.179013\n",
      "Train Epoch: 0 [137760/482500 (29%)]\tLoss: 1.328228\n",
      "Train Epoch: 0 [137920/482500 (29%)]\tLoss: 7.916881\n",
      "Train Epoch: 0 [138080/482500 (29%)]\tLoss: 5.711562\n",
      "Train Epoch: 0 [138240/482500 (29%)]\tLoss: 2.871386\n",
      "Train Epoch: 0 [138400/482500 (29%)]\tLoss: 6.561812\n",
      "Train Epoch: 0 [138560/482500 (29%)]\tLoss: 1.035919\n",
      "Train Epoch: 0 [138720/482500 (29%)]\tLoss: 1.342823\n",
      "Train Epoch: 0 [138880/482500 (29%)]\tLoss: 4.464162\n",
      "Train Epoch: 0 [139040/482500 (29%)]\tLoss: 6.374633\n",
      "Train Epoch: 0 [139200/482500 (29%)]\tLoss: 1.776652\n",
      "Train Epoch: 0 [139360/482500 (29%)]\tLoss: 3.320438\n",
      "Train Epoch: 0 [139520/482500 (29%)]\tLoss: 1.040805\n",
      "Train Epoch: 0 [139680/482500 (29%)]\tLoss: 1.273450\n",
      "Train Epoch: 0 [139840/482500 (29%)]\tLoss: 2.182301\n",
      "Train Epoch: 0 [140000/482500 (29%)]\tLoss: 3.445013\n",
      "Train Epoch: 0 [140160/482500 (29%)]\tLoss: 5.050490\n",
      "Train Epoch: 0 [140320/482500 (29%)]\tLoss: 2.012394\n",
      "Train Epoch: 0 [140480/482500 (29%)]\tLoss: 1.263184\n",
      "Train Epoch: 0 [140640/482500 (29%)]\tLoss: 1.051262\n",
      "Train Epoch: 0 [140800/482500 (29%)]\tLoss: 1.177652\n",
      "Train Epoch: 0 [140960/482500 (29%)]\tLoss: 1.478343\n",
      "Train Epoch: 0 [141120/482500 (29%)]\tLoss: 1.250618\n",
      "Train Epoch: 0 [141280/482500 (29%)]\tLoss: 1.610594\n",
      "Train Epoch: 0 [141440/482500 (29%)]\tLoss: 0.992640\n",
      "Train Epoch: 0 [141600/482500 (29%)]\tLoss: 1.490529\n",
      "Train Epoch: 0 [141760/482500 (29%)]\tLoss: 2.967347\n",
      "Train Epoch: 0 [141920/482500 (29%)]\tLoss: 1.107224\n",
      "Train Epoch: 0 [142080/482500 (29%)]\tLoss: 1.694353\n",
      "Train Epoch: 0 [142240/482500 (29%)]\tLoss: 1.237126\n",
      "Train Epoch: 0 [142400/482500 (30%)]\tLoss: 1.506284\n",
      "Train Epoch: 0 [142560/482500 (30%)]\tLoss: 1.535493\n",
      "Train Epoch: 0 [142720/482500 (30%)]\tLoss: 0.966186\n",
      "Train Epoch: 0 [142880/482500 (30%)]\tLoss: 3.157802\n",
      "Train Epoch: 0 [143040/482500 (30%)]\tLoss: 2.302893\n",
      "Train Epoch: 0 [143200/482500 (30%)]\tLoss: 1.732068\n",
      "Train Epoch: 0 [143360/482500 (30%)]\tLoss: 1.095171\n",
      "Train Epoch: 0 [143520/482500 (30%)]\tLoss: 1.404510\n",
      "Train Epoch: 0 [143680/482500 (30%)]\tLoss: 1.740796\n",
      "Train Epoch: 0 [143840/482500 (30%)]\tLoss: 1.190634\n",
      "Train Epoch: 0 [144000/482500 (30%)]\tLoss: 1.029704\n",
      "Train Epoch: 0 [144160/482500 (30%)]\tLoss: 1.090919\n",
      "Train Epoch: 0 [144320/482500 (30%)]\tLoss: 1.009786\n",
      "Train Epoch: 0 [144480/482500 (30%)]\tLoss: 1.374439\n",
      "Train Epoch: 0 [144640/482500 (30%)]\tLoss: 2.124178\n",
      "Train Epoch: 0 [144800/482500 (30%)]\tLoss: 1.760640\n",
      "Train Epoch: 0 [144960/482500 (30%)]\tLoss: 2.072653\n",
      "Train Epoch: 0 [145120/482500 (30%)]\tLoss: 1.043929\n",
      "Train Epoch: 0 [145280/482500 (30%)]\tLoss: 1.295485\n",
      "Train Epoch: 0 [145440/482500 (30%)]\tLoss: 0.913283\n",
      "Train Epoch: 0 [145600/482500 (30%)]\tLoss: 1.979908\n",
      "Train Epoch: 0 [145760/482500 (30%)]\tLoss: 2.031711\n",
      "Train Epoch: 0 [145920/482500 (30%)]\tLoss: 2.193502\n",
      "Train Epoch: 0 [146080/482500 (30%)]\tLoss: 1.773243\n",
      "Train Epoch: 0 [146240/482500 (30%)]\tLoss: 1.263000\n",
      "Train Epoch: 0 [146400/482500 (30%)]\tLoss: 2.033992\n",
      "Train Epoch: 0 [146560/482500 (30%)]\tLoss: 1.538655\n",
      "Train Epoch: 0 [146720/482500 (30%)]\tLoss: 1.479669\n",
      "Train Epoch: 0 [146880/482500 (30%)]\tLoss: 1.237519\n",
      "Train Epoch: 0 [147040/482500 (30%)]\tLoss: 1.012630\n",
      "Train Epoch: 0 [147200/482500 (31%)]\tLoss: 1.219922\n",
      "Train Epoch: 0 [147360/482500 (31%)]\tLoss: 1.180817\n",
      "Train Epoch: 0 [147520/482500 (31%)]\tLoss: 1.363533\n",
      "Train Epoch: 0 [147680/482500 (31%)]\tLoss: 1.955969\n",
      "Train Epoch: 0 [147840/482500 (31%)]\tLoss: 0.936610\n",
      "Train Epoch: 0 [148000/482500 (31%)]\tLoss: 1.171941\n",
      "Train Epoch: 0 [148160/482500 (31%)]\tLoss: 1.023990\n",
      "Train Epoch: 0 [148320/482500 (31%)]\tLoss: 3.722953\n",
      "Train Epoch: 0 [148480/482500 (31%)]\tLoss: 2.755899\n",
      "Train Epoch: 0 [148640/482500 (31%)]\tLoss: 2.377931\n",
      "Train Epoch: 0 [148800/482500 (31%)]\tLoss: 1.238637\n",
      "Train Epoch: 0 [148960/482500 (31%)]\tLoss: 0.802828\n",
      "Train Epoch: 0 [149120/482500 (31%)]\tLoss: 0.681780\n",
      "Train Epoch: 0 [149280/482500 (31%)]\tLoss: 1.072950\n",
      "Train Epoch: 0 [149440/482500 (31%)]\tLoss: 1.013973\n",
      "Train Epoch: 0 [149600/482500 (31%)]\tLoss: 1.181222\n",
      "Train Epoch: 0 [149760/482500 (31%)]\tLoss: 0.952854\n",
      "Train Epoch: 0 [149920/482500 (31%)]\tLoss: 1.303893\n",
      "Train Epoch: 0 [150080/482500 (31%)]\tLoss: 1.987864\n",
      "Train Epoch: 0 [150240/482500 (31%)]\tLoss: 1.045430\n",
      "Train Epoch: 0 [150400/482500 (31%)]\tLoss: 0.999889\n",
      "Train Epoch: 0 [150560/482500 (31%)]\tLoss: 0.815058\n",
      "Train Epoch: 0 [150720/482500 (31%)]\tLoss: 0.922914\n",
      "Train Epoch: 0 [150880/482500 (31%)]\tLoss: 2.097001\n",
      "Train Epoch: 0 [151040/482500 (31%)]\tLoss: 1.341678\n",
      "Train Epoch: 0 [151200/482500 (31%)]\tLoss: 1.912368\n",
      "Train Epoch: 0 [151360/482500 (31%)]\tLoss: 1.210762\n",
      "Train Epoch: 0 [151520/482500 (31%)]\tLoss: 1.969939\n",
      "Train Epoch: 0 [151680/482500 (31%)]\tLoss: 5.932012\n",
      "Train Epoch: 0 [151840/482500 (31%)]\tLoss: 3.983413\n",
      "Train Epoch: 0 [152000/482500 (32%)]\tLoss: 4.079737\n",
      "Train Epoch: 0 [152160/482500 (32%)]\tLoss: 17.460794\n",
      "Train Epoch: 0 [152320/482500 (32%)]\tLoss: 4.075300\n",
      "Train Epoch: 0 [152480/482500 (32%)]\tLoss: 2.078346\n",
      "Train Epoch: 0 [152640/482500 (32%)]\tLoss: 4.041534\n",
      "Train Epoch: 0 [152800/482500 (32%)]\tLoss: 2.789794\n",
      "Train Epoch: 0 [152960/482500 (32%)]\tLoss: 1.860058\n",
      "Train Epoch: 0 [153120/482500 (32%)]\tLoss: 1.685179\n",
      "Train Epoch: 0 [153280/482500 (32%)]\tLoss: 1.063885\n",
      "Train Epoch: 0 [153440/482500 (32%)]\tLoss: 1.288142\n",
      "Train Epoch: 0 [153600/482500 (32%)]\tLoss: 0.959417\n",
      "Train Epoch: 0 [153760/482500 (32%)]\tLoss: 1.691931\n",
      "Train Epoch: 0 [153920/482500 (32%)]\tLoss: 392.812469\n",
      "Train Epoch: 0 [154080/482500 (32%)]\tLoss: 91.290825\n",
      "Train Epoch: 0 [154240/482500 (32%)]\tLoss: 5.497117\n",
      "Train Epoch: 0 [154400/482500 (32%)]\tLoss: 6.480824\n",
      "Train Epoch: 0 [154560/482500 (32%)]\tLoss: 5.584124\n",
      "Train Epoch: 0 [154720/482500 (32%)]\tLoss: 2.914098\n",
      "Train Epoch: 0 [154880/482500 (32%)]\tLoss: 4.970460\n",
      "Train Epoch: 0 [155040/482500 (32%)]\tLoss: 6.582445\n",
      "Train Epoch: 0 [155200/482500 (32%)]\tLoss: 2.847449\n",
      "Train Epoch: 0 [155360/482500 (32%)]\tLoss: 2.651013\n",
      "Train Epoch: 0 [155520/482500 (32%)]\tLoss: 3.901109\n",
      "Train Epoch: 0 [155680/482500 (32%)]\tLoss: 2.858654\n",
      "Train Epoch: 0 [155840/482500 (32%)]\tLoss: 2.813040\n",
      "Train Epoch: 0 [156000/482500 (32%)]\tLoss: 2.782001\n",
      "Train Epoch: 0 [156160/482500 (32%)]\tLoss: 3.438225\n",
      "Train Epoch: 0 [156320/482500 (32%)]\tLoss: 3.385196\n",
      "Train Epoch: 0 [156480/482500 (32%)]\tLoss: 2.886564\n",
      "Train Epoch: 0 [156640/482500 (32%)]\tLoss: 5.500288\n",
      "Train Epoch: 0 [156800/482500 (32%)]\tLoss: 3.856234\n",
      "Train Epoch: 0 [156960/482500 (33%)]\tLoss: 3.347417\n",
      "Train Epoch: 0 [157120/482500 (33%)]\tLoss: 2.723858\n",
      "Train Epoch: 0 [157280/482500 (33%)]\tLoss: 2.224459\n",
      "Train Epoch: 0 [157440/482500 (33%)]\tLoss: 2.356317\n",
      "Train Epoch: 0 [157600/482500 (33%)]\tLoss: 3.375486\n",
      "Train Epoch: 0 [157760/482500 (33%)]\tLoss: 1.951639\n",
      "Train Epoch: 0 [157920/482500 (33%)]\tLoss: 2.376258\n",
      "Train Epoch: 0 [158080/482500 (33%)]\tLoss: 5.780271\n",
      "Train Epoch: 0 [158240/482500 (33%)]\tLoss: 4.274674\n",
      "Train Epoch: 0 [158400/482500 (33%)]\tLoss: 3.520446\n",
      "Train Epoch: 0 [158560/482500 (33%)]\tLoss: 2.640023\n",
      "Train Epoch: 0 [158720/482500 (33%)]\tLoss: 2.733314\n",
      "Train Epoch: 0 [158880/482500 (33%)]\tLoss: 4.864905\n",
      "Train Epoch: 0 [159040/482500 (33%)]\tLoss: 2.649436\n",
      "Train Epoch: 0 [159200/482500 (33%)]\tLoss: 9.715993\n",
      "Train Epoch: 0 [159360/482500 (33%)]\tLoss: 10.295307\n",
      "Train Epoch: 0 [159520/482500 (33%)]\tLoss: 2.444525\n",
      "Train Epoch: 0 [159680/482500 (33%)]\tLoss: 3.106738\n",
      "Train Epoch: 0 [159840/482500 (33%)]\tLoss: 3.345428\n",
      "Train Epoch: 0 [160000/482500 (33%)]\tLoss: 2.809544\n",
      "Train Epoch: 0 [160160/482500 (33%)]\tLoss: 1.966559\n",
      "Train Epoch: 0 [160320/482500 (33%)]\tLoss: 2.282300\n",
      "Train Epoch: 0 [160480/482500 (33%)]\tLoss: 4.742765\n",
      "Train Epoch: 0 [160640/482500 (33%)]\tLoss: 2.634880\n",
      "Train Epoch: 0 [160800/482500 (33%)]\tLoss: 2.016516\n",
      "Train Epoch: 0 [160960/482500 (33%)]\tLoss: 2.784719\n",
      "Train Epoch: 0 [161120/482500 (33%)]\tLoss: 2.613129\n",
      "Train Epoch: 0 [161280/482500 (33%)]\tLoss: 1.951598\n",
      "Train Epoch: 0 [161440/482500 (33%)]\tLoss: 2.212731\n",
      "Train Epoch: 0 [161600/482500 (33%)]\tLoss: 2.945219\n",
      "Train Epoch: 0 [161760/482500 (34%)]\tLoss: 2.300050\n",
      "Train Epoch: 0 [161920/482500 (34%)]\tLoss: 2.451849\n",
      "Train Epoch: 0 [162080/482500 (34%)]\tLoss: 2.219448\n",
      "Train Epoch: 0 [162240/482500 (34%)]\tLoss: 2.479010\n",
      "Train Epoch: 0 [162400/482500 (34%)]\tLoss: 1.558466\n",
      "Train Epoch: 0 [162560/482500 (34%)]\tLoss: 2.547336\n",
      "Train Epoch: 0 [162720/482500 (34%)]\tLoss: 2.332409\n",
      "Train Epoch: 0 [162880/482500 (34%)]\tLoss: 2.537828\n",
      "Train Epoch: 0 [163040/482500 (34%)]\tLoss: 2.268169\n",
      "Train Epoch: 0 [163200/482500 (34%)]\tLoss: 1.881272\n",
      "Train Epoch: 0 [163360/482500 (34%)]\tLoss: 2.433628\n",
      "Train Epoch: 0 [163520/482500 (34%)]\tLoss: 2.155289\n",
      "Train Epoch: 0 [163680/482500 (34%)]\tLoss: 1.846450\n",
      "Train Epoch: 0 [163840/482500 (34%)]\tLoss: 1.722320\n",
      "Train Epoch: 0 [164000/482500 (34%)]\tLoss: 1.976000\n",
      "Train Epoch: 0 [164160/482500 (34%)]\tLoss: 45.171196\n",
      "Train Epoch: 0 [164320/482500 (34%)]\tLoss: 8.911339\n",
      "Train Epoch: 0 [164480/482500 (34%)]\tLoss: 4.071431\n",
      "Train Epoch: 0 [164640/482500 (34%)]\tLoss: 2.704108\n",
      "Train Epoch: 0 [164800/482500 (34%)]\tLoss: 2.333530\n",
      "Train Epoch: 0 [164960/482500 (34%)]\tLoss: 3.941463\n",
      "Train Epoch: 0 [165120/482500 (34%)]\tLoss: 3.961233\n",
      "Train Epoch: 0 [165280/482500 (34%)]\tLoss: 1.726910\n",
      "Train Epoch: 0 [165440/482500 (34%)]\tLoss: 2.395181\n",
      "Train Epoch: 0 [165600/482500 (34%)]\tLoss: 1.473888\n",
      "Train Epoch: 0 [165760/482500 (34%)]\tLoss: 2.090459\n",
      "Train Epoch: 0 [165920/482500 (34%)]\tLoss: 3.171237\n",
      "Train Epoch: 0 [166080/482500 (34%)]\tLoss: 1.842485\n",
      "Train Epoch: 0 [166240/482500 (34%)]\tLoss: 6.403534\n",
      "Train Epoch: 0 [166400/482500 (34%)]\tLoss: 1.437052\n",
      "Train Epoch: 0 [166560/482500 (35%)]\tLoss: 1.719459\n",
      "Train Epoch: 0 [166720/482500 (35%)]\tLoss: 1.316435\n",
      "Train Epoch: 0 [166880/482500 (35%)]\tLoss: 2.182441\n",
      "Train Epoch: 0 [167040/482500 (35%)]\tLoss: 2.784095\n",
      "Train Epoch: 0 [167200/482500 (35%)]\tLoss: 1.722471\n",
      "Train Epoch: 0 [167360/482500 (35%)]\tLoss: 1.827025\n",
      "Train Epoch: 0 [167520/482500 (35%)]\tLoss: 2.914005\n",
      "Train Epoch: 0 [167680/482500 (35%)]\tLoss: 2.197100\n",
      "Train Epoch: 0 [167840/482500 (35%)]\tLoss: 1.498209\n",
      "Train Epoch: 0 [168000/482500 (35%)]\tLoss: 1.658551\n",
      "Train Epoch: 0 [168160/482500 (35%)]\tLoss: 2.111233\n",
      "Train Epoch: 0 [168320/482500 (35%)]\tLoss: 1.338302\n",
      "Train Epoch: 0 [168480/482500 (35%)]\tLoss: 1.746551\n",
      "Train Epoch: 0 [168640/482500 (35%)]\tLoss: 8.737098\n",
      "Train Epoch: 0 [168800/482500 (35%)]\tLoss: 5.476065\n",
      "Train Epoch: 0 [168960/482500 (35%)]\tLoss: 19.307537\n",
      "Train Epoch: 0 [169120/482500 (35%)]\tLoss: 3.210872\n",
      "Train Epoch: 0 [169280/482500 (35%)]\tLoss: 18.945646\n",
      "Train Epoch: 0 [169440/482500 (35%)]\tLoss: 9.172148\n",
      "Train Epoch: 0 [169600/482500 (35%)]\tLoss: 1.922450\n",
      "Train Epoch: 0 [169760/482500 (35%)]\tLoss: 1.710870\n",
      "Train Epoch: 0 [169920/482500 (35%)]\tLoss: 1.926665\n",
      "Train Epoch: 0 [170080/482500 (35%)]\tLoss: 2.498461\n",
      "Train Epoch: 0 [170240/482500 (35%)]\tLoss: 14.451575\n",
      "Train Epoch: 0 [170400/482500 (35%)]\tLoss: 1.868661\n",
      "Train Epoch: 0 [170560/482500 (35%)]\tLoss: 51.348370\n",
      "Train Epoch: 0 [170720/482500 (35%)]\tLoss: 8.006419\n",
      "Train Epoch: 0 [170880/482500 (35%)]\tLoss: 2.110240\n",
      "Train Epoch: 0 [171040/482500 (35%)]\tLoss: 3.878979\n",
      "Train Epoch: 0 [171200/482500 (35%)]\tLoss: 1.770584\n",
      "Train Epoch: 0 [171360/482500 (36%)]\tLoss: 1.704851\n",
      "Train Epoch: 0 [171520/482500 (36%)]\tLoss: 1.497964\n",
      "Train Epoch: 0 [171680/482500 (36%)]\tLoss: 1.700088\n",
      "Train Epoch: 0 [171840/482500 (36%)]\tLoss: 1.947961\n",
      "Train Epoch: 0 [172000/482500 (36%)]\tLoss: 1.442923\n",
      "Train Epoch: 0 [172160/482500 (36%)]\tLoss: 2.040639\n",
      "Train Epoch: 0 [172320/482500 (36%)]\tLoss: 1.432637\n",
      "Train Epoch: 0 [172480/482500 (36%)]\tLoss: 2.330853\n",
      "Train Epoch: 0 [172640/482500 (36%)]\tLoss: 1.388587\n",
      "Train Epoch: 0 [172800/482500 (36%)]\tLoss: 1.964671\n",
      "Train Epoch: 0 [172960/482500 (36%)]\tLoss: 1.750067\n",
      "Train Epoch: 0 [173120/482500 (36%)]\tLoss: 1.836759\n",
      "Train Epoch: 0 [173280/482500 (36%)]\tLoss: 1.513850\n",
      "Train Epoch: 0 [173440/482500 (36%)]\tLoss: 1.962425\n",
      "Train Epoch: 0 [173600/482500 (36%)]\tLoss: 1.272069\n",
      "Train Epoch: 0 [173760/482500 (36%)]\tLoss: 1.796481\n",
      "Train Epoch: 0 [173920/482500 (36%)]\tLoss: 1.274619\n",
      "Train Epoch: 0 [174080/482500 (36%)]\tLoss: 86.968307\n",
      "Train Epoch: 0 [174240/482500 (36%)]\tLoss: 12.569036\n",
      "Train Epoch: 0 [174400/482500 (36%)]\tLoss: 5.870572\n",
      "Train Epoch: 0 [174560/482500 (36%)]\tLoss: 4.941279\n",
      "Train Epoch: 0 [174720/482500 (36%)]\tLoss: 3.448933\n",
      "Train Epoch: 0 [174880/482500 (36%)]\tLoss: 3.378635\n",
      "Train Epoch: 0 [175040/482500 (36%)]\tLoss: 2.596029\n",
      "Train Epoch: 0 [175200/482500 (36%)]\tLoss: 1.791227\n",
      "Train Epoch: 0 [175360/482500 (36%)]\tLoss: 3.961384\n",
      "Train Epoch: 0 [175520/482500 (36%)]\tLoss: 6.081299\n",
      "Train Epoch: 0 [175680/482500 (36%)]\tLoss: 2.013444\n",
      "Train Epoch: 0 [175840/482500 (36%)]\tLoss: 2.448595\n",
      "Train Epoch: 0 [176000/482500 (36%)]\tLoss: 2.705320\n",
      "Train Epoch: 0 [176160/482500 (37%)]\tLoss: 3.571496\n",
      "Train Epoch: 0 [176320/482500 (37%)]\tLoss: 3.010648\n",
      "Train Epoch: 0 [176480/482500 (37%)]\tLoss: 5.516541\n",
      "Train Epoch: 0 [176640/482500 (37%)]\tLoss: 2.759386\n",
      "Train Epoch: 0 [176800/482500 (37%)]\tLoss: 2.792027\n",
      "Train Epoch: 0 [176960/482500 (37%)]\tLoss: 2.646923\n",
      "Train Epoch: 0 [177120/482500 (37%)]\tLoss: 2.381404\n",
      "Train Epoch: 0 [177280/482500 (37%)]\tLoss: 2.166113\n",
      "Train Epoch: 0 [177440/482500 (37%)]\tLoss: 3.120783\n",
      "Train Epoch: 0 [177600/482500 (37%)]\tLoss: 2.314567\n",
      "Train Epoch: 0 [177760/482500 (37%)]\tLoss: 2.410113\n",
      "Train Epoch: 0 [177920/482500 (37%)]\tLoss: 1.843344\n",
      "Train Epoch: 0 [178080/482500 (37%)]\tLoss: 2.491741\n",
      "Train Epoch: 0 [178240/482500 (37%)]\tLoss: 2.114090\n",
      "Train Epoch: 0 [178400/482500 (37%)]\tLoss: 2.083691\n",
      "Train Epoch: 0 [178560/482500 (37%)]\tLoss: 1.834367\n",
      "Train Epoch: 0 [178720/482500 (37%)]\tLoss: 1.467904\n",
      "Train Epoch: 0 [178880/482500 (37%)]\tLoss: 1.502714\n",
      "Train Epoch: 0 [179040/482500 (37%)]\tLoss: 1.869275\n",
      "Train Epoch: 0 [179200/482500 (37%)]\tLoss: 2.417686\n",
      "Train Epoch: 0 [179360/482500 (37%)]\tLoss: 2.668968\n",
      "Train Epoch: 0 [179520/482500 (37%)]\tLoss: 1.610142\n",
      "Train Epoch: 0 [179680/482500 (37%)]\tLoss: 1.984001\n",
      "Train Epoch: 0 [179840/482500 (37%)]\tLoss: 2.245595\n",
      "Train Epoch: 0 [180000/482500 (37%)]\tLoss: 2.121760\n",
      "Train Epoch: 0 [180160/482500 (37%)]\tLoss: 2.028547\n",
      "Train Epoch: 0 [180320/482500 (37%)]\tLoss: 1.953058\n",
      "Train Epoch: 0 [180480/482500 (37%)]\tLoss: 2.156601\n",
      "Train Epoch: 0 [180640/482500 (37%)]\tLoss: 2.085464\n",
      "Train Epoch: 0 [180800/482500 (37%)]\tLoss: 1.643076\n",
      "Train Epoch: 0 [180960/482500 (38%)]\tLoss: 1.589043\n",
      "Train Epoch: 0 [181120/482500 (38%)]\tLoss: 2.536908\n",
      "Train Epoch: 0 [181280/482500 (38%)]\tLoss: 1.632615\n",
      "Train Epoch: 0 [181440/482500 (38%)]\tLoss: 2.040960\n",
      "Train Epoch: 0 [181600/482500 (38%)]\tLoss: 2.252352\n",
      "Train Epoch: 0 [181760/482500 (38%)]\tLoss: 2.475294\n",
      "Train Epoch: 0 [181920/482500 (38%)]\tLoss: 1.722909\n",
      "Train Epoch: 0 [182080/482500 (38%)]\tLoss: 1.830954\n",
      "Train Epoch: 0 [182240/482500 (38%)]\tLoss: 2.689901\n",
      "Train Epoch: 0 [182400/482500 (38%)]\tLoss: 2.162575\n",
      "Train Epoch: 0 [182560/482500 (38%)]\tLoss: 1.567053\n",
      "Train Epoch: 0 [182720/482500 (38%)]\tLoss: 1.786068\n",
      "Train Epoch: 0 [182880/482500 (38%)]\tLoss: 1.739112\n",
      "Train Epoch: 0 [183040/482500 (38%)]\tLoss: 1.697840\n",
      "Train Epoch: 0 [183200/482500 (38%)]\tLoss: 2.193456\n",
      "Train Epoch: 0 [183360/482500 (38%)]\tLoss: 2.112399\n",
      "Train Epoch: 0 [183520/482500 (38%)]\tLoss: 1.114179\n",
      "Train Epoch: 0 [183680/482500 (38%)]\tLoss: 2.359261\n",
      "Train Epoch: 0 [183840/482500 (38%)]\tLoss: 1.603909\n",
      "Train Epoch: 0 [184000/482500 (38%)]\tLoss: 1.549701\n",
      "Train Epoch: 0 [184160/482500 (38%)]\tLoss: 1.512702\n",
      "Train Epoch: 0 [184320/482500 (38%)]\tLoss: 3.461458\n",
      "Train Epoch: 0 [184480/482500 (38%)]\tLoss: 1.862325\n",
      "Train Epoch: 0 [184640/482500 (38%)]\tLoss: 2.038532\n",
      "Train Epoch: 0 [184800/482500 (38%)]\tLoss: 3.747827\n",
      "Train Epoch: 0 [184960/482500 (38%)]\tLoss: 1.897245\n",
      "Train Epoch: 0 [185120/482500 (38%)]\tLoss: 1.400260\n",
      "Train Epoch: 0 [185280/482500 (38%)]\tLoss: 1.611952\n",
      "Train Epoch: 0 [185440/482500 (38%)]\tLoss: 97.223999\n",
      "Train Epoch: 0 [185600/482500 (38%)]\tLoss: 16.871664\n",
      "Train Epoch: 0 [185760/482500 (38%)]\tLoss: 5.288236\n",
      "Train Epoch: 0 [185920/482500 (39%)]\tLoss: 87.663979\n",
      "Train Epoch: 0 [186080/482500 (39%)]\tLoss: 17.205194\n",
      "Train Epoch: 0 [186240/482500 (39%)]\tLoss: 17.653004\n",
      "Train Epoch: 0 [186400/482500 (39%)]\tLoss: 12.568724\n",
      "Train Epoch: 0 [186560/482500 (39%)]\tLoss: 2.063869\n",
      "Train Epoch: 0 [186720/482500 (39%)]\tLoss: 10.773721\n",
      "Train Epoch: 0 [186880/482500 (39%)]\tLoss: 3.956488\n",
      "Train Epoch: 0 [187040/482500 (39%)]\tLoss: 2.872378\n",
      "Train Epoch: 0 [187200/482500 (39%)]\tLoss: 1.793709\n",
      "Train Epoch: 0 [187360/482500 (39%)]\tLoss: 5.668443\n",
      "Train Epoch: 0 [187520/482500 (39%)]\tLoss: 2.682178\n",
      "Train Epoch: 0 [187680/482500 (39%)]\tLoss: 44.142094\n",
      "Train Epoch: 0 [187840/482500 (39%)]\tLoss: 22.320486\n",
      "Train Epoch: 0 [188000/482500 (39%)]\tLoss: 4.277773\n",
      "Train Epoch: 0 [188160/482500 (39%)]\tLoss: 2.623615\n",
      "Train Epoch: 0 [188320/482500 (39%)]\tLoss: 2.651719\n",
      "Train Epoch: 0 [188480/482500 (39%)]\tLoss: 6.951404\n",
      "Train Epoch: 0 [188640/482500 (39%)]\tLoss: 4.264167\n",
      "Train Epoch: 0 [188800/482500 (39%)]\tLoss: 3.251982\n",
      "Train Epoch: 0 [188960/482500 (39%)]\tLoss: 2.188503\n",
      "Train Epoch: 0 [189120/482500 (39%)]\tLoss: 2.586029\n",
      "Train Epoch: 0 [189280/482500 (39%)]\tLoss: 2.129880\n",
      "Train Epoch: 0 [189440/482500 (39%)]\tLoss: 1.264273\n",
      "Train Epoch: 0 [189600/482500 (39%)]\tLoss: 2.828229\n",
      "Train Epoch: 0 [189760/482500 (39%)]\tLoss: 2.965740\n",
      "Train Epoch: 0 [189920/482500 (39%)]\tLoss: 2.130603\n",
      "Train Epoch: 0 [190080/482500 (39%)]\tLoss: 2.073375\n",
      "Train Epoch: 0 [190240/482500 (39%)]\tLoss: 2.341844\n",
      "Train Epoch: 0 [190400/482500 (39%)]\tLoss: 2.435556\n",
      "Train Epoch: 0 [190560/482500 (39%)]\tLoss: 1.601147\n",
      "Train Epoch: 0 [190720/482500 (40%)]\tLoss: 1.982269\n",
      "Train Epoch: 0 [190880/482500 (40%)]\tLoss: 1.502366\n",
      "Train Epoch: 0 [191040/482500 (40%)]\tLoss: 1.884509\n",
      "Train Epoch: 0 [191200/482500 (40%)]\tLoss: 2.639393\n",
      "Train Epoch: 0 [191360/482500 (40%)]\tLoss: 1.478670\n",
      "Train Epoch: 0 [191520/482500 (40%)]\tLoss: 3.275213\n",
      "Train Epoch: 0 [191680/482500 (40%)]\tLoss: 3.164467\n",
      "Train Epoch: 0 [191840/482500 (40%)]\tLoss: 1.985725\n",
      "Train Epoch: 0 [192000/482500 (40%)]\tLoss: 1.049257\n",
      "Train Epoch: 0 [192160/482500 (40%)]\tLoss: 1.573026\n",
      "Train Epoch: 0 [192320/482500 (40%)]\tLoss: 1.516388\n",
      "Train Epoch: 0 [192480/482500 (40%)]\tLoss: 2.050601\n",
      "Train Epoch: 0 [192640/482500 (40%)]\tLoss: 2.045976\n",
      "Train Epoch: 0 [192800/482500 (40%)]\tLoss: 1.471534\n",
      "Train Epoch: 0 [192960/482500 (40%)]\tLoss: 1.462033\n",
      "Train Epoch: 0 [193120/482500 (40%)]\tLoss: 1.243874\n",
      "Train Epoch: 0 [193280/482500 (40%)]\tLoss: 2.977578\n",
      "Train Epoch: 0 [193440/482500 (40%)]\tLoss: 1.942379\n",
      "Train Epoch: 0 [193600/482500 (40%)]\tLoss: 2.080106\n",
      "Train Epoch: 0 [193760/482500 (40%)]\tLoss: 1.206204\n",
      "Train Epoch: 0 [193920/482500 (40%)]\tLoss: 0.995954\n",
      "Train Epoch: 0 [194080/482500 (40%)]\tLoss: 0.985072\n",
      "Train Epoch: 0 [194240/482500 (40%)]\tLoss: 1.957039\n",
      "Train Epoch: 0 [194400/482500 (40%)]\tLoss: 1.475419\n",
      "Train Epoch: 0 [194560/482500 (40%)]\tLoss: 1.824906\n",
      "Train Epoch: 0 [194720/482500 (40%)]\tLoss: 2.584408\n",
      "Train Epoch: 0 [194880/482500 (40%)]\tLoss: 1.255630\n",
      "Train Epoch: 0 [195040/482500 (40%)]\tLoss: 3.046454\n",
      "Train Epoch: 0 [195200/482500 (40%)]\tLoss: 1.309440\n",
      "Train Epoch: 0 [195360/482500 (40%)]\tLoss: 1.757390\n",
      "Train Epoch: 0 [195520/482500 (41%)]\tLoss: 1.432431\n",
      "Train Epoch: 0 [195680/482500 (41%)]\tLoss: 0.933410\n",
      "Train Epoch: 0 [195840/482500 (41%)]\tLoss: 2.164797\n",
      "Train Epoch: 0 [196000/482500 (41%)]\tLoss: 0.880543\n",
      "Train Epoch: 0 [196160/482500 (41%)]\tLoss: 1.783849\n",
      "Train Epoch: 0 [196320/482500 (41%)]\tLoss: 1.226222\n",
      "Train Epoch: 0 [196480/482500 (41%)]\tLoss: 1.226458\n",
      "Train Epoch: 0 [196640/482500 (41%)]\tLoss: 2.033335\n",
      "Train Epoch: 0 [196800/482500 (41%)]\tLoss: 2.254710\n",
      "Train Epoch: 0 [196960/482500 (41%)]\tLoss: 1.137660\n",
      "Train Epoch: 0 [197120/482500 (41%)]\tLoss: 1.525035\n",
      "Train Epoch: 0 [197280/482500 (41%)]\tLoss: 1.394985\n",
      "Train Epoch: 0 [197440/482500 (41%)]\tLoss: 1.404263\n",
      "Train Epoch: 0 [197600/482500 (41%)]\tLoss: 1.398379\n",
      "Train Epoch: 0 [197760/482500 (41%)]\tLoss: 0.696807\n",
      "Train Epoch: 0 [197920/482500 (41%)]\tLoss: 1.031624\n",
      "Train Epoch: 0 [198080/482500 (41%)]\tLoss: 1.246310\n",
      "Train Epoch: 0 [198240/482500 (41%)]\tLoss: 1.156605\n",
      "Train Epoch: 0 [198400/482500 (41%)]\tLoss: 3.218906\n",
      "Train Epoch: 0 [198560/482500 (41%)]\tLoss: 1.568065\n",
      "Train Epoch: 0 [198720/482500 (41%)]\tLoss: 3.122949\n",
      "Train Epoch: 0 [198880/482500 (41%)]\tLoss: 1.574208\n",
      "Train Epoch: 0 [199040/482500 (41%)]\tLoss: 1.671659\n",
      "Train Epoch: 0 [199200/482500 (41%)]\tLoss: 1.693632\n",
      "Train Epoch: 0 [199360/482500 (41%)]\tLoss: 2.288155\n",
      "Train Epoch: 0 [199520/482500 (41%)]\tLoss: 2.357617\n",
      "Train Epoch: 0 [199680/482500 (41%)]\tLoss: 1.315533\n",
      "Train Epoch: 0 [199840/482500 (41%)]\tLoss: 1.650297\n",
      "Train Epoch: 0 [200000/482500 (41%)]\tLoss: 1.646671\n",
      "Train Epoch: 0 [200160/482500 (41%)]\tLoss: 2.043378\n",
      "Train Epoch: 0 [200320/482500 (42%)]\tLoss: 1.288676\n",
      "Train Epoch: 0 [200480/482500 (42%)]\tLoss: 2.120162\n",
      "Train Epoch: 0 [200640/482500 (42%)]\tLoss: 1.171490\n",
      "Train Epoch: 0 [200800/482500 (42%)]\tLoss: 1.334270\n",
      "Train Epoch: 0 [200960/482500 (42%)]\tLoss: 1.787569\n",
      "Train Epoch: 0 [201120/482500 (42%)]\tLoss: 1.712556\n",
      "Train Epoch: 0 [201280/482500 (42%)]\tLoss: 1.082788\n",
      "Train Epoch: 0 [201440/482500 (42%)]\tLoss: 1.368858\n",
      "Train Epoch: 0 [201600/482500 (42%)]\tLoss: 2.750244\n",
      "Train Epoch: 0 [201760/482500 (42%)]\tLoss: 1.491770\n",
      "Train Epoch: 0 [201920/482500 (42%)]\tLoss: 1.370172\n",
      "Train Epoch: 0 [202080/482500 (42%)]\tLoss: 2.404712\n",
      "Train Epoch: 0 [202240/482500 (42%)]\tLoss: 1.765089\n",
      "Train Epoch: 0 [202400/482500 (42%)]\tLoss: 1.398679\n",
      "Train Epoch: 0 [202560/482500 (42%)]\tLoss: 0.967036\n",
      "Train Epoch: 0 [202720/482500 (42%)]\tLoss: 1.388775\n",
      "Train Epoch: 0 [202880/482500 (42%)]\tLoss: 1.454659\n",
      "Train Epoch: 0 [203040/482500 (42%)]\tLoss: 1.209568\n",
      "Train Epoch: 0 [203200/482500 (42%)]\tLoss: 1.946108\n",
      "Train Epoch: 0 [203360/482500 (42%)]\tLoss: 0.979975\n",
      "Train Epoch: 0 [203520/482500 (42%)]\tLoss: 1.513278\n",
      "Train Epoch: 0 [203680/482500 (42%)]\tLoss: 1.251324\n",
      "Train Epoch: 0 [203840/482500 (42%)]\tLoss: 5.461956\n",
      "Train Epoch: 0 [204000/482500 (42%)]\tLoss: 1.232624\n",
      "Train Epoch: 0 [204160/482500 (42%)]\tLoss: 1.183003\n",
      "Train Epoch: 0 [204320/482500 (42%)]\tLoss: 29.158741\n",
      "Train Epoch: 0 [204480/482500 (42%)]\tLoss: 1.156099\n",
      "Train Epoch: 0 [204640/482500 (42%)]\tLoss: 1.155631\n",
      "Train Epoch: 0 [204800/482500 (42%)]\tLoss: 1.180347\n",
      "Train Epoch: 0 [204960/482500 (42%)]\tLoss: 1.066885\n",
      "Train Epoch: 0 [205120/482500 (43%)]\tLoss: 0.986901\n",
      "Train Epoch: 0 [205280/482500 (43%)]\tLoss: 0.890911\n",
      "Train Epoch: 0 [205440/482500 (43%)]\tLoss: 1.320615\n",
      "Train Epoch: 0 [205600/482500 (43%)]\tLoss: 1.191031\n",
      "Train Epoch: 0 [205760/482500 (43%)]\tLoss: 0.895942\n",
      "Train Epoch: 0 [205920/482500 (43%)]\tLoss: 1.054968\n",
      "Train Epoch: 0 [206080/482500 (43%)]\tLoss: 1.136727\n",
      "Train Epoch: 0 [206240/482500 (43%)]\tLoss: 1.462133\n",
      "Train Epoch: 0 [206400/482500 (43%)]\tLoss: 1.003277\n",
      "Train Epoch: 0 [206560/482500 (43%)]\tLoss: 1.165728\n",
      "Train Epoch: 0 [206720/482500 (43%)]\tLoss: 0.972807\n",
      "Train Epoch: 0 [206880/482500 (43%)]\tLoss: 1.086804\n",
      "Train Epoch: 0 [207040/482500 (43%)]\tLoss: 1.943850\n",
      "Train Epoch: 0 [207200/482500 (43%)]\tLoss: 1.262434\n",
      "Train Epoch: 0 [207360/482500 (43%)]\tLoss: 1.440281\n",
      "Train Epoch: 0 [207520/482500 (43%)]\tLoss: 1.766848\n",
      "Train Epoch: 0 [207680/482500 (43%)]\tLoss: 1.017547\n",
      "Train Epoch: 0 [207840/482500 (43%)]\tLoss: 1.370890\n",
      "Train Epoch: 0 [208000/482500 (43%)]\tLoss: 1.062878\n",
      "Train Epoch: 0 [208160/482500 (43%)]\tLoss: 1.515682\n",
      "Train Epoch: 0 [208320/482500 (43%)]\tLoss: 1.117490\n",
      "Train Epoch: 0 [208480/482500 (43%)]\tLoss: 1.290459\n",
      "Train Epoch: 0 [208640/482500 (43%)]\tLoss: 0.900748\n",
      "Train Epoch: 0 [208800/482500 (43%)]\tLoss: 1.056877\n",
      "Train Epoch: 0 [208960/482500 (43%)]\tLoss: 2.500014\n",
      "Train Epoch: 0 [209120/482500 (43%)]\tLoss: 1.308791\n",
      "Train Epoch: 0 [209280/482500 (43%)]\tLoss: 1.280313\n",
      "Train Epoch: 0 [209440/482500 (43%)]\tLoss: 1.293719\n",
      "Train Epoch: 0 [209600/482500 (43%)]\tLoss: 1.051189\n",
      "Train Epoch: 0 [209760/482500 (43%)]\tLoss: 0.870549\n",
      "Train Epoch: 0 [209920/482500 (44%)]\tLoss: 1.538571\n",
      "Train Epoch: 0 [210080/482500 (44%)]\tLoss: 1.386293\n",
      "Train Epoch: 0 [210240/482500 (44%)]\tLoss: 1.370183\n",
      "Train Epoch: 0 [210400/482500 (44%)]\tLoss: 0.601848\n",
      "Train Epoch: 0 [210560/482500 (44%)]\tLoss: 0.902628\n",
      "Train Epoch: 0 [210720/482500 (44%)]\tLoss: 0.659513\n",
      "Train Epoch: 0 [210880/482500 (44%)]\tLoss: 0.877463\n",
      "Train Epoch: 0 [211040/482500 (44%)]\tLoss: 1.139439\n",
      "Train Epoch: 0 [211200/482500 (44%)]\tLoss: 16.275469\n",
      "Train Epoch: 0 [211360/482500 (44%)]\tLoss: 0.754033\n",
      "Train Epoch: 0 [211520/482500 (44%)]\tLoss: 1.335964\n",
      "Train Epoch: 0 [211680/482500 (44%)]\tLoss: 1.847228\n",
      "Train Epoch: 0 [211840/482500 (44%)]\tLoss: 0.904814\n",
      "Train Epoch: 0 [212000/482500 (44%)]\tLoss: 1.052179\n",
      "Train Epoch: 0 [212160/482500 (44%)]\tLoss: 2.155693\n",
      "Train Epoch: 0 [212320/482500 (44%)]\tLoss: 1.079513\n",
      "Train Epoch: 0 [212480/482500 (44%)]\tLoss: 1.286804\n",
      "Train Epoch: 0 [212640/482500 (44%)]\tLoss: 2.347684\n",
      "Train Epoch: 0 [212800/482500 (44%)]\tLoss: 0.869632\n",
      "Train Epoch: 0 [212960/482500 (44%)]\tLoss: 1.058496\n",
      "Train Epoch: 0 [213120/482500 (44%)]\tLoss: 1.161365\n",
      "Train Epoch: 0 [213280/482500 (44%)]\tLoss: 1.091130\n",
      "Train Epoch: 0 [213440/482500 (44%)]\tLoss: 1.076820\n",
      "Train Epoch: 0 [213600/482500 (44%)]\tLoss: 1.256474\n",
      "Train Epoch: 0 [213760/482500 (44%)]\tLoss: 1.363006\n",
      "Train Epoch: 0 [213920/482500 (44%)]\tLoss: 1.411485\n",
      "Train Epoch: 0 [214080/482500 (44%)]\tLoss: 1.468102\n",
      "Train Epoch: 0 [214240/482500 (44%)]\tLoss: 1.199792\n",
      "Train Epoch: 0 [214400/482500 (44%)]\tLoss: 17.037825\n",
      "Train Epoch: 0 [214560/482500 (44%)]\tLoss: 4.547650\n",
      "Train Epoch: 0 [214720/482500 (45%)]\tLoss: 2.512862\n",
      "Train Epoch: 0 [214880/482500 (45%)]\tLoss: 1.700986\n",
      "Train Epoch: 0 [215040/482500 (45%)]\tLoss: 0.983827\n",
      "Train Epoch: 0 [215200/482500 (45%)]\tLoss: 1.142690\n",
      "Train Epoch: 0 [215360/482500 (45%)]\tLoss: 0.651673\n",
      "Train Epoch: 0 [215520/482500 (45%)]\tLoss: 1.007429\n",
      "Train Epoch: 0 [215680/482500 (45%)]\tLoss: 1.011585\n",
      "Train Epoch: 0 [215840/482500 (45%)]\tLoss: 0.886309\n",
      "Train Epoch: 0 [216000/482500 (45%)]\tLoss: 1.056787\n",
      "Train Epoch: 0 [216160/482500 (45%)]\tLoss: 0.979006\n",
      "Train Epoch: 0 [216320/482500 (45%)]\tLoss: 1.296958\n",
      "Train Epoch: 0 [216480/482500 (45%)]\tLoss: 1.053544\n",
      "Train Epoch: 0 [216640/482500 (45%)]\tLoss: 0.774429\n",
      "Train Epoch: 0 [216800/482500 (45%)]\tLoss: 0.901135\n",
      "Train Epoch: 0 [216960/482500 (45%)]\tLoss: 1.556952\n",
      "Train Epoch: 0 [217120/482500 (45%)]\tLoss: 1.165588\n",
      "Train Epoch: 0 [217280/482500 (45%)]\tLoss: 1.669450\n",
      "Train Epoch: 0 [217440/482500 (45%)]\tLoss: 1.023453\n",
      "Train Epoch: 0 [217600/482500 (45%)]\tLoss: 1.274552\n",
      "Train Epoch: 0 [217760/482500 (45%)]\tLoss: 1.544903\n",
      "Train Epoch: 0 [217920/482500 (45%)]\tLoss: 1.301227\n",
      "Train Epoch: 0 [218080/482500 (45%)]\tLoss: 4.895068\n",
      "Train Epoch: 0 [218240/482500 (45%)]\tLoss: 1.780339\n",
      "Train Epoch: 0 [218400/482500 (45%)]\tLoss: 1.521873\n",
      "Train Epoch: 0 [218560/482500 (45%)]\tLoss: 2.232817\n",
      "Train Epoch: 0 [218720/482500 (45%)]\tLoss: 3.789727\n",
      "Train Epoch: 0 [218880/482500 (45%)]\tLoss: 2.219183\n",
      "Train Epoch: 0 [219040/482500 (45%)]\tLoss: 1.127888\n",
      "Train Epoch: 0 [219200/482500 (45%)]\tLoss: 1.143145\n",
      "Train Epoch: 0 [219360/482500 (45%)]\tLoss: 0.968833\n",
      "Train Epoch: 0 [219520/482500 (45%)]\tLoss: 1.442299\n",
      "Train Epoch: 0 [219680/482500 (46%)]\tLoss: 1.383009\n",
      "Train Epoch: 0 [219840/482500 (46%)]\tLoss: 0.929106\n",
      "Train Epoch: 0 [220000/482500 (46%)]\tLoss: 0.991106\n",
      "Train Epoch: 0 [220160/482500 (46%)]\tLoss: 1.141474\n",
      "Train Epoch: 0 [220320/482500 (46%)]\tLoss: 0.731792\n",
      "Train Epoch: 0 [220480/482500 (46%)]\tLoss: 1.132759\n",
      "Train Epoch: 0 [220640/482500 (46%)]\tLoss: 1.070342\n",
      "Train Epoch: 0 [220800/482500 (46%)]\tLoss: 1.095893\n",
      "Train Epoch: 0 [220960/482500 (46%)]\tLoss: 0.958150\n",
      "Train Epoch: 0 [221120/482500 (46%)]\tLoss: 140.602539\n",
      "Train Epoch: 0 [221280/482500 (46%)]\tLoss: 982.178955\n",
      "Train Epoch: 0 [221440/482500 (46%)]\tLoss: 205.491318\n",
      "Train Epoch: 0 [221600/482500 (46%)]\tLoss: 50.151962\n",
      "Train Epoch: 0 [221760/482500 (46%)]\tLoss: 11.340386\n",
      "Train Epoch: 0 [221920/482500 (46%)]\tLoss: 3.137541\n",
      "Train Epoch: 0 [222080/482500 (46%)]\tLoss: 4.187412\n",
      "Train Epoch: 0 [222240/482500 (46%)]\tLoss: 1.954429\n",
      "Train Epoch: 0 [222400/482500 (46%)]\tLoss: 2.943653\n",
      "Train Epoch: 0 [222560/482500 (46%)]\tLoss: 3.783325\n",
      "Train Epoch: 0 [222720/482500 (46%)]\tLoss: 3.362248\n",
      "Train Epoch: 0 [222880/482500 (46%)]\tLoss: 8.509955\n",
      "Train Epoch: 0 [223040/482500 (46%)]\tLoss: 4.878781\n",
      "Train Epoch: 0 [223200/482500 (46%)]\tLoss: 5.192700\n",
      "Train Epoch: 0 [223360/482500 (46%)]\tLoss: 3.351557\n",
      "Train Epoch: 0 [223520/482500 (46%)]\tLoss: 4.269367\n",
      "Train Epoch: 0 [223680/482500 (46%)]\tLoss: 2.339685\n",
      "Train Epoch: 0 [223840/482500 (46%)]\tLoss: 2.601433\n",
      "Train Epoch: 0 [224000/482500 (46%)]\tLoss: 1.733848\n",
      "Train Epoch: 0 [224160/482500 (46%)]\tLoss: 1.610630\n",
      "Train Epoch: 0 [224320/482500 (46%)]\tLoss: 2.520838\n",
      "Train Epoch: 0 [224480/482500 (47%)]\tLoss: 1.798631\n",
      "Train Epoch: 0 [224640/482500 (47%)]\tLoss: 3.201523\n",
      "Train Epoch: 0 [224800/482500 (47%)]\tLoss: 2.897432\n",
      "Train Epoch: 0 [224960/482500 (47%)]\tLoss: 2.375618\n",
      "Train Epoch: 0 [225120/482500 (47%)]\tLoss: 5.935709\n",
      "Train Epoch: 0 [225280/482500 (47%)]\tLoss: 1.417219\n",
      "Train Epoch: 0 [225440/482500 (47%)]\tLoss: 16.254498\n",
      "Train Epoch: 0 [225600/482500 (47%)]\tLoss: 2.710175\n",
      "Train Epoch: 0 [225760/482500 (47%)]\tLoss: 4.875065\n",
      "Train Epoch: 0 [225920/482500 (47%)]\tLoss: 2.538906\n",
      "Train Epoch: 0 [226080/482500 (47%)]\tLoss: 2.433228\n",
      "Train Epoch: 0 [226240/482500 (47%)]\tLoss: 2.600733\n",
      "Train Epoch: 0 [226400/482500 (47%)]\tLoss: 2.214240\n",
      "Train Epoch: 0 [226560/482500 (47%)]\tLoss: 3.793368\n",
      "Train Epoch: 0 [226720/482500 (47%)]\tLoss: 2.233155\n",
      "Train Epoch: 0 [226880/482500 (47%)]\tLoss: 2.605225\n",
      "Train Epoch: 0 [227040/482500 (47%)]\tLoss: 2.812052\n",
      "Train Epoch: 0 [227200/482500 (47%)]\tLoss: 2.741931\n",
      "Train Epoch: 0 [227360/482500 (47%)]\tLoss: 2.257978\n",
      "Train Epoch: 0 [227520/482500 (47%)]\tLoss: 2.905216\n",
      "Train Epoch: 0 [227680/482500 (47%)]\tLoss: 3.388499\n",
      "Train Epoch: 0 [227840/482500 (47%)]\tLoss: 2.176892\n",
      "Train Epoch: 0 [228000/482500 (47%)]\tLoss: 2.745993\n",
      "Train Epoch: 0 [228160/482500 (47%)]\tLoss: 2.644418\n",
      "Train Epoch: 0 [228320/482500 (47%)]\tLoss: 2.438897\n",
      "Train Epoch: 0 [228480/482500 (47%)]\tLoss: 38.210186\n",
      "Train Epoch: 0 [228640/482500 (47%)]\tLoss: 8.725312\n",
      "Train Epoch: 0 [228800/482500 (47%)]\tLoss: 4.286709\n",
      "Train Epoch: 0 [228960/482500 (47%)]\tLoss: 2.910746\n",
      "Train Epoch: 0 [229120/482500 (47%)]\tLoss: 2.703640\n",
      "Train Epoch: 0 [229280/482500 (48%)]\tLoss: 3.721913\n",
      "Train Epoch: 0 [229440/482500 (48%)]\tLoss: 3.035574\n",
      "Train Epoch: 0 [229600/482500 (48%)]\tLoss: 3.432099\n",
      "Train Epoch: 0 [229760/482500 (48%)]\tLoss: 5.444386\n",
      "Train Epoch: 0 [229920/482500 (48%)]\tLoss: 2.429807\n",
      "Train Epoch: 0 [230080/482500 (48%)]\tLoss: 3.689445\n",
      "Train Epoch: 0 [230240/482500 (48%)]\tLoss: 3.930539\n",
      "Train Epoch: 0 [230400/482500 (48%)]\tLoss: 6.746179\n",
      "Train Epoch: 0 [230560/482500 (48%)]\tLoss: 2.904647\n",
      "Train Epoch: 0 [230720/482500 (48%)]\tLoss: 4.250651\n",
      "Train Epoch: 0 [230880/482500 (48%)]\tLoss: 2.981885\n",
      "Train Epoch: 0 [231040/482500 (48%)]\tLoss: 4.320237\n",
      "Train Epoch: 0 [231200/482500 (48%)]\tLoss: 3.808359\n",
      "Train Epoch: 0 [231360/482500 (48%)]\tLoss: 2.460196\n",
      "Train Epoch: 0 [231520/482500 (48%)]\tLoss: 2.676621\n",
      "Train Epoch: 0 [231680/482500 (48%)]\tLoss: 2.520998\n",
      "Train Epoch: 0 [231840/482500 (48%)]\tLoss: 2.078780\n",
      "Train Epoch: 0 [232000/482500 (48%)]\tLoss: 2.415527\n",
      "Train Epoch: 0 [232160/482500 (48%)]\tLoss: 1.996676\n",
      "Train Epoch: 0 [232320/482500 (48%)]\tLoss: 3.402822\n",
      "Train Epoch: 0 [232480/482500 (48%)]\tLoss: 2.339926\n",
      "Train Epoch: 0 [232640/482500 (48%)]\tLoss: 3.297109\n",
      "Train Epoch: 0 [232800/482500 (48%)]\tLoss: 2.041181\n",
      "Train Epoch: 0 [232960/482500 (48%)]\tLoss: 187.503342\n",
      "Train Epoch: 0 [233120/482500 (48%)]\tLoss: 364.624481\n",
      "Train Epoch: 0 [233280/482500 (48%)]\tLoss: 18.062286\n",
      "Train Epoch: 0 [233440/482500 (48%)]\tLoss: 8.043694\n",
      "Train Epoch: 0 [233600/482500 (48%)]\tLoss: 5.825054\n",
      "Train Epoch: 0 [233760/482500 (48%)]\tLoss: 6.014313\n",
      "Train Epoch: 0 [233920/482500 (48%)]\tLoss: 4.330644\n",
      "Train Epoch: 0 [234080/482500 (49%)]\tLoss: 5.123787\n",
      "Train Epoch: 0 [234240/482500 (49%)]\tLoss: 3.228849\n",
      "Train Epoch: 0 [234400/482500 (49%)]\tLoss: 4.026547\n",
      "Train Epoch: 0 [234560/482500 (49%)]\tLoss: 68.555702\n",
      "Train Epoch: 0 [234720/482500 (49%)]\tLoss: 2.190378\n",
      "Train Epoch: 0 [234880/482500 (49%)]\tLoss: 4.739705\n",
      "Train Epoch: 0 [235040/482500 (49%)]\tLoss: 3.162916\n",
      "Train Epoch: 0 [235200/482500 (49%)]\tLoss: 2.684488\n",
      "Train Epoch: 0 [235360/482500 (49%)]\tLoss: 4.840772\n",
      "Train Epoch: 0 [235520/482500 (49%)]\tLoss: 4.677857\n",
      "Train Epoch: 0 [235680/482500 (49%)]\tLoss: 3.289532\n",
      "Train Epoch: 0 [235840/482500 (49%)]\tLoss: 4.211583\n",
      "Train Epoch: 0 [236000/482500 (49%)]\tLoss: 4.671186\n",
      "Train Epoch: 0 [236160/482500 (49%)]\tLoss: 4.216614\n",
      "Train Epoch: 0 [236320/482500 (49%)]\tLoss: 4.950003\n",
      "Train Epoch: 0 [236480/482500 (49%)]\tLoss: 2.004878\n",
      "Train Epoch: 0 [236640/482500 (49%)]\tLoss: 5.121128\n",
      "Train Epoch: 0 [236800/482500 (49%)]\tLoss: 2.572340\n",
      "Train Epoch: 0 [236960/482500 (49%)]\tLoss: 2.475043\n",
      "Train Epoch: 0 [237120/482500 (49%)]\tLoss: 2.690567\n",
      "Train Epoch: 0 [237280/482500 (49%)]\tLoss: 2.495174\n",
      "Train Epoch: 0 [237440/482500 (49%)]\tLoss: 3.134722\n",
      "Train Epoch: 0 [237600/482500 (49%)]\tLoss: 3.102534\n",
      "Train Epoch: 0 [237760/482500 (49%)]\tLoss: 2.715733\n",
      "Train Epoch: 0 [237920/482500 (49%)]\tLoss: 3.154677\n",
      "Train Epoch: 0 [238080/482500 (49%)]\tLoss: 3.130431\n",
      "Train Epoch: 0 [238240/482500 (49%)]\tLoss: 3.949534\n",
      "Train Epoch: 0 [238400/482500 (49%)]\tLoss: 2.786655\n",
      "Train Epoch: 0 [238560/482500 (49%)]\tLoss: 1.741657\n",
      "Train Epoch: 0 [238720/482500 (49%)]\tLoss: 2.415205\n",
      "Train Epoch: 0 [238880/482500 (50%)]\tLoss: 2.569947\n",
      "Train Epoch: 0 [239040/482500 (50%)]\tLoss: 2.346159\n",
      "Train Epoch: 0 [239200/482500 (50%)]\tLoss: 2.140047\n",
      "Train Epoch: 0 [239360/482500 (50%)]\tLoss: 1.543270\n",
      "Train Epoch: 0 [239520/482500 (50%)]\tLoss: 2.561302\n",
      "Train Epoch: 0 [239680/482500 (50%)]\tLoss: 2.926519\n",
      "Train Epoch: 0 [239840/482500 (50%)]\tLoss: 2.427319\n",
      "Train Epoch: 0 [240000/482500 (50%)]\tLoss: 1.993396\n",
      "Train Epoch: 0 [240160/482500 (50%)]\tLoss: 2.320457\n",
      "Train Epoch: 0 [240320/482500 (50%)]\tLoss: 2.968119\n",
      "Train Epoch: 0 [240480/482500 (50%)]\tLoss: 2140.344482\n",
      "Train Epoch: 0 [240640/482500 (50%)]\tLoss: 15.187457\n",
      "Train Epoch: 0 [240800/482500 (50%)]\tLoss: 22.323671\n",
      "Train Epoch: 0 [240960/482500 (50%)]\tLoss: 126.605415\n",
      "Train Epoch: 0 [241120/482500 (50%)]\tLoss: 44.957436\n",
      "Train Epoch: 0 [241280/482500 (50%)]\tLoss: 3.613112\n",
      "Train Epoch: 0 [241440/482500 (50%)]\tLoss: 2.631892\n",
      "Train Epoch: 0 [241600/482500 (50%)]\tLoss: 3.292665\n",
      "Train Epoch: 0 [241760/482500 (50%)]\tLoss: 2.773854\n",
      "Train Epoch: 0 [241920/482500 (50%)]\tLoss: 5.088382\n",
      "Train Epoch: 0 [242080/482500 (50%)]\tLoss: 2.545750\n",
      "Train Epoch: 0 [242240/482500 (50%)]\tLoss: 3.046489\n",
      "Train Epoch: 0 [242400/482500 (50%)]\tLoss: 1.569258\n",
      "Train Epoch: 0 [242560/482500 (50%)]\tLoss: 3.781968\n",
      "Train Epoch: 0 [242720/482500 (50%)]\tLoss: 2.220850\n",
      "Train Epoch: 0 [242880/482500 (50%)]\tLoss: 2.444752\n",
      "Train Epoch: 0 [243040/482500 (50%)]\tLoss: 2.914038\n",
      "Train Epoch: 0 [243200/482500 (50%)]\tLoss: 2.164942\n",
      "Train Epoch: 0 [243360/482500 (50%)]\tLoss: 4.380789\n",
      "Train Epoch: 0 [243520/482500 (50%)]\tLoss: 2.127062\n",
      "Train Epoch: 0 [243680/482500 (51%)]\tLoss: 3.771928\n",
      "Train Epoch: 0 [243840/482500 (51%)]\tLoss: 3.760990\n",
      "Train Epoch: 0 [244000/482500 (51%)]\tLoss: 2.783471\n",
      "Train Epoch: 0 [244160/482500 (51%)]\tLoss: 3.047282\n",
      "Train Epoch: 0 [244320/482500 (51%)]\tLoss: 2.410097\n",
      "Train Epoch: 0 [244480/482500 (51%)]\tLoss: 3.187936\n",
      "Train Epoch: 0 [244640/482500 (51%)]\tLoss: 3.671559\n",
      "Train Epoch: 0 [244800/482500 (51%)]\tLoss: 3.269396\n",
      "Train Epoch: 0 [244960/482500 (51%)]\tLoss: 2.752376\n",
      "Train Epoch: 0 [245120/482500 (51%)]\tLoss: 2.070298\n",
      "Train Epoch: 0 [245280/482500 (51%)]\tLoss: 2.082225\n",
      "Train Epoch: 0 [245440/482500 (51%)]\tLoss: 1.927931\n",
      "Train Epoch: 0 [245600/482500 (51%)]\tLoss: 2.479642\n",
      "Train Epoch: 0 [245760/482500 (51%)]\tLoss: 2.074323\n",
      "Train Epoch: 0 [245920/482500 (51%)]\tLoss: 3.181082\n",
      "Train Epoch: 0 [246080/482500 (51%)]\tLoss: 2.720207\n",
      "Train Epoch: 0 [246240/482500 (51%)]\tLoss: 1.910139\n",
      "Train Epoch: 0 [246400/482500 (51%)]\tLoss: 2.736796\n",
      "Train Epoch: 0 [246560/482500 (51%)]\tLoss: 1.782161\n",
      "Train Epoch: 0 [246720/482500 (51%)]\tLoss: 1.709182\n",
      "Train Epoch: 0 [246880/482500 (51%)]\tLoss: 1.861908\n",
      "Train Epoch: 0 [247040/482500 (51%)]\tLoss: 1.643478\n",
      "Train Epoch: 0 [247200/482500 (51%)]\tLoss: 2.594941\n",
      "Train Epoch: 0 [247360/482500 (51%)]\tLoss: 2.420557\n",
      "Train Epoch: 0 [247520/482500 (51%)]\tLoss: 1.182953\n",
      "Train Epoch: 0 [247680/482500 (51%)]\tLoss: 2.141392\n",
      "Train Epoch: 0 [247840/482500 (51%)]\tLoss: 2.686151\n",
      "Train Epoch: 0 [248000/482500 (51%)]\tLoss: 2.029207\n",
      "Train Epoch: 0 [248160/482500 (51%)]\tLoss: 2.219753\n",
      "Train Epoch: 0 [248320/482500 (51%)]\tLoss: 1.936424\n",
      "Train Epoch: 0 [248480/482500 (51%)]\tLoss: 1.570917\n",
      "Train Epoch: 0 [248640/482500 (52%)]\tLoss: 2.219595\n",
      "Train Epoch: 0 [248800/482500 (52%)]\tLoss: 1.737641\n",
      "Train Epoch: 0 [248960/482500 (52%)]\tLoss: 2.257324\n",
      "Train Epoch: 0 [249120/482500 (52%)]\tLoss: 1.947743\n",
      "Train Epoch: 0 [249280/482500 (52%)]\tLoss: 1.287753\n",
      "Train Epoch: 0 [249440/482500 (52%)]\tLoss: 2.277498\n",
      "Train Epoch: 0 [249600/482500 (52%)]\tLoss: 2.050143\n",
      "Train Epoch: 0 [249760/482500 (52%)]\tLoss: 2.059583\n",
      "Train Epoch: 0 [249920/482500 (52%)]\tLoss: 1.589354\n",
      "Train Epoch: 0 [250080/482500 (52%)]\tLoss: 1.264205\n",
      "Train Epoch: 0 [250240/482500 (52%)]\tLoss: 1.265379\n",
      "Train Epoch: 0 [250400/482500 (52%)]\tLoss: 1.427190\n",
      "Train Epoch: 0 [250560/482500 (52%)]\tLoss: 1.743939\n",
      "Train Epoch: 0 [250720/482500 (52%)]\tLoss: 2.182538\n",
      "Train Epoch: 0 [250880/482500 (52%)]\tLoss: 1.929504\n",
      "Train Epoch: 0 [251040/482500 (52%)]\tLoss: 1.253537\n",
      "Train Epoch: 0 [251200/482500 (52%)]\tLoss: 1.707896\n",
      "Train Epoch: 0 [251360/482500 (52%)]\tLoss: 2.130823\n",
      "Train Epoch: 0 [251520/482500 (52%)]\tLoss: 1.804105\n",
      "Train Epoch: 0 [251680/482500 (52%)]\tLoss: 1.891898\n",
      "Train Epoch: 0 [251840/482500 (52%)]\tLoss: 2.612059\n",
      "Train Epoch: 0 [252000/482500 (52%)]\tLoss: 1.213060\n",
      "Train Epoch: 0 [252160/482500 (52%)]\tLoss: 1.367797\n",
      "Train Epoch: 0 [252320/482500 (52%)]\tLoss: 1.382208\n",
      "Train Epoch: 0 [252480/482500 (52%)]\tLoss: 2.297322\n",
      "Train Epoch: 0 [252640/482500 (52%)]\tLoss: 1.693453\n",
      "Train Epoch: 0 [252800/482500 (52%)]\tLoss: 1.189244\n",
      "Train Epoch: 0 [252960/482500 (52%)]\tLoss: 53.574734\n",
      "Train Epoch: 0 [253120/482500 (52%)]\tLoss: 11.736845\n",
      "Train Epoch: 0 [253280/482500 (52%)]\tLoss: 5.116379\n",
      "Train Epoch: 0 [253440/482500 (53%)]\tLoss: 2.808612\n",
      "Train Epoch: 0 [253600/482500 (53%)]\tLoss: 5.103186\n",
      "Train Epoch: 0 [253760/482500 (53%)]\tLoss: 3171.325439\n",
      "Train Epoch: 0 [253920/482500 (53%)]\tLoss: 680.366272\n",
      "Train Epoch: 0 [254080/482500 (53%)]\tLoss: 79.815941\n",
      "Train Epoch: 0 [254240/482500 (53%)]\tLoss: 19.887821\n",
      "Train Epoch: 0 [254400/482500 (53%)]\tLoss: 20.281319\n",
      "Train Epoch: 0 [254560/482500 (53%)]\tLoss: 13.295037\n",
      "Train Epoch: 0 [254720/482500 (53%)]\tLoss: 6.958157\n",
      "Train Epoch: 0 [254880/482500 (53%)]\tLoss: 3.317570\n",
      "Train Epoch: 0 [255040/482500 (53%)]\tLoss: 11.043327\n",
      "Train Epoch: 0 [255200/482500 (53%)]\tLoss: 5.128977\n",
      "Train Epoch: 0 [255360/482500 (53%)]\tLoss: 3.581912\n",
      "Train Epoch: 0 [255520/482500 (53%)]\tLoss: 6.369750\n",
      "Train Epoch: 0 [255680/482500 (53%)]\tLoss: 3.337829\n",
      "Train Epoch: 0 [255840/482500 (53%)]\tLoss: 3.478305\n",
      "Train Epoch: 0 [256000/482500 (53%)]\tLoss: 4.214890\n",
      "Train Epoch: 0 [256160/482500 (53%)]\tLoss: 3.263034\n",
      "Train Epoch: 0 [256320/482500 (53%)]\tLoss: 4.645857\n",
      "Train Epoch: 0 [256480/482500 (53%)]\tLoss: 3.302424\n",
      "Train Epoch: 0 [256640/482500 (53%)]\tLoss: 6.366259\n",
      "Train Epoch: 0 [256800/482500 (53%)]\tLoss: 5.165786\n",
      "Train Epoch: 0 [256960/482500 (53%)]\tLoss: 4.260279\n",
      "Train Epoch: 0 [257120/482500 (53%)]\tLoss: 3.750174\n",
      "Train Epoch: 0 [257280/482500 (53%)]\tLoss: 4.398293\n",
      "Train Epoch: 0 [257440/482500 (53%)]\tLoss: 4.441846\n",
      "Train Epoch: 0 [257600/482500 (53%)]\tLoss: 5.080172\n",
      "Train Epoch: 0 [257760/482500 (53%)]\tLoss: 3.599459\n",
      "Train Epoch: 0 [257920/482500 (53%)]\tLoss: 3.870565\n",
      "Train Epoch: 0 [258080/482500 (53%)]\tLoss: 3.025849\n",
      "Train Epoch: 0 [258240/482500 (54%)]\tLoss: 41.245350\n",
      "Train Epoch: 0 [258400/482500 (54%)]\tLoss: 10.815474\n",
      "Train Epoch: 0 [258560/482500 (54%)]\tLoss: 6.899309\n",
      "Train Epoch: 0 [258720/482500 (54%)]\tLoss: 7.573382\n",
      "Train Epoch: 0 [258880/482500 (54%)]\tLoss: 5.959670\n",
      "Train Epoch: 0 [259040/482500 (54%)]\tLoss: 4.950048\n",
      "Train Epoch: 0 [259200/482500 (54%)]\tLoss: 4.415933\n",
      "Train Epoch: 0 [259360/482500 (54%)]\tLoss: 6.242868\n",
      "Train Epoch: 0 [259520/482500 (54%)]\tLoss: 7.454940\n",
      "Train Epoch: 0 [259680/482500 (54%)]\tLoss: 4.219235\n",
      "Train Epoch: 0 [259840/482500 (54%)]\tLoss: 3.330915\n",
      "Train Epoch: 0 [260000/482500 (54%)]\tLoss: 16.258047\n",
      "Train Epoch: 0 [260160/482500 (54%)]\tLoss: 6.021656\n",
      "Train Epoch: 0 [260320/482500 (54%)]\tLoss: 6.509641\n",
      "Train Epoch: 0 [260480/482500 (54%)]\tLoss: 1.821501\n",
      "Train Epoch: 0 [260640/482500 (54%)]\tLoss: 5.894505\n",
      "Train Epoch: 0 [260800/482500 (54%)]\tLoss: 4.200642\n",
      "Train Epoch: 0 [260960/482500 (54%)]\tLoss: 4.444600\n",
      "Train Epoch: 0 [261120/482500 (54%)]\tLoss: 3.113451\n",
      "Train Epoch: 0 [261280/482500 (54%)]\tLoss: 3.877898\n",
      "Train Epoch: 0 [261440/482500 (54%)]\tLoss: 3.198857\n",
      "Train Epoch: 0 [261600/482500 (54%)]\tLoss: 3.435497\n",
      "Train Epoch: 0 [261760/482500 (54%)]\tLoss: 90.117928\n",
      "Train Epoch: 0 [261920/482500 (54%)]\tLoss: 5.764484\n",
      "Train Epoch: 0 [262080/482500 (54%)]\tLoss: 5.332095\n",
      "Train Epoch: 0 [262240/482500 (54%)]\tLoss: 4.911520\n",
      "Train Epoch: 0 [262400/482500 (54%)]\tLoss: 8.016113\n",
      "Train Epoch: 0 [262560/482500 (54%)]\tLoss: 3.614586\n",
      "Train Epoch: 0 [262720/482500 (54%)]\tLoss: 4.341981\n",
      "Train Epoch: 0 [262880/482500 (54%)]\tLoss: 2.395672\n",
      "Train Epoch: 0 [263040/482500 (55%)]\tLoss: 3.870525\n",
      "Train Epoch: 0 [263200/482500 (55%)]\tLoss: 5.162222\n",
      "Train Epoch: 0 [263360/482500 (55%)]\tLoss: 2.605667\n",
      "Train Epoch: 0 [263520/482500 (55%)]\tLoss: 2.856422\n",
      "Train Epoch: 0 [263680/482500 (55%)]\tLoss: 3.051299\n",
      "Train Epoch: 0 [263840/482500 (55%)]\tLoss: 2.293638\n",
      "Train Epoch: 0 [264000/482500 (55%)]\tLoss: 3.545733\n",
      "Train Epoch: 0 [264160/482500 (55%)]\tLoss: 3.083453\n",
      "Train Epoch: 0 [264320/482500 (55%)]\tLoss: 2.873569\n",
      "Train Epoch: 0 [264480/482500 (55%)]\tLoss: 3.914828\n",
      "Train Epoch: 0 [264640/482500 (55%)]\tLoss: 4.497863\n",
      "Train Epoch: 0 [264800/482500 (55%)]\tLoss: 2.513354\n",
      "Train Epoch: 0 [264960/482500 (55%)]\tLoss: 3.032333\n",
      "Train Epoch: 0 [265120/482500 (55%)]\tLoss: 2.648982\n",
      "Train Epoch: 0 [265280/482500 (55%)]\tLoss: 3.596471\n",
      "Train Epoch: 0 [265440/482500 (55%)]\tLoss: 2.486662\n",
      "Train Epoch: 0 [265600/482500 (55%)]\tLoss: 5.321544\n",
      "Train Epoch: 0 [265760/482500 (55%)]\tLoss: 1.576479\n",
      "Train Epoch: 0 [265920/482500 (55%)]\tLoss: 2.738836\n",
      "Train Epoch: 0 [266080/482500 (55%)]\tLoss: 2.857746\n",
      "Train Epoch: 0 [266240/482500 (55%)]\tLoss: 3.557809\n",
      "Train Epoch: 0 [266400/482500 (55%)]\tLoss: 3.171161\n",
      "Train Epoch: 0 [266560/482500 (55%)]\tLoss: 2.952720\n",
      "Train Epoch: 0 [266720/482500 (55%)]\tLoss: 2.915684\n",
      "Train Epoch: 0 [266880/482500 (55%)]\tLoss: 1.992766\n",
      "Train Epoch: 0 [267040/482500 (55%)]\tLoss: 3.819709\n",
      "Train Epoch: 0 [267200/482500 (55%)]\tLoss: 1.982805\n",
      "Train Epoch: 0 [267360/482500 (55%)]\tLoss: 3.272786\n",
      "Train Epoch: 0 [267520/482500 (55%)]\tLoss: 2.479765\n",
      "Train Epoch: 0 [267680/482500 (55%)]\tLoss: 1.971872\n",
      "Train Epoch: 0 [267840/482500 (56%)]\tLoss: 3.265581\n",
      "Train Epoch: 0 [268000/482500 (56%)]\tLoss: 2.363704\n",
      "Train Epoch: 0 [268160/482500 (56%)]\tLoss: 2.369227\n",
      "Train Epoch: 0 [268320/482500 (56%)]\tLoss: 1.766394\n",
      "Train Epoch: 0 [268480/482500 (56%)]\tLoss: 1.907910\n",
      "Train Epoch: 0 [268640/482500 (56%)]\tLoss: 2.409310\n",
      "Train Epoch: 0 [268800/482500 (56%)]\tLoss: 3.432647\n",
      "Train Epoch: 0 [268960/482500 (56%)]\tLoss: 1.440220\n",
      "Train Epoch: 0 [269120/482500 (56%)]\tLoss: 2.100696\n",
      "Train Epoch: 0 [269280/482500 (56%)]\tLoss: 2.572618\n",
      "Train Epoch: 0 [269440/482500 (56%)]\tLoss: 2.348627\n",
      "Train Epoch: 0 [269600/482500 (56%)]\tLoss: 1.686776\n",
      "Train Epoch: 0 [269760/482500 (56%)]\tLoss: 2.052881\n",
      "Train Epoch: 0 [269920/482500 (56%)]\tLoss: 1.855636\n",
      "Train Epoch: 0 [270080/482500 (56%)]\tLoss: 1.264174\n",
      "Train Epoch: 0 [270240/482500 (56%)]\tLoss: 2.493860\n",
      "Train Epoch: 0 [270400/482500 (56%)]\tLoss: 1.762676\n",
      "Train Epoch: 0 [270560/482500 (56%)]\tLoss: 1.548732\n",
      "Train Epoch: 0 [270720/482500 (56%)]\tLoss: 1.370493\n",
      "Train Epoch: 0 [270880/482500 (56%)]\tLoss: 11.385143\n",
      "Train Epoch: 0 [271040/482500 (56%)]\tLoss: 2.348959\n",
      "Train Epoch: 0 [271200/482500 (56%)]\tLoss: 2.311192\n",
      "Train Epoch: 0 [271360/482500 (56%)]\tLoss: 2.119637\n",
      "Train Epoch: 0 [271520/482500 (56%)]\tLoss: 2.671896\n",
      "Train Epoch: 0 [271680/482500 (56%)]\tLoss: 1.679454\n",
      "Train Epoch: 0 [271840/482500 (56%)]\tLoss: 2.032259\n",
      "Train Epoch: 0 [272000/482500 (56%)]\tLoss: 1.509532\n",
      "Train Epoch: 0 [272160/482500 (56%)]\tLoss: 2.993900\n",
      "Train Epoch: 0 [272320/482500 (56%)]\tLoss: 4.170252\n",
      "Train Epoch: 0 [272480/482500 (56%)]\tLoss: 1.544797\n",
      "Train Epoch: 0 [272640/482500 (57%)]\tLoss: 4.660559\n",
      "Train Epoch: 0 [272800/482500 (57%)]\tLoss: 2.006363\n",
      "Train Epoch: 0 [272960/482500 (57%)]\tLoss: 1.580573\n",
      "Train Epoch: 0 [273120/482500 (57%)]\tLoss: 1.363517\n",
      "Train Epoch: 0 [273280/482500 (57%)]\tLoss: 1.715547\n",
      "Train Epoch: 0 [273440/482500 (57%)]\tLoss: 0.858273\n",
      "Train Epoch: 0 [273600/482500 (57%)]\tLoss: 1.512497\n",
      "Train Epoch: 0 [273760/482500 (57%)]\tLoss: 1.685729\n",
      "Train Epoch: 0 [273920/482500 (57%)]\tLoss: 1.049855\n",
      "Train Epoch: 0 [274080/482500 (57%)]\tLoss: 1.611884\n",
      "Train Epoch: 0 [274240/482500 (57%)]\tLoss: 1.186321\n",
      "Train Epoch: 0 [274400/482500 (57%)]\tLoss: 2.313290\n",
      "Train Epoch: 0 [274560/482500 (57%)]\tLoss: 1.295375\n",
      "Train Epoch: 0 [274720/482500 (57%)]\tLoss: 1.934690\n",
      "Train Epoch: 0 [274880/482500 (57%)]\tLoss: 1.753105\n",
      "Train Epoch: 0 [275040/482500 (57%)]\tLoss: 1.910189\n",
      "Train Epoch: 0 [275200/482500 (57%)]\tLoss: 1.803466\n",
      "Train Epoch: 0 [275360/482500 (57%)]\tLoss: 1.596395\n",
      "Train Epoch: 0 [275520/482500 (57%)]\tLoss: 1.596208\n",
      "Train Epoch: 0 [275680/482500 (57%)]\tLoss: 1.022289\n",
      "Train Epoch: 0 [275840/482500 (57%)]\tLoss: 2.103649\n",
      "Train Epoch: 0 [276000/482500 (57%)]\tLoss: 1.886795\n",
      "Train Epoch: 0 [276160/482500 (57%)]\tLoss: 1.856631\n",
      "Train Epoch: 0 [276320/482500 (57%)]\tLoss: 0.824272\n",
      "Train Epoch: 0 [276480/482500 (57%)]\tLoss: 1.429053\n",
      "Train Epoch: 0 [276640/482500 (57%)]\tLoss: 1.537473\n",
      "Train Epoch: 0 [276800/482500 (57%)]\tLoss: 1.979093\n",
      "Train Epoch: 0 [276960/482500 (57%)]\tLoss: 1.405751\n",
      "Train Epoch: 0 [277120/482500 (57%)]\tLoss: 1.462861\n",
      "Train Epoch: 0 [277280/482500 (57%)]\tLoss: 0.973543\n",
      "Train Epoch: 0 [277440/482500 (58%)]\tLoss: 1.835818\n",
      "Train Epoch: 0 [277600/482500 (58%)]\tLoss: 1.046477\n",
      "Train Epoch: 0 [277760/482500 (58%)]\tLoss: 1.851483\n",
      "Train Epoch: 0 [277920/482500 (58%)]\tLoss: 1.332968\n",
      "Train Epoch: 0 [278080/482500 (58%)]\tLoss: 1.697735\n",
      "Train Epoch: 0 [278240/482500 (58%)]\tLoss: 1.219841\n",
      "Train Epoch: 0 [278400/482500 (58%)]\tLoss: 1.535638\n",
      "Train Epoch: 0 [278560/482500 (58%)]\tLoss: 1.968566\n",
      "Train Epoch: 0 [278720/482500 (58%)]\tLoss: 1.591491\n",
      "Train Epoch: 0 [278880/482500 (58%)]\tLoss: 1.462712\n",
      "Train Epoch: 0 [279040/482500 (58%)]\tLoss: 1.310918\n",
      "Train Epoch: 0 [279200/482500 (58%)]\tLoss: 1.469653\n",
      "Train Epoch: 0 [279360/482500 (58%)]\tLoss: 1.476619\n",
      "Train Epoch: 0 [279520/482500 (58%)]\tLoss: 1.392212\n",
      "Train Epoch: 0 [279680/482500 (58%)]\tLoss: 1.200035\n",
      "Train Epoch: 0 [279840/482500 (58%)]\tLoss: 1.654684\n",
      "Train Epoch: 0 [280000/482500 (58%)]\tLoss: 1.380909\n",
      "Train Epoch: 0 [280160/482500 (58%)]\tLoss: 1.265344\n",
      "Train Epoch: 0 [280320/482500 (58%)]\tLoss: 1.304273\n",
      "Train Epoch: 0 [280480/482500 (58%)]\tLoss: 1.163430\n",
      "Train Epoch: 0 [280640/482500 (58%)]\tLoss: 1.277465\n",
      "Train Epoch: 0 [280800/482500 (58%)]\tLoss: 0.919843\n",
      "Train Epoch: 0 [280960/482500 (58%)]\tLoss: 0.936771\n",
      "Train Epoch: 0 [281120/482500 (58%)]\tLoss: 1.069409\n",
      "Train Epoch: 0 [281280/482500 (58%)]\tLoss: 1.283375\n",
      "Train Epoch: 0 [281440/482500 (58%)]\tLoss: 1.707609\n",
      "Train Epoch: 0 [281600/482500 (58%)]\tLoss: 0.991654\n",
      "Train Epoch: 0 [281760/482500 (58%)]\tLoss: 1.353544\n",
      "Train Epoch: 0 [281920/482500 (58%)]\tLoss: 2.772603\n",
      "Train Epoch: 0 [282080/482500 (58%)]\tLoss: 1.775985\n",
      "Train Epoch: 0 [282240/482500 (58%)]\tLoss: 1.030223\n",
      "Train Epoch: 0 [282400/482500 (59%)]\tLoss: 1.333225\n",
      "Train Epoch: 0 [282560/482500 (59%)]\tLoss: 1.233291\n",
      "Train Epoch: 0 [282720/482500 (59%)]\tLoss: 1.186944\n",
      "Train Epoch: 0 [282880/482500 (59%)]\tLoss: 2.055330\n",
      "Train Epoch: 0 [283040/482500 (59%)]\tLoss: 1.712240\n",
      "Train Epoch: 0 [283200/482500 (59%)]\tLoss: 2.030097\n",
      "Train Epoch: 0 [283360/482500 (59%)]\tLoss: 1.354839\n",
      "Train Epoch: 0 [283520/482500 (59%)]\tLoss: 0.950580\n",
      "Train Epoch: 0 [283680/482500 (59%)]\tLoss: 0.876611\n",
      "Train Epoch: 0 [283840/482500 (59%)]\tLoss: 1.535379\n",
      "Train Epoch: 0 [284000/482500 (59%)]\tLoss: 1.641187\n",
      "Train Epoch: 0 [284160/482500 (59%)]\tLoss: 1.457864\n",
      "Train Epoch: 0 [284320/482500 (59%)]\tLoss: 1.424231\n",
      "Train Epoch: 0 [284480/482500 (59%)]\tLoss: 6.024994\n",
      "Train Epoch: 0 [284640/482500 (59%)]\tLoss: 3.346774\n",
      "Train Epoch: 0 [284800/482500 (59%)]\tLoss: 2.710822\n",
      "Train Epoch: 0 [284960/482500 (59%)]\tLoss: 1.830184\n",
      "Train Epoch: 0 [285120/482500 (59%)]\tLoss: 0.928584\n",
      "Train Epoch: 0 [285280/482500 (59%)]\tLoss: 1.203201\n",
      "Train Epoch: 0 [285440/482500 (59%)]\tLoss: 30.010712\n",
      "Train Epoch: 0 [285600/482500 (59%)]\tLoss: 3.661193\n",
      "Train Epoch: 0 [285760/482500 (59%)]\tLoss: 2.109576\n",
      "Train Epoch: 0 [285920/482500 (59%)]\tLoss: 1.808192\n",
      "Train Epoch: 0 [286080/482500 (59%)]\tLoss: 0.855202\n",
      "Train Epoch: 0 [286240/482500 (59%)]\tLoss: 1.269847\n",
      "Train Epoch: 0 [286400/482500 (59%)]\tLoss: 1.303392\n",
      "Train Epoch: 0 [286560/482500 (59%)]\tLoss: 0.995436\n",
      "Train Epoch: 0 [286720/482500 (59%)]\tLoss: 1.739545\n",
      "Train Epoch: 0 [286880/482500 (59%)]\tLoss: 2.316866\n",
      "Train Epoch: 0 [287040/482500 (59%)]\tLoss: 1.523759\n",
      "Train Epoch: 0 [287200/482500 (60%)]\tLoss: 1.475143\n",
      "Train Epoch: 0 [287360/482500 (60%)]\tLoss: 0.679504\n",
      "Train Epoch: 0 [287520/482500 (60%)]\tLoss: 1.180105\n",
      "Train Epoch: 0 [287680/482500 (60%)]\tLoss: 1.465449\n",
      "Train Epoch: 0 [287840/482500 (60%)]\tLoss: 1.209001\n",
      "Train Epoch: 0 [288000/482500 (60%)]\tLoss: 1.520426\n",
      "Train Epoch: 0 [288160/482500 (60%)]\tLoss: 7.263694\n",
      "Train Epoch: 0 [288320/482500 (60%)]\tLoss: 22.687248\n",
      "Train Epoch: 0 [288480/482500 (60%)]\tLoss: 2.806958\n",
      "Train Epoch: 0 [288640/482500 (60%)]\tLoss: 3.597052\n",
      "Train Epoch: 0 [288800/482500 (60%)]\tLoss: 2.347634\n",
      "Train Epoch: 0 [288960/482500 (60%)]\tLoss: 1.388459\n",
      "Train Epoch: 0 [289120/482500 (60%)]\tLoss: 1.809839\n",
      "Train Epoch: 0 [289280/482500 (60%)]\tLoss: 2.375343\n",
      "Train Epoch: 0 [289440/482500 (60%)]\tLoss: 1.266358\n",
      "Train Epoch: 0 [289600/482500 (60%)]\tLoss: 2.459743\n",
      "Train Epoch: 0 [289760/482500 (60%)]\tLoss: 1.490726\n",
      "Train Epoch: 0 [289920/482500 (60%)]\tLoss: 1.415581\n",
      "Train Epoch: 0 [290080/482500 (60%)]\tLoss: 0.966276\n",
      "Train Epoch: 0 [290240/482500 (60%)]\tLoss: 0.954902\n",
      "Train Epoch: 0 [290400/482500 (60%)]\tLoss: 1.608868\n",
      "Train Epoch: 0 [290560/482500 (60%)]\tLoss: 26.290476\n",
      "Train Epoch: 0 [290720/482500 (60%)]\tLoss: 6.304729\n",
      "Train Epoch: 0 [290880/482500 (60%)]\tLoss: 0.972012\n",
      "Train Epoch: 0 [291040/482500 (60%)]\tLoss: 1.199634\n",
      "Train Epoch: 0 [291200/482500 (60%)]\tLoss: 1.262263\n",
      "Train Epoch: 0 [291360/482500 (60%)]\tLoss: 1.491763\n",
      "Train Epoch: 0 [291520/482500 (60%)]\tLoss: 1.313800\n",
      "Train Epoch: 0 [291680/482500 (60%)]\tLoss: 1.042266\n",
      "Train Epoch: 0 [291840/482500 (60%)]\tLoss: 0.710544\n",
      "Train Epoch: 0 [292000/482500 (61%)]\tLoss: 1.362531\n",
      "Train Epoch: 0 [292160/482500 (61%)]\tLoss: 1.191992\n",
      "Train Epoch: 0 [292320/482500 (61%)]\tLoss: 1.116335\n",
      "Train Epoch: 0 [292480/482500 (61%)]\tLoss: 1.661431\n",
      "Train Epoch: 0 [292640/482500 (61%)]\tLoss: 1.355795\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-af950977bd11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mlog_interval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_mse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#     vis.line(X=torch.ones((1,1)).cpu()*epoch,Y=torch.Tensor([epoch_loss]).unsqueeze(0).cpu(),win=loss_window,update='append',name='loss')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#         vis.line(X=torch.ones((1,1)).cpu()*epoch,Y=torch.Tensor([epoch_mse]).unsqueeze(0).cpu(),win=loss_window,update='append',name='mse_loss')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-7059a8bb683f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, writer)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0madap_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdaptiveAvgPool3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mstartgoal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"start_goal\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mocc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"observation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/torch/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/torch/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/torch/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-c8e2dc51b1bd>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mrowid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataperow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mdataid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataperow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"select id, startgoal, occ, data from highway where id = \"\u001b[0m \u001b[0;34m+\u001b[0m  \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrowid\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetchone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mstart_goal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-c8e2dc51b1bd>\u001b[0m in \u001b[0;36mconvert_array\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msqlite3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBinary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mconvert_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epoch, epoch + 200):\n",
    "    log_interval = 20\n",
    "    epoch, epoch_loss, epoch_mse = train(epoch, writer)\n",
    "#     vis.line(X=torch.ones((1,1)).cpu()*epoch,Y=torch.Tensor([epoch_loss]).unsqueeze(0).cpu(),win=loss_window,update='append',name='loss')\n",
    "#         vis.line(X=torch.ones((1,1)).cpu()*epoch,Y=torch.Tensor([epoch_mse]).unsqueeze(0).cpu(),win=loss_window,update='append',name='mse_loss')\n",
    "    writer.add_scalar('Loss/loss', epoch_loss, epoch)\n",
    "    writer.add_scalar('Loss/mse_loss', epoch_mse, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict()\n",
    "            }, 'checkpoints/highway_conv.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda:0\n",
      "convVAE(\n",
      "  (condnn): CondNN(\n",
      "    (cnn): Conv3d(\n",
      "      (adap_pool): AdaptiveAvgPool3d(output_size=(25, 100, 600))\n",
      "      (conv_layer1): Sequential(\n",
      "        (0): Conv3d(1, 16, kernel_size=(2, 3, 3), stride=(1, 1, 1))\n",
      "        (1): LeakyReLU(negative_slope=0.01)\n",
      "        (2): Conv3d(16, 16, kernel_size=(2, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (3): LeakyReLU(negative_slope=0.01)\n",
      "        (4): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (conv_layer2): Sequential(\n",
      "        (0): Conv3d(16, 32, kernel_size=(2, 3, 3), stride=(1, 1, 1))\n",
      "        (1): LeakyReLU(negative_slope=0.01)\n",
      "        (2): Conv3d(32, 32, kernel_size=(2, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (3): LeakyReLU(negative_slope=0.01)\n",
      "        (4): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (conv_layer5): Conv3d(32, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1))\n",
      "      (adap_pool2): AdaptiveAvgPool3d(output_size=(1, 1, 1))\n",
      "      (fc5): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (relu): LeakyReLU(negative_slope=0.01)\n",
      "      (batch0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (drop): Dropout(p=0.15, inplace=False)\n",
      "      (fc6): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (batch1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (fc7): Linear(in_features=64, out_features=300, bias=True)\n",
      "    )\n",
      "    (fc2): Linear(in_features=308, out_features=300, bias=True)\n",
      "  )\n",
      "  (encoder): Encoder(\n",
      "    (sequential): Sequential(\n",
      "      (0): Linear(in_features=304, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=1024, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "    (linear_means): Linear(in_features=512, out_features=50, bias=True)\n",
      "    (linear_log_var): Linear(in_features=512, out_features=50, bias=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (sequential): Sequential(\n",
      "      (0): Linear(in_features=350, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=1024, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (5): ReLU()\n",
      "      (6): Linear(in_features=512, out_features=4, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device\", device)\n",
    "\n",
    "model = convVAE(sample_size = X_dim, \n",
    "                  cnnout_size = cnn_out_size, \n",
    "                  cond_out_size = cond_out_size, \n",
    "                  encoder_layer_sizes = [512,1024,512], \n",
    "                  latent_size = z_dim, \n",
    "                  decoder_layer_sizes = [512,1024,512]).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "checkpoint = torch.load('checkpoints/highway_conv_4.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "\n",
    "model.eval()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "%matplotlib\n",
    "from utils.HighWay import plotData, plotOrientSpeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "379623\n"
     ]
    }
   ],
   "source": [
    "test_data = test_loader.dataset\n",
    "viz_idx =   torch.randint(0,len(test_data),[1]).item()  \n",
    "#  变道场景idx\n",
    "#  \n",
    "print(viz_idx)\n",
    "\n",
    "batch = test_data[viz_idx]\n",
    "startgoal = torch.from_numpy(batch[\"start_goal\"]).to(device)\n",
    "occ = torch.from_numpy(batch[\"observation\"])\n",
    "occ = occ.unsqueeze(0)\n",
    "occ = occ.unsqueeze(1)\n",
    "adap_pool = nn.AdaptiveAvgPool3d((25,100, 600))\n",
    "occ = adap_pool(occ)\n",
    "occ = occ.to(device)\n",
    "data = torch.from_numpy(batch[\"data\"]).to(device)\n",
    "\n",
    "occ=occ.cpu().detach().numpy()\n",
    "startgoal=startgoal.cpu().detach().numpy()\n",
    "data=data.cpu().detach().numpy()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "plotData(occ, startgoal, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161608\n"
     ]
    }
   ],
   "source": [
    "test_data = test_loader.dataset\n",
    "viz_idx =   torch.randint(0,len(test_data),[1]).item()  \n",
    "#  变道场景idx\n",
    "#  308958 82146 161608\n",
    "viz_idx = 161608\n",
    "print(viz_idx)\n",
    "\n",
    "batch = test_data[viz_idx]\n",
    "startgoal = torch.from_numpy(batch[\"start_goal\"]).to(device)\n",
    "occ = torch.from_numpy(batch[\"observation\"])\n",
    "occ = occ.unsqueeze(0)\n",
    "occ = occ.unsqueeze(1)\n",
    "adap_pool = nn.AdaptiveAvgPool3d((25,100, 600))\n",
    "occ = adap_pool(occ)\n",
    "occ = occ.to(device)\n",
    "\n",
    "data = torch.from_numpy(batch[\"data\"]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    y_viz = torch.randn(1,4).to(device)\n",
    "    for i in range(0, 10):\n",
    "        num_viz = 8\n",
    "        y_viz_p = model.inference(startgoal.expand(num_viz, X_dim * 2).to(device), \n",
    "                                occ.expand(num_viz, 1, -1, -1, -1).to(device), num_viz)\n",
    "        torch.cuda.empty_cache()\n",
    "        y_viz = torch.cat((y_viz_p, y_viz), dim = 0)\n",
    "\n",
    "y_viz=y_viz.cpu().detach().numpy()\n",
    "occ=occ.cpu().detach().numpy()\n",
    "startgoal=startgoal.cpu().detach().numpy()\n",
    "data=data.cpu().detach().numpy()\n",
    "torch.cuda.empty_cache()\n",
    "# from utils.NarrowPassage import plotCondition, plotSample, plotSpeed, plotSampleAttention\n",
    "\n",
    "y_viz=y_viz[:-1]\n",
    "plotData(occ, startgoal, y_viz)\n",
    "plotOrientSpeed(startgoal, y_viz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0000000e+00,  0.0000000e+00, -1.8300001e-02,  2.9430000e+01,\n",
       "        1.3418002e+02, -2.2000122e+00, -0.0000000e+00,  2.8770000e+01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "startgoal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.24543388e+02, -2.23520970e+00,  2.84436420e-02,\n",
       "         2.85247135e+01],\n",
       "       [ 1.27335419e+02, -2.13794470e+00,  2.15644315e-02,\n",
       "         2.91948204e+01],\n",
       "       [ 1.26191803e+02, -2.26579809e+00, -3.29630915e-03,\n",
       "         2.84353790e+01],\n",
       "       [ 1.41757248e+02, -2.39534807e+00, -6.34788815e-03,\n",
       "         2.90515232e+01],\n",
       "       [ 1.30280655e+02, -2.26277113e+00,  7.57206138e-03,\n",
       "         2.84413033e+01],\n",
       "       [ 1.28432755e+02, -2.29754496e+00,  1.55592030e-02,\n",
       "         2.90652924e+01],\n",
       "       [ 1.32576416e+02, -2.33535361e+00,  2.76262220e-03,\n",
       "         2.85078144e+01],\n",
       "       [ 1.43552307e+02, -2.39504910e+00, -2.56697461e-02,\n",
       "         2.87375183e+01],\n",
       "       [ 1.24400558e+02, -2.21386480e+00,  2.81393602e-02,\n",
       "         2.85131474e+01],\n",
       "       [ 1.35934860e+02, -2.30486298e+00, -5.95413987e-03,\n",
       "         2.89163666e+01],\n",
       "       [ 1.22298363e+02, -2.07659626e+00,  3.96818575e-03,\n",
       "         2.90542393e+01],\n",
       "       [ 1.41198944e+02, -2.34258270e+00,  2.72554811e-03,\n",
       "         2.84174862e+01],\n",
       "       [ 1.27828072e+02, -2.26439476e+00,  1.49050420e-02,\n",
       "         2.82786064e+01],\n",
       "       [ 1.37660431e+02, -2.28232527e+00,  6.06257375e-03,\n",
       "         2.89516106e+01],\n",
       "       [ 1.41071335e+02, -2.43957138e+00, -2.53313128e-03,\n",
       "         2.82198963e+01],\n",
       "       [ 1.27413040e+02, -2.25695992e+00,  1.79267600e-02,\n",
       "         2.82138729e+01],\n",
       "       [ 1.55900467e+02, -2.82693577e+00,  4.06492408e-03,\n",
       "         2.77780590e+01],\n",
       "       [ 1.24380127e+02, -2.05762863e+00, -3.68886627e-04,\n",
       "         2.91440811e+01],\n",
       "       [ 1.40050415e+02, -2.46346998e+00,  1.27299493e-02,\n",
       "         2.84990005e+01],\n",
       "       [ 1.34095016e+02, -2.30061507e+00, -7.73774926e-03,\n",
       "         2.88335953e+01],\n",
       "       [ 1.36073013e+02, -2.32751226e+00,  3.36891972e-04,\n",
       "         2.84653187e+01],\n",
       "       [ 1.31704269e+02, -2.18070889e+00, -1.16447741e-02,\n",
       "         2.88236771e+01],\n",
       "       [ 1.35936600e+02, -2.44248581e+00,  2.03576759e-02,\n",
       "         2.89521713e+01],\n",
       "       [ 1.23140053e+02, -2.14247561e+00,  3.13468650e-02,\n",
       "         2.83261242e+01],\n",
       "       [ 1.35347672e+02, -2.29045367e+00,  4.05836757e-03,\n",
       "         2.86490784e+01],\n",
       "       [ 1.23649200e+02, -2.12008238e+00,  8.03620275e-03,\n",
       "         2.89729080e+01],\n",
       "       [ 1.38042709e+02, -2.39669800e+00,  1.92150548e-02,\n",
       "         2.79047318e+01],\n",
       "       [ 1.24909691e+02, -2.09908342e+00,  2.20305994e-02,\n",
       "         2.92690277e+01],\n",
       "       [ 1.46161102e+02, -2.54154181e+00, -1.24487216e-02,\n",
       "         2.81672039e+01],\n",
       "       [ 1.14612839e+02, -2.13036227e+00, -2.54672691e-02,\n",
       "         2.89806290e+01],\n",
       "       [ 1.36454498e+02, -2.38408232e+00,  1.49651831e-02,\n",
       "         2.86260738e+01],\n",
       "       [ 1.42664719e+02, -2.56880522e+00, -6.38180412e-04,\n",
       "         2.81057281e+01],\n",
       "       [ 1.21907013e+02, -2.03841925e+00,  3.75158265e-02,\n",
       "         2.84632740e+01],\n",
       "       [ 1.31958954e+02, -2.30834246e+00,  1.32662710e-03,\n",
       "         2.77528572e+01],\n",
       "       [ 1.37656403e+02, -2.30024457e+00,  1.02229184e-02,\n",
       "         2.89597702e+01],\n",
       "       [ 1.35152023e+02, -2.22208142e+00, -3.49699799e-03,\n",
       "         2.82140064e+01],\n",
       "       [ 1.28251816e+02, -2.04179502e+00,  2.20293459e-03,\n",
       "         2.87586899e+01],\n",
       "       [ 1.23785484e+02, -2.01092768e+00,  6.84768613e-03,\n",
       "         2.88177624e+01],\n",
       "       [ 1.46047836e+02, -2.46301746e+00, -6.64060656e-03,\n",
       "         2.86242619e+01],\n",
       "       [ 1.42849854e+02, -2.51013899e+00,  4.70454153e-03,\n",
       "         2.79590092e+01],\n",
       "       [ 1.39221359e+02, -2.44256186e+00,  6.31911214e-03,\n",
       "         2.81979523e+01],\n",
       "       [ 1.42119232e+02, -2.34397960e+00, -9.30338446e-03,\n",
       "         2.86340160e+01],\n",
       "       [ 1.25330627e+02, -2.24632788e+00,  1.76304057e-02,\n",
       "         2.91092396e+01],\n",
       "       [ 1.13465675e+02, -1.86175358e+00,  1.16983121e-02,\n",
       "         2.88532143e+01],\n",
       "       [ 1.24703300e+02, -2.16130280e+00,  1.13751357e-02,\n",
       "         2.88543205e+01],\n",
       "       [ 1.25659897e+02, -2.30018663e+00,  3.13950237e-03,\n",
       "         2.84161625e+01],\n",
       "       [ 1.48738861e+02, -2.62185335e+00,  1.93489250e-03,\n",
       "         2.84261265e+01],\n",
       "       [ 1.44057495e+02, -2.58919382e+00,  8.36778339e-03,\n",
       "         2.76072102e+01],\n",
       "       [ 1.33300537e+02, -2.35304403e+00,  7.02715572e-03,\n",
       "         2.90715923e+01],\n",
       "       [ 1.11495850e+02, -2.03438663e+00, -6.46304432e-03,\n",
       "         2.89324608e+01],\n",
       "       [ 1.36402206e+02, -2.27383375e+00, -1.37862498e-02,\n",
       "         2.90845432e+01],\n",
       "       [ 1.31491684e+02, -2.34634542e+00,  4.87417635e-03,\n",
       "         2.83030128e+01],\n",
       "       [ 1.30088943e+02, -2.24807787e+00,  3.11881956e-03,\n",
       "         2.88369598e+01],\n",
       "       [ 1.57983414e+02, -2.74651909e+00, -2.27415487e-02,\n",
       "         2.83981304e+01],\n",
       "       [ 1.25937668e+02, -2.15588140e+00,  3.13195661e-02,\n",
       "         2.89522114e+01],\n",
       "       [ 1.38582794e+02, -2.32641101e+00, -9.86927189e-04,\n",
       "         2.81276855e+01],\n",
       "       [ 1.25810310e+02, -2.08725643e+00,  3.13331559e-02,\n",
       "         2.84320946e+01],\n",
       "       [ 1.09520027e+02, -1.89787877e+00, -1.46847898e-02,\n",
       "         2.91731777e+01],\n",
       "       [ 1.41687515e+02, -2.38091516e+00, -1.74521133e-02,\n",
       "         2.84994602e+01],\n",
       "       [ 1.41634903e+02, -2.43300629e+00, -1.63446590e-02,\n",
       "         2.84117756e+01],\n",
       "       [ 1.31361404e+02, -2.24531269e+00,  2.75362190e-03,\n",
       "         2.85840607e+01],\n",
       "       [ 1.28831970e+02, -2.18676615e+00,  2.12455466e-02,\n",
       "         2.86629848e+01],\n",
       "       [ 1.33646790e+02, -2.27027941e+00, -8.56190268e-03,\n",
       "         2.79473305e+01],\n",
       "       [ 1.33396240e+02, -2.26389122e+00,  9.87977441e-03,\n",
       "         2.93124218e+01],\n",
       "       [ 1.27074303e+02, -2.22887135e+00, -7.27456156e-03,\n",
       "         2.83180561e+01],\n",
       "       [ 1.39598816e+02, -2.24891710e+00, -3.43137328e-03,\n",
       "         2.88064308e+01],\n",
       "       [ 1.00189896e+02, -1.74915111e+00, -3.19527909e-02,\n",
       "         2.90262775e+01],\n",
       "       [ 1.26048264e+02, -2.29550123e+00,  1.10634631e-02,\n",
       "         2.85779285e+01],\n",
       "       [ 1.23105698e+02, -2.13718653e+00,  2.10008100e-02,\n",
       "         2.88886662e+01],\n",
       "       [ 1.41940887e+02, -2.33877516e+00, -1.30619938e-02,\n",
       "         2.85580215e+01],\n",
       "       [ 1.38416489e+02, -2.40354848e+00, -1.65902898e-02,\n",
       "         2.88151817e+01],\n",
       "       [ 1.29728119e+02, -2.14347363e+00, -3.43375839e-04,\n",
       "         2.93838940e+01],\n",
       "       [ 1.27743988e+02, -2.30703235e+00,  9.26095899e-03,\n",
       "         2.89234142e+01],\n",
       "       [ 1.36097473e+02, -2.24936414e+00, -2.38566324e-02,\n",
       "         2.88427334e+01],\n",
       "       [ 1.21674500e+02, -2.05664825e+00,  3.16292699e-03,\n",
       "         2.90167065e+01],\n",
       "       [ 1.56979050e+02, -2.73192716e+00, -1.11029679e-02,\n",
       "         2.81951904e+01],\n",
       "       [ 1.41294128e+02, -2.36330628e+00,  1.56312063e-02,\n",
       "         2.86810246e+01],\n",
       "       [ 1.35597961e+02, -2.27815390e+00, -1.23517448e-02,\n",
       "         2.87342052e+01],\n",
       "       [ 1.31537216e+02, -2.27309561e+00, -1.39309103e-02,\n",
       "         2.88338223e+01],\n",
       "       [ 1.60988449e+02, -2.72305107e+00,  9.02814325e-03,\n",
       "         2.84708557e+01]], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
