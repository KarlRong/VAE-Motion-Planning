{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda:0\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, optim\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dim = 6\n",
    "c_dim = 21\n",
    "gridSize = 100\n",
    "z_dim = 5\n",
    "\n",
    "bs = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.AttentionDataset import AttentionDataset\n",
    "\n",
    "train_loader = DataLoader(AttentionDataset(np_file_data = 'data/NarrowPassage/narrowDataOcc100.npz',\n",
    "                            sample_dim = X_dim, \n",
    "                            condition_dim = c_dim,\n",
    "                            gridSize = gridSize,\n",
    "                            train = True),\n",
    "                         batch_size = bs, shuffle=True)\n",
    "test_loader = DataLoader(AttentionDataset(np_file_data = 'data/NarrowPassage/narrowDataOcc100.npz',\n",
    "                            sample_dim = X_dim, \n",
    "                            condition_dim = c_dim,\n",
    "                            gridSize = gridSize,\n",
    "                            train = False),\n",
    "                          batch_size = bs, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class convVAE(nn.Module):\n",
    "    def __init__(self, sample_size, grid_size, cnnout_size, encoder_layer_sizes, latent_size, decoder_layer_sizes):\n",
    "        super(convVAE, self).__init__()\n",
    "\n",
    "        assert type(encoder_layer_sizes) == list\n",
    "        assert type(latent_size) == int\n",
    "        assert type(decoder_layer_sizes) == list\n",
    "        \n",
    "        self.latent_size = latent_size\n",
    "        self.condnn = CondNN(sample_size, grid_size, cnnout_size)\n",
    "        self.encoder = Encoder(sample_size * 3 + cnnout_size, encoder_layer_sizes, latent_size)\n",
    "        self.decoder = Decoder(latent_size + sample_size * 2 + cnnout_size, decoder_layer_sizes, sample_size)\n",
    "\n",
    "    def cnn(self,startend, occ):\n",
    "        return self.cnn(startend, occ)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def decode(self, x):\n",
    "        return self.decoder(x)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x, startend, occ):\n",
    "        c = self.condnn(startend, occ)\n",
    "        mu, logvar = self.encode(torch.cat((x, c), dim=-1))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(torch.cat((z, c), dim=-1)), mu, logvar\n",
    "    \n",
    "    def inference(self, startend, occ, num_viz):\n",
    "        c = self.condnn(startend, occ)\n",
    "        z = torch.randn(num_viz, self.latent_size, device = c.device)\n",
    "        return self.decode(torch.cat((z, c), dim=-1))\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, layer_sizes, latent_size):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        layer_sizes = [input_size] + layer_sizes\n",
    "        modules = []\n",
    "        for i, (in_size, out_size) in enumerate(zip(layer_sizes[:-1], layer_sizes[1:])):\n",
    "            modules.append(nn.Linear(in_size, out_size))\n",
    "            modules.append(nn.ReLU())\n",
    "#             modules.append(nn.Dropout(p=0.5))\n",
    "\n",
    "        self.sequential = nn.Sequential(*modules)\n",
    "        self.linear_means = nn.Linear(layer_sizes[-1], latent_size)\n",
    "        self.linear_log_var = nn.Linear(layer_sizes[-1], latent_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sequential(x)\n",
    "        means = self.linear_means(x)\n",
    "        log_vars = self.linear_log_var(x)\n",
    "        return means, log_vars\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, layer_sizes, sample_size):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        layer_sizes = [input_size] + layer_sizes\n",
    "        modules = []\n",
    "        for i, (in_size, out_size) in enumerate(zip(layer_sizes[:-1], layer_sizes[1:])):\n",
    "            modules.append(nn.Linear(in_size, out_size))\n",
    "            modules.append(nn.ReLU())\n",
    "#             modules.append(nn.Dropout(p=0.5))\n",
    "        modules.append(nn.Linear(layer_sizes[-1], sample_size))\n",
    "\n",
    "        self.sequential = nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sequential(x)\n",
    "\n",
    "\n",
    "class CondNN(nn.Module):\n",
    "    def __init__(self, sampleSize, gridSize, outSize):\n",
    "        super(CondNN, self).__init__()\n",
    "        self.sampleSize = sampleSize\n",
    "        self.gridSize = gridSize\n",
    "        self.cnn = nn.Sequential(\n",
    "                    nn.Conv2d(1, 6, 5, padding=(2,2)),\n",
    "                    nn.MaxPool2d(3,3),\n",
    "                    nn.Conv2d(6, 16, 5, padding=(2,2)),\n",
    "                    nn.MaxPool2d(2,2),\n",
    "                    nn.Conv2d(16, 48, 5, padding=(2,2)),\n",
    "                    nn.AdaptiveAvgPool2d((11, 11)))\n",
    "        self.fc1 = nn.Linear(48 * 11 * 11, 512)\n",
    "        self.fc2 = nn.Linear(512, outSize)\n",
    "\n",
    "    def forward(self, startend, occ):\n",
    "        occ = self.cnn(occ)\n",
    "        occ = occ.view(-1, 48 * 11 * 11)\n",
    "        occ = F.relu(self.fc1(occ))\n",
    "        occ = F.relu(self.fc2(occ))\n",
    "        x = torch.cat((occ, startend), dim=-1)\n",
    "#         x = F.relu(self.fc2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convVAE(\n",
      "  (condnn): CondNN(\n",
      "    (cnn): Sequential(\n",
      "      (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "      (1): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (4): Conv2d(16, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "      (5): AdaptiveAvgPool2d(output_size=(11, 11))\n",
      "    )\n",
      "    (fc1): Linear(in_features=5808, out_features=512, bias=True)\n",
      "    (fc2): Linear(in_features=512, out_features=9, bias=True)\n",
      "  )\n",
      "  (encoder): Encoder(\n",
      "    (sequential): Sequential(\n",
      "      (0): Linear(in_features=27, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=1024, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "    (linear_means): Linear(in_features=512, out_features=5, bias=True)\n",
      "    (linear_log_var): Linear(in_features=512, out_features=5, bias=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (sequential): Sequential(\n",
      "      (0): Linear(in_features=26, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=1024, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (5): ReLU()\n",
      "      (6): Linear(in_features=512, out_features=6, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = convVAE(sample_size = X_dim, \n",
    "                  grid_size = gridSize, \n",
    "                  cnnout_size = 9,\n",
    "                  encoder_layer_sizes = [512,1024,512], \n",
    "                  latent_size = z_dim, \n",
    "                  decoder_layer_sizes = [512,1024,512]).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(recon_x, x, w, mean, log_var):\n",
    "    MSE = torch.mean((w.expand_as(x) * (recon_x-x)**2))\n",
    "    KLD = - 0.002 * torch.mean(torch.sum(1 + log_var - mean.pow(2) - log_var.exp(), 1))\n",
    "    return MSE + KLD, MSE\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    mse_loss = 0\n",
    "    w = torch.tensor([1, 1, 1, 0.5, 0.5, 0.5], dtype=torch.float).to(device)\n",
    "    for batch_idx, (sample, _, startend, occ) in enumerate(train_loader):\n",
    "        \n",
    "        sample, startend, occ = sample.to(device), startend.to(device), occ.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(sample, startend, occ)\n",
    "        loss, mse= loss_fn(recon_batch, sample, w, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        mse_loss += mse.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(sample), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item()))\n",
    "\n",
    "    epoch_loss = train_loss * len(sample) / len(train_loader.dataset)\n",
    "    epoch_mse = mse_loss * len(sample) / len(train_loader.dataset)\n",
    "    print('====> Epoch: {} Average loss: {:.7f}'.format(\n",
    "          epoch, epoch_loss))\n",
    "    return epoch, epoch_loss, epoch_mse\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    mse_loss = 0\n",
    "    w = torch.tensor([1, 1, 1, 0.5, 0.5, 0.5], dtype=torch.float).to(device)\n",
    "    for batch_idx, (sample, _, startend, occ) in enumerate(test_loader):\n",
    "        \n",
    "        sample, startend, occ = sample.to(device), startend.to(device), occ.to(device)\n",
    "        recon_batch, mu, logvar = model(sample, startend, occ)\n",
    "        loss, mse= loss_fn(recon_batch, sample, w, mu, logvar)\n",
    "        test_loss += loss.item()\n",
    "        mse_loss += mse.item()\n",
    "\n",
    "    epoch_loss = test_loss * len(sample) / len(test_loader.dataset)\n",
    "    epoch_mse = mse_loss * len(sample) / len(test_loader.dataset)\n",
    "    print('====> Epoch: {} Average test loss: {:.7f}'.format(\n",
    "          epoch, epoch_loss))\n",
    "    return epoch, epoch_loss, epoch_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Setting up a new session...\n"
     ]
    }
   ],
   "source": [
    "epoch = 0\n",
    "# shell python -m visdom.server\n",
    "from visdom import Visdom\n",
    "vis = Visdom(env='cnn_0.002_100*100')\n",
    "\n",
    "loss_window = vis.line(\n",
    "    Y=torch.zeros((1)).cpu(),\n",
    "    X=torch.zeros((1)).cpu(),\n",
    "    opts=dict(xlabel='epoch',ylabel='Loss',title='training loss',legend=['loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/66984 (0%)]\tLoss: 0.243439\n",
      "Train Epoch: 0 [25600/66984 (38%)]\tLoss: 0.036980\n",
      "Train Epoch: 0 [51200/66984 (76%)]\tLoss: 0.031420\n",
      "====> Epoch: 0 Average loss: 0.0282066\n",
      "Train Epoch: 1 [0/66984 (0%)]\tLoss: 0.029956\n",
      "Train Epoch: 1 [25600/66984 (38%)]\tLoss: 0.027781\n",
      "Train Epoch: 1 [51200/66984 (76%)]\tLoss: 0.027566\n",
      "====> Epoch: 1 Average loss: 0.0185148\n",
      "Train Epoch: 2 [0/66984 (0%)]\tLoss: 0.026860\n",
      "Train Epoch: 2 [25600/66984 (38%)]\tLoss: 0.025321\n",
      "Train Epoch: 2 [51200/66984 (76%)]\tLoss: 0.026131\n",
      "====> Epoch: 2 Average loss: 0.0171411\n",
      "Train Epoch: 3 [0/66984 (0%)]\tLoss: 0.025443\n",
      "Train Epoch: 3 [25600/66984 (38%)]\tLoss: 0.024906\n",
      "Train Epoch: 3 [51200/66984 (76%)]\tLoss: 0.024216\n",
      "====> Epoch: 3 Average loss: 0.0164511\n",
      "Train Epoch: 4 [0/66984 (0%)]\tLoss: 0.024705\n",
      "Train Epoch: 4 [25600/66984 (38%)]\tLoss: 0.024058\n",
      "Train Epoch: 4 [51200/66984 (76%)]\tLoss: 0.024319\n",
      "====> Epoch: 4 Average loss: 0.0160205\n",
      "Train Epoch: 5 [0/66984 (0%)]\tLoss: 0.023544\n",
      "Train Epoch: 5 [25600/66984 (38%)]\tLoss: 0.023952\n",
      "Train Epoch: 5 [51200/66984 (76%)]\tLoss: 0.023329\n",
      "====> Epoch: 5 Average loss: 0.0157069\n",
      "Train Epoch: 6 [0/66984 (0%)]\tLoss: 0.023980\n",
      "Train Epoch: 6 [25600/66984 (38%)]\tLoss: 0.023147\n",
      "Train Epoch: 6 [51200/66984 (76%)]\tLoss: 0.023857\n",
      "====> Epoch: 6 Average loss: 0.0155346\n",
      "Train Epoch: 7 [0/66984 (0%)]\tLoss: 0.023315\n",
      "Train Epoch: 7 [25600/66984 (38%)]\tLoss: 0.024607\n",
      "Train Epoch: 7 [51200/66984 (76%)]\tLoss: 0.023491\n",
      "====> Epoch: 7 Average loss: 0.0153503\n",
      "Train Epoch: 8 [0/66984 (0%)]\tLoss: 0.024775\n",
      "Train Epoch: 8 [25600/66984 (38%)]\tLoss: 0.024258\n",
      "Train Epoch: 8 [51200/66984 (76%)]\tLoss: 0.024611\n",
      "====> Epoch: 8 Average loss: 0.0151696\n",
      "Train Epoch: 9 [0/66984 (0%)]\tLoss: 0.023306\n",
      "Train Epoch: 9 [25600/66984 (38%)]\tLoss: 0.022969\n",
      "Train Epoch: 9 [51200/66984 (76%)]\tLoss: 0.022719\n",
      "====> Epoch: 9 Average loss: 0.0150323\n",
      "Train Epoch: 10 [0/66984 (0%)]\tLoss: 0.022399\n",
      "Train Epoch: 10 [25600/66984 (38%)]\tLoss: 0.022028\n",
      "Train Epoch: 10 [51200/66984 (76%)]\tLoss: 0.023292\n",
      "====> Epoch: 10 Average loss: 0.0148762\n",
      "Train Epoch: 11 [0/66984 (0%)]\tLoss: 0.022832\n",
      "Train Epoch: 11 [25600/66984 (38%)]\tLoss: 0.023119\n",
      "Train Epoch: 11 [51200/66984 (76%)]\tLoss: 0.022832\n",
      "====> Epoch: 11 Average loss: 0.0148464\n",
      "Train Epoch: 12 [0/66984 (0%)]\tLoss: 0.022483\n",
      "Train Epoch: 12 [25600/66984 (38%)]\tLoss: 0.021826\n",
      "Train Epoch: 12 [51200/66984 (76%)]\tLoss: 0.022670\n",
      "====> Epoch: 12 Average loss: 0.0147575\n",
      "Train Epoch: 13 [0/66984 (0%)]\tLoss: 0.022381\n",
      "Train Epoch: 13 [25600/66984 (38%)]\tLoss: 0.021741\n",
      "Train Epoch: 13 [51200/66984 (76%)]\tLoss: 0.022056\n",
      "====> Epoch: 13 Average loss: 0.0146145\n",
      "Train Epoch: 14 [0/66984 (0%)]\tLoss: 0.021519\n",
      "Train Epoch: 14 [25600/66984 (38%)]\tLoss: 0.021387\n",
      "Train Epoch: 14 [51200/66984 (76%)]\tLoss: 0.022389\n",
      "====> Epoch: 14 Average loss: 0.0145169\n",
      "Train Epoch: 15 [0/66984 (0%)]\tLoss: 0.021928\n",
      "Train Epoch: 15 [25600/66984 (38%)]\tLoss: 0.020732\n",
      "Train Epoch: 15 [51200/66984 (76%)]\tLoss: 0.022637\n",
      "====> Epoch: 15 Average loss: 0.0144748\n",
      "Train Epoch: 16 [0/66984 (0%)]\tLoss: 0.021751\n",
      "Train Epoch: 16 [25600/66984 (38%)]\tLoss: 0.021427\n",
      "Train Epoch: 16 [51200/66984 (76%)]\tLoss: 0.021958\n",
      "====> Epoch: 16 Average loss: 0.0143882\n",
      "Train Epoch: 17 [0/66984 (0%)]\tLoss: 0.021529\n",
      "Train Epoch: 17 [25600/66984 (38%)]\tLoss: 0.021696\n",
      "Train Epoch: 17 [51200/66984 (76%)]\tLoss: 0.021654\n",
      "====> Epoch: 17 Average loss: 0.0143786\n",
      "Train Epoch: 18 [0/66984 (0%)]\tLoss: 0.022279\n",
      "Train Epoch: 18 [25600/66984 (38%)]\tLoss: 0.020656\n",
      "Train Epoch: 18 [51200/66984 (76%)]\tLoss: 0.022319\n",
      "====> Epoch: 18 Average loss: 0.0142417\n",
      "Train Epoch: 19 [0/66984 (0%)]\tLoss: 0.022242\n",
      "Train Epoch: 19 [25600/66984 (38%)]\tLoss: 0.021329\n",
      "Train Epoch: 19 [51200/66984 (76%)]\tLoss: 0.020865\n",
      "====> Epoch: 19 Average loss: 0.0142045\n",
      "Train Epoch: 20 [0/66984 (0%)]\tLoss: 0.020914\n",
      "Train Epoch: 20 [25600/66984 (38%)]\tLoss: 0.021809\n",
      "Train Epoch: 20 [51200/66984 (76%)]\tLoss: 0.021773\n",
      "====> Epoch: 20 Average loss: 0.0142095\n",
      "Train Epoch: 21 [0/66984 (0%)]\tLoss: 0.021343\n",
      "Train Epoch: 21 [25600/66984 (38%)]\tLoss: 0.022684\n",
      "Train Epoch: 21 [51200/66984 (76%)]\tLoss: 0.021421\n",
      "====> Epoch: 21 Average loss: 0.0141320\n",
      "Train Epoch: 22 [0/66984 (0%)]\tLoss: 0.021911\n",
      "Train Epoch: 22 [25600/66984 (38%)]\tLoss: 0.021345\n",
      "Train Epoch: 22 [51200/66984 (76%)]\tLoss: 0.021465\n",
      "====> Epoch: 22 Average loss: 0.0140829\n",
      "Train Epoch: 23 [0/66984 (0%)]\tLoss: 0.020826\n",
      "Train Epoch: 23 [25600/66984 (38%)]\tLoss: 0.022006\n",
      "Train Epoch: 23 [51200/66984 (76%)]\tLoss: 0.022022\n",
      "====> Epoch: 23 Average loss: 0.0140249\n",
      "Train Epoch: 24 [0/66984 (0%)]\tLoss: 0.020269\n",
      "Train Epoch: 24 [25600/66984 (38%)]\tLoss: 0.021091\n",
      "Train Epoch: 24 [51200/66984 (76%)]\tLoss: 0.020178\n",
      "====> Epoch: 24 Average loss: 0.0139374\n",
      "Train Epoch: 25 [0/66984 (0%)]\tLoss: 0.021077\n",
      "Train Epoch: 25 [25600/66984 (38%)]\tLoss: 0.020294\n",
      "Train Epoch: 25 [51200/66984 (76%)]\tLoss: 0.021416\n",
      "====> Epoch: 25 Average loss: 0.0138813\n",
      "Train Epoch: 26 [0/66984 (0%)]\tLoss: 0.021390\n",
      "Train Epoch: 26 [25600/66984 (38%)]\tLoss: 0.021563\n",
      "Train Epoch: 26 [51200/66984 (76%)]\tLoss: 0.021525\n",
      "====> Epoch: 26 Average loss: 0.0138730\n",
      "Train Epoch: 27 [0/66984 (0%)]\tLoss: 0.020922\n",
      "Train Epoch: 27 [25600/66984 (38%)]\tLoss: 0.021032\n",
      "Train Epoch: 27 [51200/66984 (76%)]\tLoss: 0.021390\n",
      "====> Epoch: 27 Average loss: 0.0138500\n",
      "Train Epoch: 28 [0/66984 (0%)]\tLoss: 0.020188\n",
      "Train Epoch: 28 [25600/66984 (38%)]\tLoss: 0.020644\n",
      "Train Epoch: 28 [51200/66984 (76%)]\tLoss: 0.020652\n",
      "====> Epoch: 28 Average loss: 0.0138025\n",
      "Train Epoch: 29 [0/66984 (0%)]\tLoss: 0.020200\n",
      "Train Epoch: 29 [25600/66984 (38%)]\tLoss: 0.021954\n",
      "Train Epoch: 29 [51200/66984 (76%)]\tLoss: 0.021108\n",
      "====> Epoch: 29 Average loss: 0.0136990\n",
      "Train Epoch: 30 [0/66984 (0%)]\tLoss: 0.020892\n",
      "Train Epoch: 30 [25600/66984 (38%)]\tLoss: 0.019828\n",
      "Train Epoch: 30 [51200/66984 (76%)]\tLoss: 0.020733\n",
      "====> Epoch: 30 Average loss: 0.0136761\n",
      "Train Epoch: 31 [0/66984 (0%)]\tLoss: 0.020997\n",
      "Train Epoch: 31 [25600/66984 (38%)]\tLoss: 0.020666\n",
      "Train Epoch: 31 [51200/66984 (76%)]\tLoss: 0.021358\n",
      "====> Epoch: 31 Average loss: 0.0136917\n",
      "Train Epoch: 32 [0/66984 (0%)]\tLoss: 0.021547\n",
      "Train Epoch: 32 [25600/66984 (38%)]\tLoss: 0.020641\n",
      "Train Epoch: 32 [51200/66984 (76%)]\tLoss: 0.020611\n",
      "====> Epoch: 32 Average loss: 0.0136557\n",
      "Train Epoch: 33 [0/66984 (0%)]\tLoss: 0.020571\n",
      "Train Epoch: 33 [25600/66984 (38%)]\tLoss: 0.020126\n",
      "Train Epoch: 33 [51200/66984 (76%)]\tLoss: 0.022239\n",
      "====> Epoch: 33 Average loss: 0.0136139\n",
      "Train Epoch: 34 [0/66984 (0%)]\tLoss: 0.019746\n",
      "Train Epoch: 34 [25600/66984 (38%)]\tLoss: 0.020690\n",
      "Train Epoch: 34 [51200/66984 (76%)]\tLoss: 0.020048\n",
      "====> Epoch: 34 Average loss: 0.0135450\n",
      "Train Epoch: 35 [0/66984 (0%)]\tLoss: 0.020685\n",
      "Train Epoch: 35 [25600/66984 (38%)]\tLoss: 0.019893\n",
      "Train Epoch: 35 [51200/66984 (76%)]\tLoss: 0.020360\n",
      "====> Epoch: 35 Average loss: 0.0135734\n",
      "Train Epoch: 36 [0/66984 (0%)]\tLoss: 0.020558\n",
      "Train Epoch: 36 [25600/66984 (38%)]\tLoss: 0.021202\n",
      "Train Epoch: 36 [51200/66984 (76%)]\tLoss: 0.020470\n",
      "====> Epoch: 36 Average loss: 0.0135197\n",
      "Train Epoch: 37 [0/66984 (0%)]\tLoss: 0.020715\n",
      "Train Epoch: 37 [25600/66984 (38%)]\tLoss: 0.020620\n",
      "Train Epoch: 37 [51200/66984 (76%)]\tLoss: 0.020650\n",
      "====> Epoch: 37 Average loss: 0.0134920\n",
      "Train Epoch: 38 [0/66984 (0%)]\tLoss: 0.020150\n",
      "Train Epoch: 38 [25600/66984 (38%)]\tLoss: 0.020714\n",
      "Train Epoch: 38 [51200/66984 (76%)]\tLoss: 0.020686\n",
      "====> Epoch: 38 Average loss: 0.0134206\n",
      "Train Epoch: 39 [0/66984 (0%)]\tLoss: 0.020826\n",
      "Train Epoch: 39 [25600/66984 (38%)]\tLoss: 0.021582\n",
      "Train Epoch: 39 [51200/66984 (76%)]\tLoss: 0.021675\n",
      "====> Epoch: 39 Average loss: 0.0134522\n",
      "Train Epoch: 40 [0/66984 (0%)]\tLoss: 0.020623\n",
      "Train Epoch: 40 [25600/66984 (38%)]\tLoss: 0.018825\n",
      "Train Epoch: 40 [51200/66984 (76%)]\tLoss: 0.019446\n",
      "====> Epoch: 40 Average loss: 0.0133752\n",
      "Train Epoch: 41 [0/66984 (0%)]\tLoss: 0.020571\n",
      "Train Epoch: 41 [25600/66984 (38%)]\tLoss: 0.020031\n",
      "Train Epoch: 41 [51200/66984 (76%)]\tLoss: 0.020531\n",
      "====> Epoch: 41 Average loss: 0.0133643\n",
      "Train Epoch: 42 [0/66984 (0%)]\tLoss: 0.019938\n",
      "Train Epoch: 42 [25600/66984 (38%)]\tLoss: 0.020291\n",
      "Train Epoch: 42 [51200/66984 (76%)]\tLoss: 0.020097\n",
      "====> Epoch: 42 Average loss: 0.0133774\n",
      "Train Epoch: 43 [0/66984 (0%)]\tLoss: 0.019527\n",
      "Train Epoch: 43 [25600/66984 (38%)]\tLoss: 0.020249\n",
      "Train Epoch: 43 [51200/66984 (76%)]\tLoss: 0.020173\n",
      "====> Epoch: 43 Average loss: 0.0133288\n",
      "Train Epoch: 44 [0/66984 (0%)]\tLoss: 0.021046\n",
      "Train Epoch: 44 [25600/66984 (38%)]\tLoss: 0.020338\n",
      "Train Epoch: 44 [51200/66984 (76%)]\tLoss: 0.021965\n",
      "====> Epoch: 44 Average loss: 0.0133007\n",
      "Train Epoch: 45 [0/66984 (0%)]\tLoss: 0.019770\n",
      "Train Epoch: 45 [25600/66984 (38%)]\tLoss: 0.019234\n",
      "Train Epoch: 45 [51200/66984 (76%)]\tLoss: 0.020295\n",
      "====> Epoch: 45 Average loss: 0.0132848\n",
      "Train Epoch: 46 [0/66984 (0%)]\tLoss: 0.020621\n",
      "Train Epoch: 46 [25600/66984 (38%)]\tLoss: 0.019968\n",
      "Train Epoch: 46 [51200/66984 (76%)]\tLoss: 0.019966\n",
      "====> Epoch: 46 Average loss: 0.0132168\n",
      "Train Epoch: 47 [0/66984 (0%)]\tLoss: 0.019772\n",
      "Train Epoch: 47 [25600/66984 (38%)]\tLoss: 0.020163\n",
      "Train Epoch: 47 [51200/66984 (76%)]\tLoss: 0.019565\n",
      "====> Epoch: 47 Average loss: 0.0132552\n",
      "Train Epoch: 48 [0/66984 (0%)]\tLoss: 0.019236\n",
      "Train Epoch: 48 [25600/66984 (38%)]\tLoss: 0.020562\n",
      "Train Epoch: 48 [51200/66984 (76%)]\tLoss: 0.020736\n",
      "====> Epoch: 48 Average loss: 0.0131545\n",
      "Train Epoch: 49 [0/66984 (0%)]\tLoss: 0.019819\n",
      "Train Epoch: 49 [25600/66984 (38%)]\tLoss: 0.019348\n",
      "Train Epoch: 49 [51200/66984 (76%)]\tLoss: 0.020136\n",
      "====> Epoch: 49 Average loss: 0.0131438\n",
      "Train Epoch: 50 [0/66984 (0%)]\tLoss: 0.020100\n",
      "Train Epoch: 50 [25600/66984 (38%)]\tLoss: 0.019190\n",
      "Train Epoch: 50 [51200/66984 (76%)]\tLoss: 0.020914\n",
      "====> Epoch: 50 Average loss: 0.0131211\n",
      "Train Epoch: 51 [0/66984 (0%)]\tLoss: 0.019855\n",
      "Train Epoch: 51 [25600/66984 (38%)]\tLoss: 0.019349\n",
      "Train Epoch: 51 [51200/66984 (76%)]\tLoss: 0.020044\n",
      "====> Epoch: 51 Average loss: 0.0131099\n",
      "Train Epoch: 52 [0/66984 (0%)]\tLoss: 0.019700\n",
      "Train Epoch: 52 [25600/66984 (38%)]\tLoss: 0.019912\n",
      "Train Epoch: 52 [51200/66984 (76%)]\tLoss: 0.019850\n",
      "====> Epoch: 52 Average loss: 0.0130401\n",
      "Train Epoch: 53 [0/66984 (0%)]\tLoss: 0.019489\n",
      "Train Epoch: 53 [25600/66984 (38%)]\tLoss: 0.020120\n",
      "Train Epoch: 53 [51200/66984 (76%)]\tLoss: 0.020290\n",
      "====> Epoch: 53 Average loss: 0.0130789\n",
      "Train Epoch: 54 [0/66984 (0%)]\tLoss: 0.020572\n",
      "Train Epoch: 54 [25600/66984 (38%)]\tLoss: 0.019550\n",
      "Train Epoch: 54 [51200/66984 (76%)]\tLoss: 0.020699\n",
      "====> Epoch: 54 Average loss: 0.0130588\n",
      "Train Epoch: 55 [0/66984 (0%)]\tLoss: 0.019015\n",
      "Train Epoch: 55 [25600/66984 (38%)]\tLoss: 0.019844\n",
      "Train Epoch: 55 [51200/66984 (76%)]\tLoss: 0.020407\n",
      "====> Epoch: 55 Average loss: 0.0129987\n",
      "Train Epoch: 56 [0/66984 (0%)]\tLoss: 0.019144\n",
      "Train Epoch: 56 [25600/66984 (38%)]\tLoss: 0.020569\n",
      "Train Epoch: 56 [51200/66984 (76%)]\tLoss: 0.019246\n",
      "====> Epoch: 56 Average loss: 0.0129689\n",
      "Train Epoch: 57 [0/66984 (0%)]\tLoss: 0.019867\n",
      "Train Epoch: 57 [25600/66984 (38%)]\tLoss: 0.019231\n",
      "Train Epoch: 57 [51200/66984 (76%)]\tLoss: 0.020043\n",
      "====> Epoch: 57 Average loss: 0.0129596\n",
      "Train Epoch: 58 [0/66984 (0%)]\tLoss: 0.020334\n",
      "Train Epoch: 58 [25600/66984 (38%)]\tLoss: 0.019563\n",
      "Train Epoch: 58 [51200/66984 (76%)]\tLoss: 0.019681\n",
      "====> Epoch: 58 Average loss: 0.0129173\n",
      "Train Epoch: 59 [0/66984 (0%)]\tLoss: 0.019714\n",
      "Train Epoch: 59 [25600/66984 (38%)]\tLoss: 0.019318\n",
      "Train Epoch: 59 [51200/66984 (76%)]\tLoss: 0.020309\n",
      "====> Epoch: 59 Average loss: 0.0129092\n",
      "Train Epoch: 60 [0/66984 (0%)]\tLoss: 0.019577\n",
      "Train Epoch: 60 [25600/66984 (38%)]\tLoss: 0.019573\n",
      "Train Epoch: 60 [51200/66984 (76%)]\tLoss: 0.019046\n",
      "====> Epoch: 60 Average loss: 0.0128257\n",
      "Train Epoch: 61 [0/66984 (0%)]\tLoss: 0.020349\n",
      "Train Epoch: 61 [25600/66984 (38%)]\tLoss: 0.019247\n",
      "Train Epoch: 61 [51200/66984 (76%)]\tLoss: 0.019768\n",
      "====> Epoch: 61 Average loss: 0.0128195\n",
      "Train Epoch: 62 [0/66984 (0%)]\tLoss: 0.019181\n",
      "Train Epoch: 62 [25600/66984 (38%)]\tLoss: 0.020167\n",
      "Train Epoch: 62 [51200/66984 (76%)]\tLoss: 0.018938\n",
      "====> Epoch: 62 Average loss: 0.0128265\n",
      "Train Epoch: 63 [0/66984 (0%)]\tLoss: 0.019587\n",
      "Train Epoch: 63 [25600/66984 (38%)]\tLoss: 0.019362\n",
      "Train Epoch: 63 [51200/66984 (76%)]\tLoss: 0.019817\n",
      "====> Epoch: 63 Average loss: 0.0127991\n",
      "Train Epoch: 64 [0/66984 (0%)]\tLoss: 0.020015\n",
      "Train Epoch: 64 [25600/66984 (38%)]\tLoss: 0.019829\n",
      "Train Epoch: 64 [51200/66984 (76%)]\tLoss: 0.019376\n",
      "====> Epoch: 64 Average loss: 0.0128014\n",
      "Train Epoch: 65 [0/66984 (0%)]\tLoss: 0.019642\n",
      "Train Epoch: 65 [25600/66984 (38%)]\tLoss: 0.020132\n",
      "Train Epoch: 65 [51200/66984 (76%)]\tLoss: 0.020570\n",
      "====> Epoch: 65 Average loss: 0.0127625\n",
      "Train Epoch: 66 [0/66984 (0%)]\tLoss: 0.018683\n",
      "Train Epoch: 66 [25600/66984 (38%)]\tLoss: 0.020424\n",
      "Train Epoch: 66 [51200/66984 (76%)]\tLoss: 0.019175\n",
      "====> Epoch: 66 Average loss: 0.0127265\n",
      "Train Epoch: 67 [0/66984 (0%)]\tLoss: 0.019033\n",
      "Train Epoch: 67 [25600/66984 (38%)]\tLoss: 0.020074\n",
      "Train Epoch: 67 [51200/66984 (76%)]\tLoss: 0.019671\n",
      "====> Epoch: 67 Average loss: 0.0126947\n",
      "Train Epoch: 68 [0/66984 (0%)]\tLoss: 0.019043\n",
      "Train Epoch: 68 [25600/66984 (38%)]\tLoss: 0.020193\n",
      "Train Epoch: 68 [51200/66984 (76%)]\tLoss: 0.020432\n",
      "====> Epoch: 68 Average loss: 0.0126893\n",
      "Train Epoch: 69 [0/66984 (0%)]\tLoss: 0.019239\n",
      "Train Epoch: 69 [25600/66984 (38%)]\tLoss: 0.018573\n",
      "Train Epoch: 69 [51200/66984 (76%)]\tLoss: 0.018461\n",
      "====> Epoch: 69 Average loss: 0.0126484\n",
      "Train Epoch: 70 [0/66984 (0%)]\tLoss: 0.018705\n",
      "Train Epoch: 70 [25600/66984 (38%)]\tLoss: 0.019858\n",
      "Train Epoch: 70 [51200/66984 (76%)]\tLoss: 0.019576\n",
      "====> Epoch: 70 Average loss: 0.0126455\n",
      "Train Epoch: 71 [0/66984 (0%)]\tLoss: 0.018961\n",
      "Train Epoch: 71 [25600/66984 (38%)]\tLoss: 0.019772\n",
      "Train Epoch: 71 [51200/66984 (76%)]\tLoss: 0.019106\n",
      "====> Epoch: 71 Average loss: 0.0126287\n",
      "Train Epoch: 72 [0/66984 (0%)]\tLoss: 0.018846\n",
      "Train Epoch: 72 [25600/66984 (38%)]\tLoss: 0.018457\n",
      "Train Epoch: 72 [51200/66984 (76%)]\tLoss: 0.018646\n",
      "====> Epoch: 72 Average loss: 0.0125978\n",
      "Train Epoch: 73 [0/66984 (0%)]\tLoss: 0.019385\n",
      "Train Epoch: 73 [25600/66984 (38%)]\tLoss: 0.019429\n",
      "Train Epoch: 73 [51200/66984 (76%)]\tLoss: 0.018834\n",
      "====> Epoch: 73 Average loss: 0.0125919\n",
      "Train Epoch: 74 [0/66984 (0%)]\tLoss: 0.019133\n",
      "Train Epoch: 74 [25600/66984 (38%)]\tLoss: 0.019477\n",
      "Train Epoch: 74 [51200/66984 (76%)]\tLoss: 0.018326\n",
      "====> Epoch: 74 Average loss: 0.0125451\n",
      "Train Epoch: 75 [0/66984 (0%)]\tLoss: 0.019353\n",
      "Train Epoch: 75 [25600/66984 (38%)]\tLoss: 0.018711\n",
      "Train Epoch: 75 [51200/66984 (76%)]\tLoss: 0.019952\n",
      "====> Epoch: 75 Average loss: 0.0125203\n",
      "Train Epoch: 76 [0/66984 (0%)]\tLoss: 0.019912\n",
      "Train Epoch: 76 [25600/66984 (38%)]\tLoss: 0.018625\n",
      "Train Epoch: 76 [51200/66984 (76%)]\tLoss: 0.020121\n",
      "====> Epoch: 76 Average loss: 0.0124911\n",
      "Train Epoch: 77 [0/66984 (0%)]\tLoss: 0.019027\n",
      "Train Epoch: 77 [25600/66984 (38%)]\tLoss: 0.018666\n",
      "Train Epoch: 77 [51200/66984 (76%)]\tLoss: 0.019037\n",
      "====> Epoch: 77 Average loss: 0.0124683\n",
      "Train Epoch: 78 [0/66984 (0%)]\tLoss: 0.018400\n",
      "Train Epoch: 78 [25600/66984 (38%)]\tLoss: 0.019327\n",
      "Train Epoch: 78 [51200/66984 (76%)]\tLoss: 0.019458\n",
      "====> Epoch: 78 Average loss: 0.0125076\n",
      "Train Epoch: 79 [0/66984 (0%)]\tLoss: 0.018190\n",
      "Train Epoch: 79 [25600/66984 (38%)]\tLoss: 0.018765\n",
      "Train Epoch: 79 [51200/66984 (76%)]\tLoss: 0.019526\n",
      "====> Epoch: 79 Average loss: 0.0124657\n",
      "Train Epoch: 80 [0/66984 (0%)]\tLoss: 0.018417\n",
      "Train Epoch: 80 [25600/66984 (38%)]\tLoss: 0.018032\n",
      "Train Epoch: 80 [51200/66984 (76%)]\tLoss: 0.018180\n",
      "====> Epoch: 80 Average loss: 0.0124341\n",
      "Train Epoch: 81 [0/66984 (0%)]\tLoss: 0.018692\n",
      "Train Epoch: 81 [25600/66984 (38%)]\tLoss: 0.018603\n",
      "Train Epoch: 81 [51200/66984 (76%)]\tLoss: 0.018898\n",
      "====> Epoch: 81 Average loss: 0.0124081\n",
      "Train Epoch: 82 [0/66984 (0%)]\tLoss: 0.019240\n",
      "Train Epoch: 82 [25600/66984 (38%)]\tLoss: 0.018628\n",
      "Train Epoch: 82 [51200/66984 (76%)]\tLoss: 0.019113\n",
      "====> Epoch: 82 Average loss: 0.0124032\n",
      "Train Epoch: 83 [0/66984 (0%)]\tLoss: 0.018350\n",
      "Train Epoch: 83 [25600/66984 (38%)]\tLoss: 0.018546\n",
      "Train Epoch: 83 [51200/66984 (76%)]\tLoss: 0.018918\n",
      "====> Epoch: 83 Average loss: 0.0123468\n",
      "Train Epoch: 84 [0/66984 (0%)]\tLoss: 0.018884\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-e0f24c287c8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mlog_interval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_mse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepoch_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_window\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'append'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepoch_mse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_window\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'append'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mse_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-0dd3918eaadb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mrecon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstartend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mocc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-6d8979a3a575>\u001b[0m in \u001b[0;36mloss_fn\u001b[0;34m(recon_x, x, w, mean, log_var)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mMSE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrecon_x\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mKLD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.002\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlog_var\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlog_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mMSE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mKLD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMSE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epoch, epoch + 200):\n",
    "    log_interval = 100\n",
    "    epoch, epoch_loss, epoch_mse = train(epoch)\n",
    "    vis.line(X=torch.ones((1,1)).cpu()*epoch,Y=torch.Tensor([epoch_loss]).unsqueeze(0).cpu(),win=loss_window,update='append',name='loss')\n",
    "    vis.line(X=torch.ones((1,1)).cpu()*epoch,Y=torch.Tensor([epoch_mse]).unsqueeze(0).cpu(),win=loss_window,update='append',name='mse_loss')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rong/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type convVAE. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/rong/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type CondNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/rong/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Encoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/rong/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Decoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict()\n",
    "            }, 'checkpoints/cnn100.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "convVAE(\n",
       "  (condnn): CondNN(\n",
       "    (cnn): Sequential(\n",
       "      (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (1): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "      (2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (4): Conv2d(16, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (5): AdaptiveAvgPool2d(output_size=(11, 11))\n",
       "    )\n",
       "    (fc1): Linear(in_features=5808, out_features=512, bias=True)\n",
       "    (fc2): Linear(in_features=512, out_features=9, bias=True)\n",
       "  )\n",
       "  (encoder): Encoder(\n",
       "    (sequential): Sequential(\n",
       "      (0): Linear(in_features=27, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (linear_means): Linear(in_features=512, out_features=5, bias=True)\n",
       "    (linear_log_var): Linear(in_features=512, out_features=5, bias=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (sequential): Sequential(\n",
       "      (0): Linear(in_features=26, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=512, out_features=6, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "model = convVAE(sample_size = X_dim, \n",
    "                  grid_size = gridSize, \n",
    "                  cnnout_size = 9,\n",
    "                  encoder_layer_sizes = [512,1024,512], \n",
    "                  latent_size = z_dim, \n",
    "                  decoder_layer_sizes = [512,1024,512]).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_loader.dataset\n",
    "viz_idx =   torch.randint(0,len(test_data),[1]).item()  \n",
    "\n",
    "# viz_idx = 8712\n",
    "\n",
    "_, con, startend, occ = test_data[viz_idx]\n",
    "startend, occ = torch.tensor(startend), torch.unsqueeze(torch.tensor(occ), 0)\n",
    "model.eval()\n",
    "num_viz = 1000\n",
    "y_viz = model.inference(startend.expand(num_viz, -1).to(device), \n",
    "                        occ.expand(num_viz, -1, -1, -1).to(device), num_viz)\n",
    "\n",
    "y_viz=y_viz.cpu().detach().numpy()\n",
    "occ=occ.cpu().detach().numpy()\n",
    "\n",
    "from utils.NarrowPassage import plotCondition, plotSample, plotSpeed\n",
    "\n",
    "# plotCondition(condition)\n",
    "plotSample(y_viz, con)\n",
    "plotSpeed(y_viz, con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
