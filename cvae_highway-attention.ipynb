{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda:0\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import sqlite3\n",
    "import torch\n",
    "import io\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device\", device)\n",
    "\n",
    "def adapt_array(arr):\n",
    "    out = io.BytesIO()\n",
    "    np.save(out, arr)\n",
    "    out.seek(0)\n",
    "    return sqlite3.Binary(out.read())\n",
    "\n",
    "def convert_array(text):\n",
    "    out = io.BytesIO(text)\n",
    "    out.seek(0)\n",
    "    return np.load(out)\n",
    "\n",
    "def position_reformed(data, startgoal):\n",
    "    datax = (data[0]-startgoal[0])*(3/2) + startx\n",
    "    datay = (data[1]-startgoal[1])*(3/2) + starty\n",
    "    return datax, datay\n",
    "\n",
    "# Converts np.array to TEXT when inserting\n",
    "sqlite3.register_adapter(np.ndarray, adapt_array)\n",
    "# Converts TEXT to np.array when selecting\n",
    "sqlite3.register_converter(\"array\", convert_array)\n",
    "\n",
    "class TrafficDataset(Dataset):\n",
    "    def __init__(self, dbpath, train=True, ratio_test=0.8, num_sce=100, num_data=25):\n",
    "        self.con = sqlite3.connect(dbpath, detect_types=sqlite3.PARSE_DECLTYPES)\n",
    "        self.path = dbpath\n",
    "        self.cur = self.con.cursor()\n",
    "        self.cur.execute(\"select id from highway\")\n",
    "        self.idlist = self.cur.fetchall()\n",
    "        self.cur.execute(\"select data from highway where id = \" +  str(1))\n",
    "        self.dataperow = len(self.cur.fetchone()[0])\n",
    "        numTrain = int(ratio_test * len(self.idlist))\n",
    "        if (train):\n",
    "            self.filelist = self.idlist[:numTrain]\n",
    "        else:\n",
    "            self.filelist = self.idlist[numTrain:]\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.idlist) * self.dataperow\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        rowid = idx // self.dataperow\n",
    "        dataid = idx % self.dataperow\n",
    "        self.cur.execute(\"select id, startgoal, occ, data from highway where id = \" +  str(rowid+1))\n",
    "        results = self.cur.fetchone()\n",
    "        start_goal = results[1].astype(np.single)\n",
    "        observed = results[2].astype(np.single)\n",
    "        data = results[3][dataid].astype(np.single)\n",
    "        \n",
    "        start_goal[4] = start_goal[4] - start_goal[0]\n",
    "        start_goal[5] = start_goal[5] - start_goal[1]\n",
    "        \n",
    "        data[0] = data[0] - start_goal[0]\n",
    "        data[1] = data[1] - start_goal[1]\n",
    "        \n",
    "        start_goal[0] = 0\n",
    "        start_goal[1] = 0\n",
    "        \n",
    "         \n",
    "        sample = {'start_goal': start_goal,\n",
    "                             'observation': observed,\n",
    "                            'data': data}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "dbpath = '/home/rong/disk/database/highway.db'\n",
    "bs = 8\n",
    "train_loader = DataLoader(TrafficDataset(dbpath = dbpath,\n",
    "                            train = True),\n",
    "                         batch_size = bs, shuffle=True, drop_last = True)\n",
    "test_loader = DataLoader(TrafficDataset(dbpath = dbpath,\n",
    "                            train = False),\n",
    "                          batch_size = bs, shuffle=True, drop_last = True)\n",
    "\n",
    "# batch = next(iter(train_loader))\n",
    "# batch['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "\n",
    "class convVAE(nn.Module):\n",
    "    def __init__(self, sample_size, cnnout_size, cond_out_size, encoder_layer_sizes, latent_size, decoder_layer_sizes):\n",
    "        super(convVAE, self).__init__()\n",
    "\n",
    "        assert type(encoder_layer_sizes) == list\n",
    "        assert type(latent_size) == int\n",
    "        assert type(decoder_layer_sizes) == list\n",
    "        \n",
    "        self.latent_size = latent_size\n",
    "        self.condnn = CondNN(sample_size, cnnout_size)\n",
    "        self.encoder = Encoder(sample_size + cond_out_size, encoder_layer_sizes, latent_size)\n",
    "        self.decoder = Decoder(latent_size +cond_out_size, decoder_layer_sizes, sample_size)\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def decode(self, x):\n",
    "        return self.decoder(x)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x, startend, occ):\n",
    "        c, _= self.condnn(startend, occ)\n",
    "#         print(\"x size: \", x.shape)\n",
    "#         print(\"c size\", c.shape)\n",
    "        mu, logvar = self.encode(torch.cat((x, c), dim=1))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(torch.cat((z, c), dim=-1)), mu, logvar\n",
    "    \n",
    "    def inference(self, startend, occ, num_viz):\n",
    "        c, alpha = self.condnn(startend, occ)\n",
    "        z = torch.randn(num_viz, self.latent_size, device = c.device)\n",
    "        return self.decode(torch.cat((z, c), dim=-1)), alpha\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, layer_sizes, latent_size):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        layer_sizes = [input_size] + layer_sizes\n",
    "        modules = []\n",
    "        for i, (in_size, out_size) in enumerate(zip(layer_sizes[:-1], layer_sizes[1:])):\n",
    "            modules.append(nn.Linear(in_size, out_size))\n",
    "            modules.append(nn.ReLU())\n",
    "#             modules.append(nn.Dropout(p=0.5))\n",
    "\n",
    "        self.sequential = nn.Sequential(*modules)\n",
    "        self.linear_means = nn.Linear(layer_sizes[-1], latent_size)\n",
    "        self.linear_log_var = nn.Linear(layer_sizes[-1], latent_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sequential(x)\n",
    "        means = self.linear_means(x)\n",
    "        log_vars = self.linear_log_var(x)\n",
    "        return means, log_vars\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, layer_sizes, sample_size):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        layer_sizes = [input_size] + layer_sizes\n",
    "        modules = []\n",
    "        for i, (in_size, out_size) in enumerate(zip(layer_sizes[:-1], layer_sizes[1:])):\n",
    "            modules.append(nn.Linear(in_size, out_size))\n",
    "            modules.append(nn.ReLU())\n",
    "#             modules.append(nn.Dropout(p=0.5))\n",
    "        modules.append(nn.Linear(layer_sizes[-1], sample_size))\n",
    "\n",
    "        self.sequential = nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sequential(x)\n",
    "\n",
    "\n",
    "class CondNN(nn.Module):\n",
    "    def __init__(self, sampleSize, outSize, encoder_dim=64, attention_dim=64):\n",
    "        super(CondNN, self).__init__()\n",
    "        self.sampleSize = sampleSize\n",
    "        self.cnn = Conv3d(cnn_out_size)\n",
    "        self.Attention = Attention(cnn_encode_dim=encoder_dim, condition_dim=sampleSize * 2, attention_dim=attention_dim) # + 3 for position\n",
    "        self.fc1 = nn.Linear(encoder_dim + sampleSize * 2, outSize)\n",
    "\n",
    "    def forward(self, startend, occ):\n",
    "        cnn_encode = self.cnn(occ)\n",
    "        attention_weighted_encoding, alpha = self.Attention(cnn_encode, startend)\n",
    "        x = torch.cat((attention_weighted_encoding, startend), dim=-1)\n",
    "#         print(\"condnn cated size:\", x.shape)\n",
    "        x = self.fc1(x)\n",
    "        return x, alpha\n",
    "\n",
    "class Conv3d(nn.Module):\n",
    "    def __init__(self, cnn_out_size):\n",
    "        super(Conv3d, self).__init__()\n",
    "\n",
    "        self.adap_pool = nn.AdaptiveAvgPool3d((25, 100, 600))\n",
    "        self.conv_layer1 = self._make_conv_layer(1, 16)\n",
    "        self.conv_layer2 = self._make_conv_layer(16, 32)\n",
    "#         self.conv_layer3 = self._make_conv_layer(64, 124)\n",
    "        self.conv_layer5=nn.Conv3d(32, 64, kernel_size=(1, 3, 3), padding=0)\n",
    "        \n",
    "        self.adap_pool2 = nn.AdaptiveAvgPool3d((6, 10, 60))\n",
    "\n",
    "    def _make_conv_layer(self, in_c, out_c):\n",
    "        conv_layer = nn.Sequential(\n",
    "        nn.Conv3d(in_c, out_c, kernel_size=(2, 3, 3), padding=0),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Conv3d(out_c, out_c, kernel_size=(2, 3, 3), padding=1),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.MaxPool3d((2, 2, 2)),\n",
    "        )\n",
    "        return conv_layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.adap_pool(x)\n",
    "#         print(x.size())\n",
    "        x = self.conv_layer1(x)\n",
    "#         print(x.size())\n",
    "        x = self.conv_layer2(x)\n",
    "#         print(x.size())\n",
    "        x=self.conv_layer5(x)\n",
    "#         print(x.size())\n",
    "        x = self.adap_pool2(x)\n",
    "#         print(x.size())\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, cnn_encode_dim, condition_dim, attention_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.encoder_att = nn.Linear(cnn_encode_dim + 3, attention_dim) #位置信息\n",
    "        self.condition_att = nn.Linear(condition_dim, attention_dim)\n",
    "        self.full_att = nn.Linear(attention_dim, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.position = self.initPosition()\n",
    "    \n",
    "    def initPosition(self):\n",
    "        x = np.linspace(0, 5, 6, dtype='float32')\n",
    "        y = np.linspace(0, 9, 10, dtype='float32')\n",
    "        t = np.linspace(0, 59, 60, dtype='float32')\n",
    "        xv, yv, tv = np.meshgrid(x,y,t)\n",
    "        xv, yv, tv = xv.reshape((1,-1, 1)), yv.reshape((1,-1, 1)), tv.reshape((1,-1, 1))\n",
    "        position = torch.from_numpy(np.concatenate((xv, yv, tv), axis = 2))\n",
    "        \n",
    "        return position\n",
    "        \n",
    "        \n",
    "    def forward(self, encoder_out, condition):\n",
    "        batch_size = encoder_out.size(0)\n",
    "        encoder_dim = encoder_out.size(1)\n",
    "#         print(\"encoder_dim: \", encoder_dim)\n",
    "        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n",
    "        self.position = self.position.to(encoder_out.device)\n",
    "        self.position = self.position.expand(batch_size, self.position.shape[1], self.position.shape[2]) #存疑\n",
    "#         print(\"encoder_out: \", encoder_out.shape)\n",
    "        encoder_out_pos = torch.cat((encoder_out, self.position), dim = 2)\n",
    "#         print(\"cat encoder_out: \", encoder_out.shape)\n",
    "        att1 = self.encoder_att(encoder_out_pos)\n",
    "        att2 = self.condition_att(condition)# 依然不清晰\n",
    "        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)\n",
    "        alpha = self.softmax(att)\n",
    "#         print(\"alpha shape: \", alpha.shape)\n",
    "        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)\n",
    "            \n",
    "        return attention_weighted_encoding, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda:0\n",
      "convVAE(\n",
      "  (condnn): CondNN(\n",
      "    (cnn): Conv3d(\n",
      "      (adap_pool): AdaptiveAvgPool3d(output_size=(25, 100, 600))\n",
      "      (conv_layer1): Sequential(\n",
      "        (0): Conv3d(1, 16, kernel_size=(2, 3, 3), stride=(1, 1, 1))\n",
      "        (1): LeakyReLU(negative_slope=0.01)\n",
      "        (2): Conv3d(16, 16, kernel_size=(2, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (3): LeakyReLU(negative_slope=0.01)\n",
      "        (4): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (conv_layer2): Sequential(\n",
      "        (0): Conv3d(16, 32, kernel_size=(2, 3, 3), stride=(1, 1, 1))\n",
      "        (1): LeakyReLU(negative_slope=0.01)\n",
      "        (2): Conv3d(32, 32, kernel_size=(2, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (3): LeakyReLU(negative_slope=0.01)\n",
      "        (4): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (conv_layer5): Conv3d(32, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1))\n",
      "      (adap_pool2): AdaptiveAvgPool3d(output_size=(6, 10, 60))\n",
      "    )\n",
      "    (Attention): Attention(\n",
      "      (encoder_att): Linear(in_features=67, out_features=64, bias=True)\n",
      "      (condition_att): Linear(in_features=8, out_features=64, bias=True)\n",
      "      (full_att): Linear(in_features=64, out_features=1, bias=True)\n",
      "      (relu): ReLU()\n",
      "      (softmax): Softmax(dim=1)\n",
      "    )\n",
      "    (fc1): Linear(in_features=72, out_features=300, bias=True)\n",
      "  )\n",
      "  (encoder): Encoder(\n",
      "    (sequential): Sequential(\n",
      "      (0): Linear(in_features=304, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=1024, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "    (linear_means): Linear(in_features=512, out_features=50, bias=True)\n",
      "    (linear_log_var): Linear(in_features=512, out_features=50, bias=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (sequential): Sequential(\n",
      "      (0): Linear(in_features=350, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=1024, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (5): ReLU()\n",
      "      (6): Linear(in_features=512, out_features=4, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "X_dim = 4\n",
    "z_dim = 50\n",
    "cnn_out_size = 300\n",
    "cond_out_size = 300\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device\", device)\n",
    "\n",
    "model = convVAE(sample_size = X_dim, \n",
    "                  cnnout_size = cnn_out_size, \n",
    "                  cond_out_size = cond_out_size, \n",
    "                  encoder_layer_sizes = [512,1024,512], \n",
    "                  latent_size = z_dim, \n",
    "                  decoder_layer_sizes = [512,1024,512]).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(recon_x, x, w, mean, log_var):\n",
    "    MSE = torch.mean((w.expand_as(x) * (recon_x-x)**2))\n",
    "    KLD = - 0.002 * torch.mean(torch.sum(1 + log_var - mean.pow(2) - log_var.exp(), 1))\n",
    "    return MSE + KLD, MSE\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, writer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    mse_loss = 0\n",
    "    w = torch.tensor([5, 10, 1, 3], dtype=torch.float).to(device)\n",
    "    adap_pool = nn.AdaptiveAvgPool3d((25,100, 600))\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        startgoal = batch[\"start_goal\"].to(device)\n",
    "        occ = batch[\"observation\"]\n",
    "        occ = adap_pool(occ)\n",
    "        occ = occ.to(device)\n",
    "        occ = occ.unsqueeze(1)\n",
    "        data = batch[\"data\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data, startgoal, occ)\n",
    "        loss, mse= loss_fn(recon_batch, data, w, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        mse_loss += mse.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item()))\n",
    "        \n",
    "            writer.add_scalar('BatchLoss/loss', loss.item(), batch_idx)\n",
    "            writer.add_scalar('BatchLoss/mse_loss', mse.item(), batch_idx)\n",
    "\n",
    "    epoch_loss = train_loss * len(data) / len(train_loader.dataset)\n",
    "    epoch_mse = mse_loss * len(data) / len(train_loader.dataset)\n",
    "    print('====> Epoch: {} Average loss: {:.7f}'.format(\n",
    "          epoch, epoch_loss))\n",
    "    return epoch, epoch_loss, epoch_mse\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    mse_loss = 0\n",
    "    w = torch.tensor([1, 1, 1, 0.5], dtype=torch.float).to(device)\n",
    "    for batch_idx, batch in enumerate(test_loader):\n",
    "        startgoal = batch[\"start_goal\"].to(device)\n",
    "        occ = batch[\"observation\"].to(device)\n",
    "        occ = occ.unsqueeze(1)\n",
    "        data = batch[\"data\"].to(device)\n",
    "        \n",
    "        recon_batch, mu, logvar = model(sample, startend, occ)\n",
    "        loss, mse= loss_fn(recon_batch, data, w, mu, logvar)\n",
    "        test_loss += loss.item()\n",
    "        mse_loss += mse.item()\n",
    "\n",
    "    epoch_loss = test_loss * len(data) / len(test_loader.dataset)\n",
    "    epoch_mse = mse_loss * len(data) / len(test_loader.dataset)\n",
    "    print('====> Epoch: {} Average test loss: {:.7f}'.format(\n",
    "          epoch, epoch_loss))\n",
    "    return epoch, epoch_loss, epoch_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Writer will output to ./runs/ directory by default\n",
    "writer = SummaryWriter('runs/highway_attention_01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/482500 (0%)]\tLoss: 8055.420410\n",
      "Train Epoch: 0 [160/482500 (0%)]\tLoss: 2803.440674\n",
      "Train Epoch: 0 [320/482500 (0%)]\tLoss: 1127.901733\n",
      "Train Epoch: 0 [480/482500 (0%)]\tLoss: 899.661133\n",
      "Train Epoch: 0 [640/482500 (0%)]\tLoss: 1501.718140\n",
      "Train Epoch: 0 [800/482500 (0%)]\tLoss: 228.068207\n",
      "Train Epoch: 0 [960/482500 (0%)]\tLoss: 91.873367\n",
      "Train Epoch: 0 [1120/482500 (0%)]\tLoss: 15.596081\n",
      "Train Epoch: 0 [1280/482500 (0%)]\tLoss: 16.236187\n",
      "Train Epoch: 0 [1440/482500 (0%)]\tLoss: 11.649496\n",
      "Train Epoch: 0 [1600/482500 (0%)]\tLoss: 10.139577\n",
      "Train Epoch: 0 [1760/482500 (0%)]\tLoss: 12.778826\n",
      "Train Epoch: 0 [1920/482500 (0%)]\tLoss: 4.730198\n",
      "Train Epoch: 0 [2080/482500 (0%)]\tLoss: 7.960318\n",
      "Train Epoch: 0 [2240/482500 (0%)]\tLoss: 3.263066\n",
      "Train Epoch: 0 [2400/482500 (0%)]\tLoss: 472.378815\n",
      "Train Epoch: 0 [2560/482500 (1%)]\tLoss: 55.074020\n",
      "Train Epoch: 0 [2720/482500 (1%)]\tLoss: 105.411690\n",
      "Train Epoch: 0 [2880/482500 (1%)]\tLoss: 21.365833\n",
      "Train Epoch: 0 [3040/482500 (1%)]\tLoss: 16.737120\n",
      "Train Epoch: 0 [3200/482500 (1%)]\tLoss: 14.891021\n",
      "Train Epoch: 0 [3360/482500 (1%)]\tLoss: 10.283529\n",
      "Train Epoch: 0 [3520/482500 (1%)]\tLoss: 13.141476\n",
      "Train Epoch: 0 [3680/482500 (1%)]\tLoss: 14.046994\n",
      "Train Epoch: 0 [3840/482500 (1%)]\tLoss: 13.816860\n",
      "Train Epoch: 0 [4000/482500 (1%)]\tLoss: 6.162323\n",
      "Train Epoch: 0 [4160/482500 (1%)]\tLoss: 27.017975\n",
      "Train Epoch: 0 [4320/482500 (1%)]\tLoss: 11.073697\n",
      "Train Epoch: 0 [4480/482500 (1%)]\tLoss: 12.191998\n",
      "Train Epoch: 0 [4640/482500 (1%)]\tLoss: 8.648805\n",
      "Train Epoch: 0 [4800/482500 (1%)]\tLoss: 21.810869\n",
      "Train Epoch: 0 [4960/482500 (1%)]\tLoss: 12.639225\n",
      "Train Epoch: 0 [5120/482500 (1%)]\tLoss: 6.260325\n",
      "Train Epoch: 0 [5280/482500 (1%)]\tLoss: 9.095867\n",
      "Train Epoch: 0 [5440/482500 (1%)]\tLoss: 7.081441\n",
      "Train Epoch: 0 [5600/482500 (1%)]\tLoss: 7.791136\n",
      "Train Epoch: 0 [5760/482500 (1%)]\tLoss: 3.659140\n",
      "Train Epoch: 0 [5920/482500 (1%)]\tLoss: 5.372436\n",
      "Train Epoch: 0 [6080/482500 (1%)]\tLoss: 4.377861\n",
      "Train Epoch: 0 [6240/482500 (1%)]\tLoss: 7.917012\n",
      "Train Epoch: 0 [6400/482500 (1%)]\tLoss: 4.534068\n",
      "Train Epoch: 0 [6560/482500 (1%)]\tLoss: 5.253175\n",
      "Train Epoch: 0 [6720/482500 (1%)]\tLoss: 5.359980\n",
      "Train Epoch: 0 [6880/482500 (1%)]\tLoss: 8.236500\n",
      "Train Epoch: 0 [7040/482500 (1%)]\tLoss: 6.592879\n",
      "Train Epoch: 0 [7200/482500 (1%)]\tLoss: 7.652762\n",
      "Train Epoch: 0 [7360/482500 (2%)]\tLoss: 3.240803\n",
      "Train Epoch: 0 [7520/482500 (2%)]\tLoss: 57.550480\n",
      "Train Epoch: 0 [7680/482500 (2%)]\tLoss: 6.567560\n",
      "Train Epoch: 0 [7840/482500 (2%)]\tLoss: 6.748602\n",
      "Train Epoch: 0 [8000/482500 (2%)]\tLoss: 3.318976\n",
      "Train Epoch: 0 [8160/482500 (2%)]\tLoss: 6.232121\n",
      "Train Epoch: 0 [8320/482500 (2%)]\tLoss: 1.930738\n",
      "Train Epoch: 0 [8480/482500 (2%)]\tLoss: 3.856409\n",
      "Train Epoch: 0 [8640/482500 (2%)]\tLoss: 1.902129\n",
      "Train Epoch: 0 [8800/482500 (2%)]\tLoss: 2.400387\n",
      "Train Epoch: 0 [8960/482500 (2%)]\tLoss: 3.358330\n",
      "Train Epoch: 0 [9120/482500 (2%)]\tLoss: 3.300986\n",
      "Train Epoch: 0 [9280/482500 (2%)]\tLoss: 3.280328\n",
      "Train Epoch: 0 [9440/482500 (2%)]\tLoss: 3.402942\n",
      "Train Epoch: 0 [9600/482500 (2%)]\tLoss: 2.960479\n",
      "Train Epoch: 0 [9760/482500 (2%)]\tLoss: 26.953041\n",
      "Train Epoch: 0 [9920/482500 (2%)]\tLoss: 7.726581\n",
      "Train Epoch: 0 [10080/482500 (2%)]\tLoss: 4.364435\n",
      "Train Epoch: 0 [10240/482500 (2%)]\tLoss: 4.453455\n",
      "Train Epoch: 0 [10400/482500 (2%)]\tLoss: 3.654582\n",
      "Train Epoch: 0 [10560/482500 (2%)]\tLoss: 3.443020\n",
      "Train Epoch: 0 [10720/482500 (2%)]\tLoss: 7.646850\n",
      "Train Epoch: 0 [10880/482500 (2%)]\tLoss: 2.239908\n",
      "Train Epoch: 0 [11040/482500 (2%)]\tLoss: 2.487379\n",
      "Train Epoch: 0 [11200/482500 (2%)]\tLoss: 4.276803\n",
      "Train Epoch: 0 [11360/482500 (2%)]\tLoss: 21.957062\n",
      "Train Epoch: 0 [11520/482500 (2%)]\tLoss: 5.630459\n",
      "Train Epoch: 0 [11680/482500 (2%)]\tLoss: 11.664507\n",
      "Train Epoch: 0 [11840/482500 (2%)]\tLoss: 8.520851\n",
      "Train Epoch: 0 [12000/482500 (2%)]\tLoss: 1.711709\n",
      "Train Epoch: 0 [12160/482500 (3%)]\tLoss: 3.861924\n",
      "Train Epoch: 0 [12320/482500 (3%)]\tLoss: 2.957268\n",
      "Train Epoch: 0 [12480/482500 (3%)]\tLoss: 2.299198\n",
      "Train Epoch: 0 [12640/482500 (3%)]\tLoss: 3.275020\n",
      "Train Epoch: 0 [12800/482500 (3%)]\tLoss: 10.549952\n",
      "Train Epoch: 0 [12960/482500 (3%)]\tLoss: 2.350064\n",
      "Train Epoch: 0 [13120/482500 (3%)]\tLoss: 3.782519\n",
      "Train Epoch: 0 [13280/482500 (3%)]\tLoss: 3.116883\n",
      "Train Epoch: 0 [13440/482500 (3%)]\tLoss: 10.074734\n",
      "Train Epoch: 0 [13600/482500 (3%)]\tLoss: 5.754781\n",
      "Train Epoch: 0 [13760/482500 (3%)]\tLoss: 13.331037\n",
      "Train Epoch: 0 [13920/482500 (3%)]\tLoss: 5.158079\n",
      "Train Epoch: 0 [14080/482500 (3%)]\tLoss: 20.689543\n",
      "Train Epoch: 0 [14240/482500 (3%)]\tLoss: 5.391764\n",
      "Train Epoch: 0 [14400/482500 (3%)]\tLoss: 6.860976\n",
      "Train Epoch: 0 [14560/482500 (3%)]\tLoss: 4.158898\n",
      "Train Epoch: 0 [14720/482500 (3%)]\tLoss: 4.015477\n",
      "Train Epoch: 0 [14880/482500 (3%)]\tLoss: 2.898576\n",
      "Train Epoch: 0 [15040/482500 (3%)]\tLoss: 2.863908\n",
      "Train Epoch: 0 [15200/482500 (3%)]\tLoss: 6.377289\n",
      "Train Epoch: 0 [15360/482500 (3%)]\tLoss: 5.472583\n",
      "Train Epoch: 0 [15520/482500 (3%)]\tLoss: 52.085285\n",
      "Train Epoch: 0 [15680/482500 (3%)]\tLoss: 8.949002\n",
      "Train Epoch: 0 [15840/482500 (3%)]\tLoss: 7.103034\n",
      "Train Epoch: 0 [16000/482500 (3%)]\tLoss: 35.194557\n",
      "Train Epoch: 0 [16160/482500 (3%)]\tLoss: 15.094013\n",
      "Train Epoch: 0 [16320/482500 (3%)]\tLoss: 3.896124\n",
      "Train Epoch: 0 [16480/482500 (3%)]\tLoss: 5.016160\n",
      "Train Epoch: 0 [16640/482500 (3%)]\tLoss: 4.147486\n",
      "Train Epoch: 0 [16800/482500 (3%)]\tLoss: 2.819775\n",
      "Train Epoch: 0 [16960/482500 (4%)]\tLoss: 2.911482\n",
      "Train Epoch: 0 [17120/482500 (4%)]\tLoss: 3.596726\n",
      "Train Epoch: 0 [17280/482500 (4%)]\tLoss: 3.142762\n",
      "Train Epoch: 0 [17440/482500 (4%)]\tLoss: 3.951601\n",
      "Train Epoch: 0 [17600/482500 (4%)]\tLoss: 2.070341\n",
      "Train Epoch: 0 [17760/482500 (4%)]\tLoss: 2.059275\n",
      "Train Epoch: 0 [17920/482500 (4%)]\tLoss: 10.194019\n",
      "Train Epoch: 0 [18080/482500 (4%)]\tLoss: 5.738621\n",
      "Train Epoch: 0 [18240/482500 (4%)]\tLoss: 5.997898\n",
      "Train Epoch: 0 [18400/482500 (4%)]\tLoss: 7.150934\n",
      "Train Epoch: 0 [18560/482500 (4%)]\tLoss: 9.691321\n",
      "Train Epoch: 0 [18720/482500 (4%)]\tLoss: 1.643120\n",
      "Train Epoch: 0 [18880/482500 (4%)]\tLoss: 4.024273\n",
      "Train Epoch: 0 [19040/482500 (4%)]\tLoss: 24.789131\n",
      "Train Epoch: 0 [19200/482500 (4%)]\tLoss: 3.455097\n",
      "Train Epoch: 0 [19360/482500 (4%)]\tLoss: 4.567102\n",
      "Train Epoch: 0 [19520/482500 (4%)]\tLoss: 7.931508\n",
      "Train Epoch: 0 [19680/482500 (4%)]\tLoss: 3.554011\n",
      "Train Epoch: 0 [19840/482500 (4%)]\tLoss: 2.169697\n",
      "Train Epoch: 0 [20000/482500 (4%)]\tLoss: 3.450662\n",
      "Train Epoch: 0 [20160/482500 (4%)]\tLoss: 2.173473\n",
      "Train Epoch: 0 [20320/482500 (4%)]\tLoss: 93.437302\n",
      "Train Epoch: 0 [20480/482500 (4%)]\tLoss: 29.113440\n",
      "Train Epoch: 0 [20640/482500 (4%)]\tLoss: 10.964314\n",
      "Train Epoch: 0 [20800/482500 (4%)]\tLoss: 7.328856\n",
      "Train Epoch: 0 [20960/482500 (4%)]\tLoss: 11.243309\n",
      "Train Epoch: 0 [21120/482500 (4%)]\tLoss: 9.866099\n",
      "Train Epoch: 0 [21280/482500 (4%)]\tLoss: 22.272930\n",
      "Train Epoch: 0 [21440/482500 (4%)]\tLoss: 24.453922\n",
      "Train Epoch: 0 [21600/482500 (4%)]\tLoss: 10.271714\n",
      "Train Epoch: 0 [21760/482500 (5%)]\tLoss: 3.100096\n",
      "Train Epoch: 0 [21920/482500 (5%)]\tLoss: 4.894637\n",
      "Train Epoch: 0 [22080/482500 (5%)]\tLoss: 2.706857\n",
      "Train Epoch: 0 [22240/482500 (5%)]\tLoss: 4.007077\n",
      "Train Epoch: 0 [22400/482500 (5%)]\tLoss: 4.724977\n",
      "Train Epoch: 0 [22560/482500 (5%)]\tLoss: 5.713057\n",
      "Train Epoch: 0 [22720/482500 (5%)]\tLoss: 3.627082\n",
      "Train Epoch: 0 [22880/482500 (5%)]\tLoss: 3.531570\n",
      "Train Epoch: 0 [23040/482500 (5%)]\tLoss: 1.743584\n",
      "Train Epoch: 0 [23200/482500 (5%)]\tLoss: 8.045148\n",
      "Train Epoch: 0 [23360/482500 (5%)]\tLoss: 3.200341\n",
      "Train Epoch: 0 [23520/482500 (5%)]\tLoss: 7.749076\n",
      "Train Epoch: 0 [23680/482500 (5%)]\tLoss: 4.013009\n",
      "Train Epoch: 0 [23840/482500 (5%)]\tLoss: 2.698610\n",
      "Train Epoch: 0 [24000/482500 (5%)]\tLoss: 3.122670\n",
      "Train Epoch: 0 [24160/482500 (5%)]\tLoss: 3.937902\n",
      "Train Epoch: 0 [24320/482500 (5%)]\tLoss: 62.177391\n",
      "Train Epoch: 0 [24480/482500 (5%)]\tLoss: 16.597881\n",
      "Train Epoch: 0 [24640/482500 (5%)]\tLoss: 4.780892\n",
      "Train Epoch: 0 [24800/482500 (5%)]\tLoss: 7.492106\n",
      "Train Epoch: 0 [24960/482500 (5%)]\tLoss: 3.647142\n",
      "Train Epoch: 0 [25120/482500 (5%)]\tLoss: 5.019094\n",
      "Train Epoch: 0 [25280/482500 (5%)]\tLoss: 2.691893\n",
      "Train Epoch: 0 [25440/482500 (5%)]\tLoss: 3.199392\n",
      "Train Epoch: 0 [25600/482500 (5%)]\tLoss: 2.629089\n",
      "Train Epoch: 0 [25760/482500 (5%)]\tLoss: 3.936287\n",
      "Train Epoch: 0 [25920/482500 (5%)]\tLoss: 4.301377\n",
      "Train Epoch: 0 [26080/482500 (5%)]\tLoss: 4.142431\n",
      "Train Epoch: 0 [26240/482500 (5%)]\tLoss: 2.748830\n",
      "Train Epoch: 0 [26400/482500 (5%)]\tLoss: 17.695320\n",
      "Train Epoch: 0 [26560/482500 (6%)]\tLoss: 10.518337\n",
      "Train Epoch: 0 [26720/482500 (6%)]\tLoss: 7.434819\n",
      "Train Epoch: 0 [26880/482500 (6%)]\tLoss: 4.762072\n",
      "Train Epoch: 0 [27040/482500 (6%)]\tLoss: 2.653910\n",
      "Train Epoch: 0 [27200/482500 (6%)]\tLoss: 2.858863\n",
      "Train Epoch: 0 [27360/482500 (6%)]\tLoss: 8.525786\n",
      "Train Epoch: 0 [27520/482500 (6%)]\tLoss: 1.472203\n",
      "Train Epoch: 0 [27680/482500 (6%)]\tLoss: 2.667155\n",
      "Train Epoch: 0 [27840/482500 (6%)]\tLoss: 3.409804\n",
      "Train Epoch: 0 [28000/482500 (6%)]\tLoss: 3.332900\n",
      "Train Epoch: 0 [28160/482500 (6%)]\tLoss: 1.997073\n",
      "Train Epoch: 0 [28320/482500 (6%)]\tLoss: 2.871896\n",
      "Train Epoch: 0 [28480/482500 (6%)]\tLoss: 1.836315\n",
      "Train Epoch: 0 [28640/482500 (6%)]\tLoss: 7.711156\n",
      "Train Epoch: 0 [28800/482500 (6%)]\tLoss: 12.878807\n",
      "Train Epoch: 0 [28960/482500 (6%)]\tLoss: 14.978935\n",
      "Train Epoch: 0 [29120/482500 (6%)]\tLoss: 529.926392\n",
      "Train Epoch: 0 [29280/482500 (6%)]\tLoss: 24.881931\n",
      "Train Epoch: 0 [29440/482500 (6%)]\tLoss: 5.212252\n",
      "Train Epoch: 0 [29600/482500 (6%)]\tLoss: 5.867767\n",
      "Train Epoch: 0 [29760/482500 (6%)]\tLoss: 3.354588\n",
      "Train Epoch: 0 [29920/482500 (6%)]\tLoss: 2.173199\n",
      "Train Epoch: 0 [30080/482500 (6%)]\tLoss: 3.320080\n",
      "Train Epoch: 0 [30240/482500 (6%)]\tLoss: 1.290420\n",
      "Train Epoch: 0 [30400/482500 (6%)]\tLoss: 3.130492\n",
      "Train Epoch: 0 [30560/482500 (6%)]\tLoss: 45.928608\n",
      "Train Epoch: 0 [30720/482500 (6%)]\tLoss: 19.822630\n",
      "Train Epoch: 0 [30880/482500 (6%)]\tLoss: 18.370937\n",
      "Train Epoch: 0 [31040/482500 (6%)]\tLoss: 13.287464\n",
      "Train Epoch: 0 [31200/482500 (6%)]\tLoss: 14.835525\n",
      "Train Epoch: 0 [31360/482500 (6%)]\tLoss: 4.396312\n",
      "Train Epoch: 0 [31520/482500 (7%)]\tLoss: 14.910336\n",
      "Train Epoch: 0 [31680/482500 (7%)]\tLoss: 6.177117\n",
      "Train Epoch: 0 [31840/482500 (7%)]\tLoss: 18.158468\n",
      "Train Epoch: 0 [32000/482500 (7%)]\tLoss: 223.398361\n",
      "Train Epoch: 0 [32160/482500 (7%)]\tLoss: 24.733673\n",
      "Train Epoch: 0 [32320/482500 (7%)]\tLoss: 3.913748\n",
      "Train Epoch: 0 [32480/482500 (7%)]\tLoss: 3.528903\n",
      "Train Epoch: 0 [32640/482500 (7%)]\tLoss: 5.137572\n",
      "Train Epoch: 0 [32800/482500 (7%)]\tLoss: 3.987419\n",
      "Train Epoch: 0 [32960/482500 (7%)]\tLoss: 4.767636\n",
      "Train Epoch: 0 [33120/482500 (7%)]\tLoss: 4.630475\n",
      "Train Epoch: 0 [33280/482500 (7%)]\tLoss: 4.673037\n",
      "Train Epoch: 0 [33440/482500 (7%)]\tLoss: 3.523701\n",
      "Train Epoch: 0 [33600/482500 (7%)]\tLoss: 2.899928\n",
      "Train Epoch: 0 [33760/482500 (7%)]\tLoss: 3.083609\n",
      "Train Epoch: 0 [33920/482500 (7%)]\tLoss: 4.885880\n",
      "Train Epoch: 0 [34080/482500 (7%)]\tLoss: 3.423129\n",
      "Train Epoch: 0 [34240/482500 (7%)]\tLoss: 3.397654\n",
      "Train Epoch: 0 [34400/482500 (7%)]\tLoss: 2.383610\n",
      "Train Epoch: 0 [34560/482500 (7%)]\tLoss: 3.003054\n",
      "Train Epoch: 0 [34720/482500 (7%)]\tLoss: 2.338831\n",
      "Train Epoch: 0 [34880/482500 (7%)]\tLoss: 1.952629\n",
      "Train Epoch: 0 [35040/482500 (7%)]\tLoss: 2.673872\n",
      "Train Epoch: 0 [35200/482500 (7%)]\tLoss: 2.200386\n",
      "Train Epoch: 0 [35360/482500 (7%)]\tLoss: 1.541737\n",
      "Train Epoch: 0 [35520/482500 (7%)]\tLoss: 1.647393\n",
      "Train Epoch: 0 [35680/482500 (7%)]\tLoss: 4.778244\n",
      "Train Epoch: 0 [35840/482500 (7%)]\tLoss: 3.891350\n",
      "Train Epoch: 0 [36000/482500 (7%)]\tLoss: 3.497987\n",
      "Train Epoch: 0 [36160/482500 (7%)]\tLoss: 3.412049\n",
      "Train Epoch: 0 [36320/482500 (8%)]\tLoss: 2.882158\n",
      "Train Epoch: 0 [36480/482500 (8%)]\tLoss: 2.451809\n",
      "Train Epoch: 0 [36640/482500 (8%)]\tLoss: 35.456657\n",
      "Train Epoch: 0 [36800/482500 (8%)]\tLoss: 7.121449\n",
      "Train Epoch: 0 [36960/482500 (8%)]\tLoss: 6.079770\n",
      "Train Epoch: 0 [37120/482500 (8%)]\tLoss: 5.262684\n",
      "Train Epoch: 0 [37280/482500 (8%)]\tLoss: 1.999677\n",
      "Train Epoch: 0 [37440/482500 (8%)]\tLoss: 2.468117\n",
      "Train Epoch: 0 [37600/482500 (8%)]\tLoss: 3.179591\n",
      "Train Epoch: 0 [37760/482500 (8%)]\tLoss: 3.787934\n",
      "Train Epoch: 0 [37920/482500 (8%)]\tLoss: 3.091105\n",
      "Train Epoch: 0 [38080/482500 (8%)]\tLoss: 2.222516\n",
      "Train Epoch: 0 [38240/482500 (8%)]\tLoss: 209.379013\n",
      "Train Epoch: 0 [38400/482500 (8%)]\tLoss: 12.730904\n",
      "Train Epoch: 0 [38560/482500 (8%)]\tLoss: 17.777609\n",
      "Train Epoch: 0 [38720/482500 (8%)]\tLoss: 1108.758667\n",
      "Train Epoch: 0 [38880/482500 (8%)]\tLoss: 210.490982\n",
      "Train Epoch: 0 [39040/482500 (8%)]\tLoss: 20.216209\n",
      "Train Epoch: 0 [39200/482500 (8%)]\tLoss: 13.502699\n",
      "Train Epoch: 0 [39360/482500 (8%)]\tLoss: 7.979842\n",
      "Train Epoch: 0 [39520/482500 (8%)]\tLoss: 1.992540\n",
      "Train Epoch: 0 [39680/482500 (8%)]\tLoss: 2.241417\n",
      "Train Epoch: 0 [39840/482500 (8%)]\tLoss: 3.481801\n",
      "Train Epoch: 0 [40000/482500 (8%)]\tLoss: 3.141335\n",
      "Train Epoch: 0 [40160/482500 (8%)]\tLoss: 3.290831\n",
      "Train Epoch: 0 [40320/482500 (8%)]\tLoss: 3.470376\n",
      "Train Epoch: 0 [40480/482500 (8%)]\tLoss: 4.152674\n",
      "Train Epoch: 0 [40640/482500 (8%)]\tLoss: 6.739407\n",
      "Train Epoch: 0 [40800/482500 (8%)]\tLoss: 4.663903\n",
      "Train Epoch: 0 [40960/482500 (8%)]\tLoss: 2.494651\n",
      "Train Epoch: 0 [41120/482500 (9%)]\tLoss: 17.806534\n",
      "Train Epoch: 0 [41280/482500 (9%)]\tLoss: 10.853416\n",
      "Train Epoch: 0 [41440/482500 (9%)]\tLoss: 3.496498\n",
      "Train Epoch: 0 [41600/482500 (9%)]\tLoss: 4.720291\n",
      "Train Epoch: 0 [41760/482500 (9%)]\tLoss: 3.093315\n",
      "Train Epoch: 0 [41920/482500 (9%)]\tLoss: 2.406504\n",
      "Train Epoch: 0 [42080/482500 (9%)]\tLoss: 4.492173\n",
      "Train Epoch: 0 [42240/482500 (9%)]\tLoss: 8.561514\n",
      "Train Epoch: 0 [42400/482500 (9%)]\tLoss: 3.251925\n",
      "Train Epoch: 0 [42560/482500 (9%)]\tLoss: 5.413135\n",
      "Train Epoch: 0 [42720/482500 (9%)]\tLoss: 2.820511\n",
      "Train Epoch: 0 [42880/482500 (9%)]\tLoss: 2.969904\n",
      "Train Epoch: 0 [43040/482500 (9%)]\tLoss: 3.511149\n",
      "Train Epoch: 0 [43200/482500 (9%)]\tLoss: 2.816508\n",
      "Train Epoch: 0 [43360/482500 (9%)]\tLoss: 4.452408\n",
      "Train Epoch: 0 [43520/482500 (9%)]\tLoss: 1.759868\n",
      "Train Epoch: 0 [43680/482500 (9%)]\tLoss: 1.361743\n",
      "Train Epoch: 0 [43840/482500 (9%)]\tLoss: 2.591060\n",
      "Train Epoch: 0 [44000/482500 (9%)]\tLoss: 6.031441\n",
      "Train Epoch: 0 [44160/482500 (9%)]\tLoss: 3.537257\n",
      "Train Epoch: 0 [44320/482500 (9%)]\tLoss: 3.814107\n",
      "Train Epoch: 0 [44480/482500 (9%)]\tLoss: 3.353376\n",
      "Train Epoch: 0 [44640/482500 (9%)]\tLoss: 38.291527\n",
      "Train Epoch: 0 [44800/482500 (9%)]\tLoss: 4.952058\n",
      "Train Epoch: 0 [44960/482500 (9%)]\tLoss: 5.359758\n",
      "Train Epoch: 0 [45120/482500 (9%)]\tLoss: 3.377656\n",
      "Train Epoch: 0 [45280/482500 (9%)]\tLoss: 1.655733\n",
      "Train Epoch: 0 [45440/482500 (9%)]\tLoss: 2.654712\n",
      "Train Epoch: 0 [45600/482500 (9%)]\tLoss: 1.741678\n",
      "Train Epoch: 0 [45760/482500 (9%)]\tLoss: 4.788493\n",
      "Train Epoch: 0 [45920/482500 (10%)]\tLoss: 2.993963\n",
      "Train Epoch: 0 [46080/482500 (10%)]\tLoss: 5.209586\n",
      "Train Epoch: 0 [46240/482500 (10%)]\tLoss: 2.320521\n",
      "Train Epoch: 0 [46400/482500 (10%)]\tLoss: 3.394091\n",
      "Train Epoch: 0 [46560/482500 (10%)]\tLoss: 2.744310\n",
      "Train Epoch: 0 [46720/482500 (10%)]\tLoss: 2.787262\n",
      "Train Epoch: 0 [46880/482500 (10%)]\tLoss: 2.636933\n",
      "Train Epoch: 0 [47040/482500 (10%)]\tLoss: 2.581007\n",
      "Train Epoch: 0 [47200/482500 (10%)]\tLoss: 1.841549\n",
      "Train Epoch: 0 [47360/482500 (10%)]\tLoss: 2.428374\n",
      "Train Epoch: 0 [47520/482500 (10%)]\tLoss: 2.071478\n",
      "Train Epoch: 0 [47680/482500 (10%)]\tLoss: 2.150089\n",
      "Train Epoch: 0 [47840/482500 (10%)]\tLoss: 2.699923\n",
      "Train Epoch: 0 [48000/482500 (10%)]\tLoss: 1.564966\n",
      "Train Epoch: 0 [48160/482500 (10%)]\tLoss: 1.384102\n",
      "Train Epoch: 0 [48320/482500 (10%)]\tLoss: 1.441521\n",
      "Train Epoch: 0 [48480/482500 (10%)]\tLoss: 2.935760\n",
      "Train Epoch: 0 [48640/482500 (10%)]\tLoss: 2.321841\n",
      "Train Epoch: 0 [48800/482500 (10%)]\tLoss: 1.518623\n",
      "Train Epoch: 0 [48960/482500 (10%)]\tLoss: 2.073491\n",
      "Train Epoch: 0 [49120/482500 (10%)]\tLoss: 2.489504\n",
      "Train Epoch: 0 [49280/482500 (10%)]\tLoss: 3.702187\n",
      "Train Epoch: 0 [49440/482500 (10%)]\tLoss: 2.935284\n",
      "Train Epoch: 0 [49600/482500 (10%)]\tLoss: 2.490238\n",
      "Train Epoch: 0 [49760/482500 (10%)]\tLoss: 3.056258\n",
      "Train Epoch: 0 [49920/482500 (10%)]\tLoss: 2.327985\n",
      "Train Epoch: 0 [50080/482500 (10%)]\tLoss: 1.713986\n",
      "Train Epoch: 0 [50240/482500 (10%)]\tLoss: 2.453670\n",
      "Train Epoch: 0 [50400/482500 (10%)]\tLoss: 7.157952\n",
      "Train Epoch: 0 [50560/482500 (10%)]\tLoss: 1.505348\n",
      "Train Epoch: 0 [50720/482500 (11%)]\tLoss: 3.162325\n",
      "Train Epoch: 0 [50880/482500 (11%)]\tLoss: 6.090225\n",
      "Train Epoch: 0 [51040/482500 (11%)]\tLoss: 5.614702\n",
      "Train Epoch: 0 [51200/482500 (11%)]\tLoss: 4.638820\n",
      "Train Epoch: 0 [51360/482500 (11%)]\tLoss: 2.565362\n",
      "Train Epoch: 0 [51520/482500 (11%)]\tLoss: 7.161971\n",
      "Train Epoch: 0 [51680/482500 (11%)]\tLoss: 14.467718\n",
      "Train Epoch: 0 [51840/482500 (11%)]\tLoss: 2.351162\n",
      "Train Epoch: 0 [52000/482500 (11%)]\tLoss: 4.122709\n",
      "Train Epoch: 0 [52160/482500 (11%)]\tLoss: 5.580588\n",
      "Train Epoch: 0 [52320/482500 (11%)]\tLoss: 1.352479\n",
      "Train Epoch: 0 [52480/482500 (11%)]\tLoss: 2.938626\n",
      "Train Epoch: 0 [52640/482500 (11%)]\tLoss: 1.874836\n",
      "Train Epoch: 0 [52800/482500 (11%)]\tLoss: 1.658891\n",
      "Train Epoch: 0 [52960/482500 (11%)]\tLoss: 1.229213\n",
      "Train Epoch: 0 [53120/482500 (11%)]\tLoss: 1.331265\n",
      "Train Epoch: 0 [53280/482500 (11%)]\tLoss: 1.515123\n",
      "Train Epoch: 0 [53440/482500 (11%)]\tLoss: 2.649586\n",
      "Train Epoch: 0 [53600/482500 (11%)]\tLoss: 1.509916\n",
      "Train Epoch: 0 [53760/482500 (11%)]\tLoss: 17.641207\n",
      "Train Epoch: 0 [53920/482500 (11%)]\tLoss: 6.637427\n",
      "Train Epoch: 0 [54080/482500 (11%)]\tLoss: 3.040705\n",
      "Train Epoch: 0 [54240/482500 (11%)]\tLoss: 2.447478\n",
      "Train Epoch: 0 [54400/482500 (11%)]\tLoss: 63.189793\n",
      "Train Epoch: 0 [54560/482500 (11%)]\tLoss: 31.031181\n",
      "Train Epoch: 0 [54720/482500 (11%)]\tLoss: 3.876042\n",
      "Train Epoch: 0 [54880/482500 (11%)]\tLoss: 4.515646\n",
      "Train Epoch: 0 [55040/482500 (11%)]\tLoss: 2.798918\n",
      "Train Epoch: 0 [55200/482500 (11%)]\tLoss: 2.986216\n",
      "Train Epoch: 0 [55360/482500 (11%)]\tLoss: 3.613341\n",
      "Train Epoch: 0 [55520/482500 (12%)]\tLoss: 4.188376\n",
      "Train Epoch: 0 [55680/482500 (12%)]\tLoss: 3.137853\n",
      "Train Epoch: 0 [55840/482500 (12%)]\tLoss: 27.552496\n",
      "Train Epoch: 0 [56000/482500 (12%)]\tLoss: 2.659802\n",
      "Train Epoch: 0 [56160/482500 (12%)]\tLoss: 2.391696\n",
      "Train Epoch: 0 [56320/482500 (12%)]\tLoss: 2.242008\n",
      "Train Epoch: 0 [56480/482500 (12%)]\tLoss: 2.317065\n",
      "Train Epoch: 0 [56640/482500 (12%)]\tLoss: 2.925685\n",
      "Train Epoch: 0 [56800/482500 (12%)]\tLoss: 1.188227\n",
      "Train Epoch: 0 [56960/482500 (12%)]\tLoss: 1.901344\n",
      "Train Epoch: 0 [57120/482500 (12%)]\tLoss: 2.180519\n",
      "Train Epoch: 0 [57280/482500 (12%)]\tLoss: 1.972300\n",
      "Train Epoch: 0 [57440/482500 (12%)]\tLoss: 2.358565\n",
      "Train Epoch: 0 [57600/482500 (12%)]\tLoss: 3.608037\n",
      "Train Epoch: 0 [57760/482500 (12%)]\tLoss: 3.048873\n",
      "Train Epoch: 0 [57920/482500 (12%)]\tLoss: 2.661737\n",
      "Train Epoch: 0 [58080/482500 (12%)]\tLoss: 1.309135\n",
      "Train Epoch: 0 [58240/482500 (12%)]\tLoss: 59.756130\n",
      "Train Epoch: 0 [58400/482500 (12%)]\tLoss: 3.391489\n",
      "Train Epoch: 0 [58560/482500 (12%)]\tLoss: 2.840545\n",
      "Train Epoch: 0 [58720/482500 (12%)]\tLoss: 5.494680\n",
      "Train Epoch: 0 [58880/482500 (12%)]\tLoss: 2.513609\n",
      "Train Epoch: 0 [59040/482500 (12%)]\tLoss: 2.061085\n",
      "Train Epoch: 0 [59200/482500 (12%)]\tLoss: 1.625335\n",
      "Train Epoch: 0 [59360/482500 (12%)]\tLoss: 3.392437\n",
      "Train Epoch: 0 [59520/482500 (12%)]\tLoss: 2.240484\n",
      "Train Epoch: 0 [59680/482500 (12%)]\tLoss: 1.713105\n",
      "Train Epoch: 0 [59840/482500 (12%)]\tLoss: 2.451499\n",
      "Train Epoch: 0 [60000/482500 (12%)]\tLoss: 1.146893\n",
      "Train Epoch: 0 [60160/482500 (12%)]\tLoss: 1.987794\n",
      "Train Epoch: 0 [60320/482500 (13%)]\tLoss: 6.613763\n",
      "Train Epoch: 0 [60480/482500 (13%)]\tLoss: 3.210801\n",
      "Train Epoch: 0 [60640/482500 (13%)]\tLoss: 3.494330\n",
      "Train Epoch: 0 [60800/482500 (13%)]\tLoss: 6.188175\n",
      "Train Epoch: 0 [60960/482500 (13%)]\tLoss: 2.732814\n",
      "Train Epoch: 0 [61120/482500 (13%)]\tLoss: 2.781993\n",
      "Train Epoch: 0 [61280/482500 (13%)]\tLoss: 2.244494\n",
      "Train Epoch: 0 [61440/482500 (13%)]\tLoss: 1.167667\n",
      "Train Epoch: 0 [61600/482500 (13%)]\tLoss: 1.387571\n",
      "Train Epoch: 0 [61760/482500 (13%)]\tLoss: 2.764120\n",
      "Train Epoch: 0 [61920/482500 (13%)]\tLoss: 1.532615\n",
      "Train Epoch: 0 [62080/482500 (13%)]\tLoss: 1.858572\n",
      "Train Epoch: 0 [62240/482500 (13%)]\tLoss: 2.013954\n",
      "Train Epoch: 0 [62400/482500 (13%)]\tLoss: 3.185189\n",
      "Train Epoch: 0 [62560/482500 (13%)]\tLoss: 0.765544\n",
      "Train Epoch: 0 [62720/482500 (13%)]\tLoss: 2.751149\n",
      "Train Epoch: 0 [62880/482500 (13%)]\tLoss: 1.998148\n",
      "Train Epoch: 0 [63040/482500 (13%)]\tLoss: 3.183135\n",
      "Train Epoch: 0 [63200/482500 (13%)]\tLoss: 1.702500\n",
      "Train Epoch: 0 [63360/482500 (13%)]\tLoss: 2.440356\n",
      "Train Epoch: 0 [63520/482500 (13%)]\tLoss: 2.458546\n",
      "Train Epoch: 0 [63680/482500 (13%)]\tLoss: 2.104866\n",
      "Train Epoch: 0 [63840/482500 (13%)]\tLoss: 2.139627\n",
      "Train Epoch: 0 [64000/482500 (13%)]\tLoss: 1.395135\n",
      "Train Epoch: 0 [64160/482500 (13%)]\tLoss: 1.551790\n",
      "Train Epoch: 0 [64320/482500 (13%)]\tLoss: 1.077944\n",
      "Train Epoch: 0 [64480/482500 (13%)]\tLoss: 2.146142\n",
      "Train Epoch: 0 [64640/482500 (13%)]\tLoss: 1.560052\n",
      "Train Epoch: 0 [64800/482500 (13%)]\tLoss: 1.800282\n",
      "Train Epoch: 0 [64960/482500 (13%)]\tLoss: 32.394245\n",
      "Train Epoch: 0 [65120/482500 (13%)]\tLoss: 10.607244\n",
      "Train Epoch: 0 [65280/482500 (14%)]\tLoss: 9.950947\n",
      "Train Epoch: 0 [65440/482500 (14%)]\tLoss: 2.718161\n",
      "Train Epoch: 0 [65600/482500 (14%)]\tLoss: 3.088579\n",
      "Train Epoch: 0 [65760/482500 (14%)]\tLoss: 3.050616\n",
      "Train Epoch: 0 [65920/482500 (14%)]\tLoss: 3.153849\n",
      "Train Epoch: 0 [66080/482500 (14%)]\tLoss: 1.900788\n",
      "Train Epoch: 0 [66240/482500 (14%)]\tLoss: 2.335970\n",
      "Train Epoch: 0 [66400/482500 (14%)]\tLoss: 1.579879\n",
      "Train Epoch: 0 [66560/482500 (14%)]\tLoss: 1.061872\n",
      "Train Epoch: 0 [66720/482500 (14%)]\tLoss: 1.963005\n",
      "Train Epoch: 0 [66880/482500 (14%)]\tLoss: 3.121867\n",
      "Train Epoch: 0 [67040/482500 (14%)]\tLoss: 1.497104\n",
      "Train Epoch: 0 [67200/482500 (14%)]\tLoss: 1.520379\n",
      "Train Epoch: 0 [67360/482500 (14%)]\tLoss: 1.759841\n",
      "Train Epoch: 0 [67520/482500 (14%)]\tLoss: 2.516321\n",
      "Train Epoch: 0 [67680/482500 (14%)]\tLoss: 3.859522\n",
      "Train Epoch: 0 [67840/482500 (14%)]\tLoss: 1.540357\n",
      "Train Epoch: 0 [68000/482500 (14%)]\tLoss: 1.461030\n",
      "Train Epoch: 0 [68160/482500 (14%)]\tLoss: 1.217437\n",
      "Train Epoch: 0 [68320/482500 (14%)]\tLoss: 2.115508\n",
      "Train Epoch: 0 [68480/482500 (14%)]\tLoss: 2.304692\n",
      "Train Epoch: 0 [68640/482500 (14%)]\tLoss: 601.067322\n",
      "Train Epoch: 0 [68800/482500 (14%)]\tLoss: 38.552307\n",
      "Train Epoch: 0 [68960/482500 (14%)]\tLoss: 6.291437\n",
      "Train Epoch: 0 [69120/482500 (14%)]\tLoss: 9.800098\n",
      "Train Epoch: 0 [69280/482500 (14%)]\tLoss: 3.925810\n",
      "Train Epoch: 0 [69440/482500 (14%)]\tLoss: 1.723394\n",
      "Train Epoch: 0 [69600/482500 (14%)]\tLoss: 291.462769\n",
      "Train Epoch: 0 [69760/482500 (14%)]\tLoss: 17.438684\n",
      "Train Epoch: 0 [69920/482500 (14%)]\tLoss: 14.546443\n",
      "Train Epoch: 0 [70080/482500 (15%)]\tLoss: 5.299662\n",
      "Train Epoch: 0 [70240/482500 (15%)]\tLoss: 2.868773\n",
      "Train Epoch: 0 [70400/482500 (15%)]\tLoss: 2.164025\n",
      "Train Epoch: 0 [70560/482500 (15%)]\tLoss: 2.125592\n",
      "Train Epoch: 0 [70720/482500 (15%)]\tLoss: 1.995072\n",
      "Train Epoch: 0 [70880/482500 (15%)]\tLoss: 1.650296\n",
      "Train Epoch: 0 [71040/482500 (15%)]\tLoss: 2.104997\n",
      "Train Epoch: 0 [71200/482500 (15%)]\tLoss: 2.091377\n",
      "Train Epoch: 0 [71360/482500 (15%)]\tLoss: 1.583596\n",
      "Train Epoch: 0 [71520/482500 (15%)]\tLoss: 1.726883\n",
      "Train Epoch: 0 [71680/482500 (15%)]\tLoss: 1.579815\n",
      "Train Epoch: 0 [71840/482500 (15%)]\tLoss: 2.772746\n",
      "Train Epoch: 0 [72000/482500 (15%)]\tLoss: 1.925288\n",
      "Train Epoch: 0 [72160/482500 (15%)]\tLoss: 2.777539\n",
      "Train Epoch: 0 [72320/482500 (15%)]\tLoss: 1.858821\n",
      "Train Epoch: 0 [72480/482500 (15%)]\tLoss: 34.531830\n",
      "Train Epoch: 0 [72640/482500 (15%)]\tLoss: 4.444170\n",
      "Train Epoch: 0 [72800/482500 (15%)]\tLoss: 2.248802\n",
      "Train Epoch: 0 [72960/482500 (15%)]\tLoss: 3.038793\n",
      "Train Epoch: 0 [73120/482500 (15%)]\tLoss: 41.803925\n",
      "Train Epoch: 0 [73280/482500 (15%)]\tLoss: 3.159711\n",
      "Train Epoch: 0 [73440/482500 (15%)]\tLoss: 5.005379\n",
      "Train Epoch: 0 [73600/482500 (15%)]\tLoss: 2.647350\n",
      "Train Epoch: 0 [73760/482500 (15%)]\tLoss: 2.589692\n",
      "Train Epoch: 0 [73920/482500 (15%)]\tLoss: 2.995193\n",
      "Train Epoch: 0 [74080/482500 (15%)]\tLoss: 1.289217\n",
      "Train Epoch: 0 [74240/482500 (15%)]\tLoss: 1.968363\n",
      "Train Epoch: 0 [74400/482500 (15%)]\tLoss: 3.180569\n",
      "Train Epoch: 0 [74560/482500 (15%)]\tLoss: 1.821911\n",
      "Train Epoch: 0 [74720/482500 (15%)]\tLoss: 1.761971\n",
      "Train Epoch: 0 [74880/482500 (16%)]\tLoss: 1.753144\n",
      "Train Epoch: 0 [75040/482500 (16%)]\tLoss: 2.039521\n",
      "Train Epoch: 0 [75200/482500 (16%)]\tLoss: 2.370103\n",
      "Train Epoch: 0 [75360/482500 (16%)]\tLoss: 3.383791\n",
      "Train Epoch: 0 [75520/482500 (16%)]\tLoss: 1.346449\n",
      "Train Epoch: 0 [75680/482500 (16%)]\tLoss: 2044.625977\n",
      "Train Epoch: 0 [75840/482500 (16%)]\tLoss: 268.955261\n",
      "Train Epoch: 0 [76000/482500 (16%)]\tLoss: 57.261784\n",
      "Train Epoch: 0 [76160/482500 (16%)]\tLoss: 12.765406\n",
      "Train Epoch: 0 [76320/482500 (16%)]\tLoss: 11.293531\n",
      "Train Epoch: 0 [76480/482500 (16%)]\tLoss: 4.434343\n",
      "Train Epoch: 0 [76640/482500 (16%)]\tLoss: 4.535511\n",
      "Train Epoch: 0 [76800/482500 (16%)]\tLoss: 6.045781\n",
      "Train Epoch: 0 [76960/482500 (16%)]\tLoss: 6.581230\n",
      "Train Epoch: 0 [77120/482500 (16%)]\tLoss: 4.789161\n",
      "Train Epoch: 0 [77280/482500 (16%)]\tLoss: 1.910128\n",
      "Train Epoch: 0 [77440/482500 (16%)]\tLoss: 2.578779\n",
      "Train Epoch: 0 [77600/482500 (16%)]\tLoss: 1.137370\n",
      "Train Epoch: 0 [77760/482500 (16%)]\tLoss: 4.511580\n",
      "Train Epoch: 0 [77920/482500 (16%)]\tLoss: 2.987926\n",
      "Train Epoch: 0 [78080/482500 (16%)]\tLoss: 4.969269\n",
      "Train Epoch: 0 [78240/482500 (16%)]\tLoss: 1.645304\n",
      "Train Epoch: 0 [78400/482500 (16%)]\tLoss: 3.454986\n",
      "Train Epoch: 0 [78560/482500 (16%)]\tLoss: 3.796910\n",
      "Train Epoch: 0 [78720/482500 (16%)]\tLoss: 4.510802\n",
      "Train Epoch: 0 [78880/482500 (16%)]\tLoss: 4.577451\n",
      "Train Epoch: 0 [79040/482500 (16%)]\tLoss: 3.068686\n",
      "Train Epoch: 0 [79200/482500 (16%)]\tLoss: 5.415575\n",
      "Train Epoch: 0 [79360/482500 (16%)]\tLoss: 3.529570\n",
      "Train Epoch: 0 [79520/482500 (16%)]\tLoss: 4.974715\n",
      "Train Epoch: 0 [79680/482500 (17%)]\tLoss: 2.586270\n",
      "Train Epoch: 0 [79840/482500 (17%)]\tLoss: 2.614011\n",
      "Train Epoch: 0 [80000/482500 (17%)]\tLoss: 2.210213\n",
      "Train Epoch: 0 [80160/482500 (17%)]\tLoss: 2.801239\n",
      "Train Epoch: 0 [80320/482500 (17%)]\tLoss: 3.478984\n",
      "Train Epoch: 0 [80480/482500 (17%)]\tLoss: 4.597642\n",
      "Train Epoch: 0 [80640/482500 (17%)]\tLoss: 1.468087\n",
      "Train Epoch: 0 [80800/482500 (17%)]\tLoss: 1.903045\n",
      "Train Epoch: 0 [80960/482500 (17%)]\tLoss: 2.080476\n",
      "Train Epoch: 0 [81120/482500 (17%)]\tLoss: 2.131403\n",
      "Train Epoch: 0 [81280/482500 (17%)]\tLoss: 2.017435\n",
      "Train Epoch: 0 [81440/482500 (17%)]\tLoss: 1.654841\n",
      "Train Epoch: 0 [81600/482500 (17%)]\tLoss: 2.107642\n",
      "Train Epoch: 0 [81760/482500 (17%)]\tLoss: 2.636035\n",
      "Train Epoch: 0 [81920/482500 (17%)]\tLoss: 2.342079\n",
      "Train Epoch: 0 [82080/482500 (17%)]\tLoss: 2.833031\n",
      "Train Epoch: 0 [82240/482500 (17%)]\tLoss: 3.350316\n",
      "Train Epoch: 0 [82400/482500 (17%)]\tLoss: 7.333203\n",
      "Train Epoch: 0 [82560/482500 (17%)]\tLoss: 8.470477\n",
      "Train Epoch: 0 [82720/482500 (17%)]\tLoss: 24.792324\n",
      "Train Epoch: 0 [82880/482500 (17%)]\tLoss: 8.055464\n",
      "Train Epoch: 0 [83040/482500 (17%)]\tLoss: 4.896586\n",
      "Train Epoch: 0 [83200/482500 (17%)]\tLoss: 4.388206\n",
      "Train Epoch: 0 [83360/482500 (17%)]\tLoss: 1.528951\n",
      "Train Epoch: 0 [83520/482500 (17%)]\tLoss: 2.818872\n",
      "Train Epoch: 0 [83680/482500 (17%)]\tLoss: 1.772538\n",
      "Train Epoch: 0 [83840/482500 (17%)]\tLoss: 2.806315\n",
      "Train Epoch: 0 [84000/482500 (17%)]\tLoss: 2.740505\n",
      "Train Epoch: 0 [84160/482500 (17%)]\tLoss: 1.525407\n",
      "Train Epoch: 0 [84320/482500 (17%)]\tLoss: 2.248043\n",
      "Train Epoch: 0 [84480/482500 (18%)]\tLoss: 1.754981\n",
      "Train Epoch: 0 [84640/482500 (18%)]\tLoss: 1.812752\n",
      "Train Epoch: 0 [84800/482500 (18%)]\tLoss: 1.996832\n",
      "Train Epoch: 0 [84960/482500 (18%)]\tLoss: 1.445326\n",
      "Train Epoch: 0 [85120/482500 (18%)]\tLoss: 1.967132\n",
      "Train Epoch: 0 [85280/482500 (18%)]\tLoss: 3.949916\n",
      "Train Epoch: 0 [85440/482500 (18%)]\tLoss: 1.599014\n",
      "Train Epoch: 0 [85600/482500 (18%)]\tLoss: 1.771336\n",
      "Train Epoch: 0 [85760/482500 (18%)]\tLoss: 1.578077\n",
      "Train Epoch: 0 [85920/482500 (18%)]\tLoss: 1.379902\n",
      "Train Epoch: 0 [86080/482500 (18%)]\tLoss: 1.437871\n",
      "Train Epoch: 0 [86240/482500 (18%)]\tLoss: 2.492536\n",
      "Train Epoch: 0 [86400/482500 (18%)]\tLoss: 2.538769\n",
      "Train Epoch: 0 [86560/482500 (18%)]\tLoss: 1.895149\n",
      "Train Epoch: 0 [86720/482500 (18%)]\tLoss: 2.568583\n",
      "Train Epoch: 0 [86880/482500 (18%)]\tLoss: 209.191971\n",
      "Train Epoch: 0 [87040/482500 (18%)]\tLoss: 63.739052\n",
      "Train Epoch: 0 [87200/482500 (18%)]\tLoss: 13.030103\n",
      "Train Epoch: 0 [87360/482500 (18%)]\tLoss: 12.700312\n",
      "Train Epoch: 0 [87520/482500 (18%)]\tLoss: 4.856221\n",
      "Train Epoch: 0 [87680/482500 (18%)]\tLoss: 3.030784\n",
      "Train Epoch: 0 [87840/482500 (18%)]\tLoss: 2.410020\n",
      "Train Epoch: 0 [88000/482500 (18%)]\tLoss: 3.253696\n",
      "Train Epoch: 0 [88160/482500 (18%)]\tLoss: 2.004375\n",
      "Train Epoch: 0 [88320/482500 (18%)]\tLoss: 1.958331\n",
      "Train Epoch: 0 [88480/482500 (18%)]\tLoss: 1.790902\n",
      "Train Epoch: 0 [88640/482500 (18%)]\tLoss: 3.085000\n",
      "Train Epoch: 0 [88800/482500 (18%)]\tLoss: 8.415428\n",
      "Train Epoch: 0 [88960/482500 (18%)]\tLoss: 3.276242\n",
      "Train Epoch: 0 [89120/482500 (18%)]\tLoss: 3.074381\n",
      "Train Epoch: 0 [89280/482500 (19%)]\tLoss: 2.373868\n",
      "Train Epoch: 0 [89440/482500 (19%)]\tLoss: 17.512274\n",
      "Train Epoch: 0 [89600/482500 (19%)]\tLoss: 7.322316\n",
      "Train Epoch: 0 [89760/482500 (19%)]\tLoss: 2.851111\n",
      "Train Epoch: 0 [89920/482500 (19%)]\tLoss: 3.148094\n",
      "Train Epoch: 0 [90080/482500 (19%)]\tLoss: 2.578932\n",
      "Train Epoch: 0 [90240/482500 (19%)]\tLoss: 2.280564\n",
      "Train Epoch: 0 [90400/482500 (19%)]\tLoss: 0.929507\n",
      "Train Epoch: 0 [90560/482500 (19%)]\tLoss: 1.618703\n",
      "Train Epoch: 0 [90720/482500 (19%)]\tLoss: 2.701136\n",
      "Train Epoch: 0 [90880/482500 (19%)]\tLoss: 1.751035\n",
      "Train Epoch: 0 [91040/482500 (19%)]\tLoss: 1.990668\n",
      "Train Epoch: 0 [91200/482500 (19%)]\tLoss: 1.070861\n",
      "Train Epoch: 0 [91360/482500 (19%)]\tLoss: 1.610194\n",
      "Train Epoch: 0 [91520/482500 (19%)]\tLoss: 1.823133\n",
      "Train Epoch: 0 [91680/482500 (19%)]\tLoss: 2.077307\n",
      "Train Epoch: 0 [91840/482500 (19%)]\tLoss: 1.894210\n",
      "Train Epoch: 0 [92000/482500 (19%)]\tLoss: 2.436775\n",
      "Train Epoch: 0 [92160/482500 (19%)]\tLoss: 3.784636\n",
      "Train Epoch: 0 [92320/482500 (19%)]\tLoss: 3.147032\n",
      "Train Epoch: 0 [92480/482500 (19%)]\tLoss: 1.406895\n",
      "Train Epoch: 0 [92640/482500 (19%)]\tLoss: 1.298884\n",
      "Train Epoch: 0 [92800/482500 (19%)]\tLoss: 1.501127\n",
      "Train Epoch: 0 [92960/482500 (19%)]\tLoss: 6.034337\n",
      "Train Epoch: 0 [93120/482500 (19%)]\tLoss: 1.439094\n",
      "Train Epoch: 0 [93280/482500 (19%)]\tLoss: 2.043500\n",
      "Train Epoch: 0 [93440/482500 (19%)]\tLoss: 96.306831\n",
      "Train Epoch: 0 [93600/482500 (19%)]\tLoss: 9.213430\n",
      "Train Epoch: 0 [93760/482500 (19%)]\tLoss: 2.696421\n",
      "Train Epoch: 0 [93920/482500 (19%)]\tLoss: 7.054203\n",
      "Train Epoch: 0 [94080/482500 (19%)]\tLoss: 2.090649\n",
      "Train Epoch: 0 [94240/482500 (20%)]\tLoss: 1.863971\n",
      "Train Epoch: 0 [94400/482500 (20%)]\tLoss: 1.899964\n",
      "Train Epoch: 0 [94560/482500 (20%)]\tLoss: 17.629274\n",
      "Train Epoch: 0 [94720/482500 (20%)]\tLoss: 7.460579\n",
      "Train Epoch: 0 [94880/482500 (20%)]\tLoss: 2.359793\n",
      "Train Epoch: 0 [95040/482500 (20%)]\tLoss: 602.753174\n",
      "Train Epoch: 0 [95200/482500 (20%)]\tLoss: 158.367645\n",
      "Train Epoch: 0 [95360/482500 (20%)]\tLoss: 39.953587\n",
      "Train Epoch: 0 [95520/482500 (20%)]\tLoss: 8.712428\n",
      "Train Epoch: 0 [95680/482500 (20%)]\tLoss: 9.892150\n",
      "Train Epoch: 0 [95840/482500 (20%)]\tLoss: 4.095184\n",
      "Train Epoch: 0 [96000/482500 (20%)]\tLoss: 10.327483\n",
      "Train Epoch: 0 [96160/482500 (20%)]\tLoss: 8.251541\n",
      "Train Epoch: 0 [96320/482500 (20%)]\tLoss: 6.779476\n",
      "Train Epoch: 0 [96480/482500 (20%)]\tLoss: 3.551143\n",
      "Train Epoch: 0 [96640/482500 (20%)]\tLoss: 2.549963\n",
      "Train Epoch: 0 [96800/482500 (20%)]\tLoss: 1.928961\n",
      "Train Epoch: 0 [96960/482500 (20%)]\tLoss: 4.457255\n",
      "Train Epoch: 0 [97120/482500 (20%)]\tLoss: 3.722071\n",
      "Train Epoch: 0 [97280/482500 (20%)]\tLoss: 1.976420\n",
      "Train Epoch: 0 [97440/482500 (20%)]\tLoss: 57.722118\n",
      "Train Epoch: 0 [97600/482500 (20%)]\tLoss: 62.342857\n",
      "Train Epoch: 0 [97760/482500 (20%)]\tLoss: 5.863286\n",
      "Train Epoch: 0 [97920/482500 (20%)]\tLoss: 8.141215\n",
      "Train Epoch: 0 [98080/482500 (20%)]\tLoss: 4.560946\n",
      "Train Epoch: 0 [98240/482500 (20%)]\tLoss: 7.035824\n",
      "Train Epoch: 0 [98400/482500 (20%)]\tLoss: 3.410817\n",
      "Train Epoch: 0 [98560/482500 (20%)]\tLoss: 3.384965\n",
      "Train Epoch: 0 [98720/482500 (20%)]\tLoss: 3.304975\n",
      "Train Epoch: 0 [98880/482500 (20%)]\tLoss: 4.213223\n",
      "Train Epoch: 0 [99040/482500 (21%)]\tLoss: 2.700511\n",
      "Train Epoch: 0 [99200/482500 (21%)]\tLoss: 3.849423\n",
      "Train Epoch: 0 [99360/482500 (21%)]\tLoss: 2.520472\n",
      "Train Epoch: 0 [99520/482500 (21%)]\tLoss: 4.105035\n",
      "Train Epoch: 0 [99680/482500 (21%)]\tLoss: 1.859055\n",
      "Train Epoch: 0 [99840/482500 (21%)]\tLoss: 1.315555\n",
      "Train Epoch: 0 [100000/482500 (21%)]\tLoss: 3.696258\n",
      "Train Epoch: 0 [100160/482500 (21%)]\tLoss: 1.871451\n",
      "Train Epoch: 0 [100320/482500 (21%)]\tLoss: 2.923867\n",
      "Train Epoch: 0 [100480/482500 (21%)]\tLoss: 3.685158\n",
      "Train Epoch: 0 [100640/482500 (21%)]\tLoss: 2.001937\n",
      "Train Epoch: 0 [100800/482500 (21%)]\tLoss: 2.281636\n",
      "Train Epoch: 0 [100960/482500 (21%)]\tLoss: 1.845845\n",
      "Train Epoch: 0 [101120/482500 (21%)]\tLoss: 2.433077\n",
      "Train Epoch: 0 [101280/482500 (21%)]\tLoss: 2.308640\n",
      "Train Epoch: 0 [101440/482500 (21%)]\tLoss: 2.014499\n",
      "Train Epoch: 0 [101600/482500 (21%)]\tLoss: 2.329574\n",
      "Train Epoch: 0 [101760/482500 (21%)]\tLoss: 2.354134\n",
      "Train Epoch: 0 [101920/482500 (21%)]\tLoss: 1.858525\n",
      "Train Epoch: 0 [102080/482500 (21%)]\tLoss: 2.016374\n",
      "Train Epoch: 0 [102240/482500 (21%)]\tLoss: 2.135287\n",
      "Train Epoch: 0 [102400/482500 (21%)]\tLoss: 0.995839\n",
      "Train Epoch: 0 [102560/482500 (21%)]\tLoss: 2.065230\n",
      "Train Epoch: 0 [102720/482500 (21%)]\tLoss: 1.262162\n",
      "Train Epoch: 0 [102880/482500 (21%)]\tLoss: 2.448161\n",
      "Train Epoch: 0 [103040/482500 (21%)]\tLoss: 1.803728\n",
      "Train Epoch: 0 [103200/482500 (21%)]\tLoss: 3.170492\n",
      "Train Epoch: 0 [103360/482500 (21%)]\tLoss: 6.731897\n",
      "Train Epoch: 0 [103520/482500 (21%)]\tLoss: 10.511691\n",
      "Train Epoch: 0 [103680/482500 (21%)]\tLoss: 2.563530\n",
      "Train Epoch: 0 [103840/482500 (22%)]\tLoss: 4.057544\n",
      "Train Epoch: 0 [104000/482500 (22%)]\tLoss: 2.248266\n",
      "Train Epoch: 0 [104160/482500 (22%)]\tLoss: 2.045380\n",
      "Train Epoch: 0 [104320/482500 (22%)]\tLoss: 1.570181\n",
      "Train Epoch: 0 [104480/482500 (22%)]\tLoss: 1.241845\n",
      "Train Epoch: 0 [104640/482500 (22%)]\tLoss: 1.512158\n",
      "Train Epoch: 0 [104800/482500 (22%)]\tLoss: 1.694716\n",
      "Train Epoch: 0 [104960/482500 (22%)]\tLoss: 2.194572\n",
      "Train Epoch: 0 [105120/482500 (22%)]\tLoss: 1.322263\n",
      "Train Epoch: 0 [105280/482500 (22%)]\tLoss: 1.877319\n",
      "Train Epoch: 0 [105440/482500 (22%)]\tLoss: 163.742432\n",
      "Train Epoch: 0 [105600/482500 (22%)]\tLoss: 24.608677\n",
      "Train Epoch: 0 [105760/482500 (22%)]\tLoss: 13.938684\n",
      "Train Epoch: 0 [105920/482500 (22%)]\tLoss: 7.890493\n",
      "Train Epoch: 0 [106080/482500 (22%)]\tLoss: 4.548185\n",
      "Train Epoch: 0 [106240/482500 (22%)]\tLoss: 8.369423\n",
      "Train Epoch: 0 [106400/482500 (22%)]\tLoss: 14.037369\n",
      "Train Epoch: 0 [106560/482500 (22%)]\tLoss: 361.993073\n",
      "Train Epoch: 0 [106720/482500 (22%)]\tLoss: 106.583359\n",
      "Train Epoch: 0 [106880/482500 (22%)]\tLoss: 28.573872\n",
      "Train Epoch: 0 [107040/482500 (22%)]\tLoss: 12.566860\n",
      "Train Epoch: 0 [107200/482500 (22%)]\tLoss: 9.081074\n",
      "Train Epoch: 0 [107360/482500 (22%)]\tLoss: 12.387434\n",
      "Train Epoch: 0 [107520/482500 (22%)]\tLoss: 10.263076\n",
      "Train Epoch: 0 [107680/482500 (22%)]\tLoss: 15.478346\n",
      "Train Epoch: 0 [107840/482500 (22%)]\tLoss: 8.406109\n",
      "Train Epoch: 0 [108000/482500 (22%)]\tLoss: 8.681438\n",
      "Train Epoch: 0 [108160/482500 (22%)]\tLoss: 8.991212\n",
      "Train Epoch: 0 [108320/482500 (22%)]\tLoss: 8.572045\n",
      "Train Epoch: 0 [108480/482500 (22%)]\tLoss: 12.799478\n",
      "Train Epoch: 0 [108640/482500 (23%)]\tLoss: 8.776846\n",
      "Train Epoch: 0 [108800/482500 (23%)]\tLoss: 4.304229\n",
      "Train Epoch: 0 [108960/482500 (23%)]\tLoss: 7.539296\n",
      "Train Epoch: 0 [109120/482500 (23%)]\tLoss: 7.479489\n",
      "Train Epoch: 0 [109280/482500 (23%)]\tLoss: 1.874503\n",
      "Train Epoch: 0 [109440/482500 (23%)]\tLoss: 3.882195\n",
      "Train Epoch: 0 [109600/482500 (23%)]\tLoss: 6.485888\n",
      "Train Epoch: 0 [109760/482500 (23%)]\tLoss: 6.281836\n",
      "Train Epoch: 0 [109920/482500 (23%)]\tLoss: 2.821290\n",
      "Train Epoch: 0 [110080/482500 (23%)]\tLoss: 4.422099\n",
      "Train Epoch: 0 [110240/482500 (23%)]\tLoss: 3.602499\n",
      "Train Epoch: 0 [110400/482500 (23%)]\tLoss: 2.851827\n",
      "Train Epoch: 0 [110560/482500 (23%)]\tLoss: 8.663215\n",
      "Train Epoch: 0 [110720/482500 (23%)]\tLoss: 5.769100\n",
      "Train Epoch: 0 [110880/482500 (23%)]\tLoss: 5.032206\n",
      "Train Epoch: 0 [111040/482500 (23%)]\tLoss: 5.695828\n",
      "Train Epoch: 0 [111200/482500 (23%)]\tLoss: 3.594682\n",
      "Train Epoch: 0 [111360/482500 (23%)]\tLoss: 3.872714\n",
      "Train Epoch: 0 [111520/482500 (23%)]\tLoss: 3.294503\n",
      "Train Epoch: 0 [111680/482500 (23%)]\tLoss: 4.375129\n",
      "Train Epoch: 0 [111840/482500 (23%)]\tLoss: 3.709526\n",
      "Train Epoch: 0 [112000/482500 (23%)]\tLoss: 5.414009\n",
      "Train Epoch: 0 [112160/482500 (23%)]\tLoss: 3.408493\n",
      "Train Epoch: 0 [112320/482500 (23%)]\tLoss: 2.713327\n",
      "Train Epoch: 0 [112480/482500 (23%)]\tLoss: 4.241985\n",
      "Train Epoch: 0 [112640/482500 (23%)]\tLoss: 3.652295\n",
      "Train Epoch: 0 [112800/482500 (23%)]\tLoss: 3.168257\n",
      "Train Epoch: 0 [112960/482500 (23%)]\tLoss: 2.656596\n",
      "Train Epoch: 0 [113120/482500 (23%)]\tLoss: 2.374113\n",
      "Train Epoch: 0 [113280/482500 (23%)]\tLoss: 2.887988\n",
      "Train Epoch: 0 [113440/482500 (24%)]\tLoss: 2.243995\n",
      "Train Epoch: 0 [113600/482500 (24%)]\tLoss: 3.029101\n",
      "Train Epoch: 0 [113760/482500 (24%)]\tLoss: 1.390693\n",
      "Train Epoch: 0 [113920/482500 (24%)]\tLoss: 2.641001\n",
      "Train Epoch: 0 [114080/482500 (24%)]\tLoss: 2.054031\n",
      "Train Epoch: 0 [114240/482500 (24%)]\tLoss: 4.028152\n",
      "Train Epoch: 0 [114400/482500 (24%)]\tLoss: 4.432287\n",
      "Train Epoch: 0 [114560/482500 (24%)]\tLoss: 2.195117\n",
      "Train Epoch: 0 [114720/482500 (24%)]\tLoss: 2.306379\n",
      "Train Epoch: 0 [114880/482500 (24%)]\tLoss: 1.445720\n",
      "Train Epoch: 0 [115040/482500 (24%)]\tLoss: 2.473110\n",
      "Train Epoch: 0 [115200/482500 (24%)]\tLoss: 1.891921\n",
      "Train Epoch: 0 [115360/482500 (24%)]\tLoss: 2.279863\n",
      "Train Epoch: 0 [115520/482500 (24%)]\tLoss: 1.664097\n",
      "Train Epoch: 0 [115680/482500 (24%)]\tLoss: 665.989502\n",
      "Train Epoch: 0 [115840/482500 (24%)]\tLoss: 151.533340\n",
      "Train Epoch: 0 [116000/482500 (24%)]\tLoss: 37.329048\n",
      "Train Epoch: 0 [116160/482500 (24%)]\tLoss: 22.489180\n",
      "Train Epoch: 0 [116320/482500 (24%)]\tLoss: 9.905995\n",
      "Train Epoch: 0 [116480/482500 (24%)]\tLoss: 8.220163\n",
      "Train Epoch: 0 [116640/482500 (24%)]\tLoss: 8.561223\n",
      "Train Epoch: 0 [116800/482500 (24%)]\tLoss: 7.808859\n",
      "Train Epoch: 0 [116960/482500 (24%)]\tLoss: 12.308714\n",
      "Train Epoch: 0 [117120/482500 (24%)]\tLoss: 4.293577\n",
      "Train Epoch: 0 [117280/482500 (24%)]\tLoss: 6.284030\n",
      "Train Epoch: 0 [117440/482500 (24%)]\tLoss: 108.662323\n",
      "Train Epoch: 0 [117600/482500 (24%)]\tLoss: 19.526085\n",
      "Train Epoch: 0 [117760/482500 (24%)]\tLoss: 10.597644\n",
      "Train Epoch: 0 [117920/482500 (24%)]\tLoss: 6.078693\n",
      "Train Epoch: 0 [118080/482500 (24%)]\tLoss: 5.180806\n",
      "Train Epoch: 0 [118240/482500 (25%)]\tLoss: 4.879153\n",
      "Train Epoch: 0 [118400/482500 (25%)]\tLoss: 5.860147\n",
      "Train Epoch: 0 [118560/482500 (25%)]\tLoss: 5.097589\n",
      "Train Epoch: 0 [118720/482500 (25%)]\tLoss: 5.027432\n",
      "Train Epoch: 0 [118880/482500 (25%)]\tLoss: 3.715582\n",
      "Train Epoch: 0 [119040/482500 (25%)]\tLoss: 3.594690\n",
      "Train Epoch: 0 [119200/482500 (25%)]\tLoss: 4.412915\n",
      "Train Epoch: 0 [119360/482500 (25%)]\tLoss: 3.547832\n",
      "Train Epoch: 0 [119520/482500 (25%)]\tLoss: 6.587891\n",
      "Train Epoch: 0 [119680/482500 (25%)]\tLoss: 2.253695\n",
      "Train Epoch: 0 [119840/482500 (25%)]\tLoss: 2.548211\n",
      "Train Epoch: 0 [120000/482500 (25%)]\tLoss: 1.856315\n",
      "Train Epoch: 0 [120160/482500 (25%)]\tLoss: 16.094164\n",
      "Train Epoch: 0 [120320/482500 (25%)]\tLoss: 2.829244\n",
      "Train Epoch: 0 [120480/482500 (25%)]\tLoss: 4.563886\n",
      "Train Epoch: 0 [120640/482500 (25%)]\tLoss: 3.572234\n",
      "Train Epoch: 0 [120800/482500 (25%)]\tLoss: 8.019056\n",
      "Train Epoch: 0 [120960/482500 (25%)]\tLoss: 3.226917\n",
      "Train Epoch: 0 [121120/482500 (25%)]\tLoss: 4.172303\n",
      "Train Epoch: 0 [121280/482500 (25%)]\tLoss: 3.762696\n",
      "Train Epoch: 0 [121440/482500 (25%)]\tLoss: 3.465841\n",
      "Train Epoch: 0 [121600/482500 (25%)]\tLoss: 3.727962\n",
      "Train Epoch: 0 [121760/482500 (25%)]\tLoss: 3.377728\n",
      "Train Epoch: 0 [121920/482500 (25%)]\tLoss: 5.497797\n",
      "Train Epoch: 0 [122080/482500 (25%)]\tLoss: 4.077350\n",
      "Train Epoch: 0 [122240/482500 (25%)]\tLoss: 3.483637\n",
      "Train Epoch: 0 [122400/482500 (25%)]\tLoss: 3.163101\n",
      "Train Epoch: 0 [122560/482500 (25%)]\tLoss: 3.768532\n",
      "Train Epoch: 0 [122720/482500 (25%)]\tLoss: 2.314113\n",
      "Train Epoch: 0 [122880/482500 (25%)]\tLoss: 1.584002\n",
      "Train Epoch: 0 [123040/482500 (26%)]\tLoss: 2.477877\n",
      "Train Epoch: 0 [123200/482500 (26%)]\tLoss: 2.765236\n",
      "Train Epoch: 0 [123360/482500 (26%)]\tLoss: 15.670651\n",
      "Train Epoch: 0 [123520/482500 (26%)]\tLoss: 6.205645\n",
      "Train Epoch: 0 [123680/482500 (26%)]\tLoss: 1.855629\n",
      "Train Epoch: 0 [123840/482500 (26%)]\tLoss: 2.075331\n",
      "Train Epoch: 0 [124000/482500 (26%)]\tLoss: 6.093932\n",
      "Train Epoch: 0 [124160/482500 (26%)]\tLoss: 3.672034\n",
      "Train Epoch: 0 [124320/482500 (26%)]\tLoss: 4.339241\n",
      "Train Epoch: 0 [124480/482500 (26%)]\tLoss: 3.274695\n",
      "Train Epoch: 0 [124640/482500 (26%)]\tLoss: 4.571959\n",
      "Train Epoch: 0 [124800/482500 (26%)]\tLoss: 17.957323\n",
      "Train Epoch: 0 [124960/482500 (26%)]\tLoss: 6.413491\n",
      "Train Epoch: 0 [125120/482500 (26%)]\tLoss: 3.118465\n",
      "Train Epoch: 0 [125280/482500 (26%)]\tLoss: 3.376047\n",
      "Train Epoch: 0 [125440/482500 (26%)]\tLoss: 3.398920\n",
      "Train Epoch: 0 [125600/482500 (26%)]\tLoss: 3.648058\n",
      "Train Epoch: 0 [125760/482500 (26%)]\tLoss: 2.760949\n",
      "Train Epoch: 0 [125920/482500 (26%)]\tLoss: 5.144804\n",
      "Train Epoch: 0 [126080/482500 (26%)]\tLoss: 3.042482\n",
      "Train Epoch: 0 [126240/482500 (26%)]\tLoss: 2.924430\n",
      "Train Epoch: 0 [126400/482500 (26%)]\tLoss: 2.577267\n",
      "Train Epoch: 0 [126560/482500 (26%)]\tLoss: 2.445955\n",
      "Train Epoch: 0 [126720/482500 (26%)]\tLoss: 1.755662\n",
      "Train Epoch: 0 [126880/482500 (26%)]\tLoss: 1.706726\n",
      "Train Epoch: 0 [127040/482500 (26%)]\tLoss: 2.633117\n",
      "Train Epoch: 0 [127200/482500 (26%)]\tLoss: 1.707377\n",
      "Train Epoch: 0 [127360/482500 (26%)]\tLoss: 26.866274\n",
      "Train Epoch: 0 [127520/482500 (26%)]\tLoss: 9.062984\n",
      "Train Epoch: 0 [127680/482500 (26%)]\tLoss: 3.154314\n",
      "Train Epoch: 0 [127840/482500 (26%)]\tLoss: 2.846545\n",
      "Train Epoch: 0 [128000/482500 (27%)]\tLoss: 3.579094\n",
      "Train Epoch: 0 [128160/482500 (27%)]\tLoss: 3.024194\n",
      "Train Epoch: 0 [128320/482500 (27%)]\tLoss: 3.882071\n",
      "Train Epoch: 0 [128480/482500 (27%)]\tLoss: 2.826157\n",
      "Train Epoch: 0 [128640/482500 (27%)]\tLoss: 3.972526\n",
      "Train Epoch: 0 [128800/482500 (27%)]\tLoss: 2.555804\n",
      "Train Epoch: 0 [128960/482500 (27%)]\tLoss: 3.083175\n",
      "Train Epoch: 0 [129120/482500 (27%)]\tLoss: 1.813587\n",
      "Train Epoch: 0 [129280/482500 (27%)]\tLoss: 2.455652\n",
      "Train Epoch: 0 [129440/482500 (27%)]\tLoss: 1.115158\n",
      "Train Epoch: 0 [129600/482500 (27%)]\tLoss: 2.589328\n",
      "Train Epoch: 0 [129760/482500 (27%)]\tLoss: 2.487637\n",
      "Train Epoch: 0 [129920/482500 (27%)]\tLoss: 2.023340\n",
      "Train Epoch: 0 [130080/482500 (27%)]\tLoss: 2.600956\n",
      "Train Epoch: 0 [130240/482500 (27%)]\tLoss: 2.562769\n",
      "Train Epoch: 0 [130400/482500 (27%)]\tLoss: 2.260911\n",
      "Train Epoch: 0 [130560/482500 (27%)]\tLoss: 2.710907\n",
      "Train Epoch: 0 [130720/482500 (27%)]\tLoss: 1.986657\n",
      "Train Epoch: 0 [130880/482500 (27%)]\tLoss: 1.635374\n",
      "Train Epoch: 0 [131040/482500 (27%)]\tLoss: 3.487563\n",
      "Train Epoch: 0 [131200/482500 (27%)]\tLoss: 3.443970\n",
      "Train Epoch: 0 [131360/482500 (27%)]\tLoss: 2.582231\n",
      "Train Epoch: 0 [131520/482500 (27%)]\tLoss: 2.582009\n",
      "Train Epoch: 0 [131680/482500 (27%)]\tLoss: 2.713631\n",
      "Train Epoch: 0 [131840/482500 (27%)]\tLoss: 5.200441\n",
      "Train Epoch: 0 [132000/482500 (27%)]\tLoss: 1.808553\n",
      "Train Epoch: 0 [132160/482500 (27%)]\tLoss: 2.154469\n",
      "Train Epoch: 0 [132320/482500 (27%)]\tLoss: 2.580774\n",
      "Train Epoch: 0 [132480/482500 (27%)]\tLoss: 1.804841\n",
      "Train Epoch: 0 [132640/482500 (27%)]\tLoss: 3.780370\n",
      "Train Epoch: 0 [132800/482500 (28%)]\tLoss: 2.186274\n",
      "Train Epoch: 0 [132960/482500 (28%)]\tLoss: 3.278151\n",
      "Train Epoch: 0 [133120/482500 (28%)]\tLoss: 0.988461\n",
      "Train Epoch: 0 [133280/482500 (28%)]\tLoss: 1.410037\n",
      "Train Epoch: 0 [133440/482500 (28%)]\tLoss: 2.415570\n",
      "Train Epoch: 0 [133600/482500 (28%)]\tLoss: 1.927828\n",
      "Train Epoch: 0 [133760/482500 (28%)]\tLoss: 1.530692\n",
      "Train Epoch: 0 [133920/482500 (28%)]\tLoss: 1.892761\n",
      "Train Epoch: 0 [134080/482500 (28%)]\tLoss: 3.184689\n",
      "Train Epoch: 0 [134240/482500 (28%)]\tLoss: 43.725807\n",
      "Train Epoch: 0 [134400/482500 (28%)]\tLoss: 5.400879\n",
      "Train Epoch: 0 [134560/482500 (28%)]\tLoss: 5.409839\n",
      "Train Epoch: 0 [134720/482500 (28%)]\tLoss: 3.016605\n",
      "Train Epoch: 0 [134880/482500 (28%)]\tLoss: 2.210563\n",
      "Train Epoch: 0 [135040/482500 (28%)]\tLoss: 1.639606\n",
      "Train Epoch: 0 [135200/482500 (28%)]\tLoss: 3.676133\n",
      "Train Epoch: 0 [135360/482500 (28%)]\tLoss: 1.668081\n",
      "Train Epoch: 0 [135520/482500 (28%)]\tLoss: 1394.676636\n",
      "Train Epoch: 0 [135680/482500 (28%)]\tLoss: 133.592682\n",
      "Train Epoch: 0 [135840/482500 (28%)]\tLoss: 18.087088\n",
      "Train Epoch: 0 [136000/482500 (28%)]\tLoss: 15.778746\n",
      "Train Epoch: 0 [136160/482500 (28%)]\tLoss: 11.028111\n",
      "Train Epoch: 0 [136320/482500 (28%)]\tLoss: 5.031987\n",
      "Train Epoch: 0 [136480/482500 (28%)]\tLoss: 4.395185\n",
      "Train Epoch: 0 [136640/482500 (28%)]\tLoss: 2.213674\n",
      "Train Epoch: 0 [136800/482500 (28%)]\tLoss: 4.830612\n",
      "Train Epoch: 0 [136960/482500 (28%)]\tLoss: 2.474992\n",
      "Train Epoch: 0 [137120/482500 (28%)]\tLoss: 5.019483\n",
      "Train Epoch: 0 [137280/482500 (28%)]\tLoss: 4.415285\n",
      "Train Epoch: 0 [137440/482500 (28%)]\tLoss: 3.340054\n",
      "Train Epoch: 0 [137600/482500 (29%)]\tLoss: 2.150885\n",
      "Train Epoch: 0 [137760/482500 (29%)]\tLoss: 2.614894\n",
      "Train Epoch: 0 [137920/482500 (29%)]\tLoss: 3.836765\n",
      "Train Epoch: 0 [138080/482500 (29%)]\tLoss: 3.052205\n",
      "Train Epoch: 0 [138240/482500 (29%)]\tLoss: 3.205837\n",
      "Train Epoch: 0 [138400/482500 (29%)]\tLoss: 4.393300\n",
      "Train Epoch: 0 [138560/482500 (29%)]\tLoss: 3.896194\n",
      "Train Epoch: 0 [138720/482500 (29%)]\tLoss: 786.798523\n",
      "Train Epoch: 0 [138880/482500 (29%)]\tLoss: 139.343933\n",
      "Train Epoch: 0 [139040/482500 (29%)]\tLoss: 22.146589\n",
      "Train Epoch: 0 [139200/482500 (29%)]\tLoss: 12.286314\n",
      "Train Epoch: 0 [139360/482500 (29%)]\tLoss: 4.826234\n",
      "Train Epoch: 0 [139520/482500 (29%)]\tLoss: 4.561266\n",
      "Train Epoch: 0 [139680/482500 (29%)]\tLoss: 6.159046\n",
      "Train Epoch: 0 [139840/482500 (29%)]\tLoss: 2.812861\n",
      "Train Epoch: 0 [140000/482500 (29%)]\tLoss: 7.173234\n",
      "Train Epoch: 0 [140160/482500 (29%)]\tLoss: 5.006457\n",
      "Train Epoch: 0 [140320/482500 (29%)]\tLoss: 1.995481\n",
      "Train Epoch: 0 [140480/482500 (29%)]\tLoss: 2.939776\n",
      "Train Epoch: 0 [140640/482500 (29%)]\tLoss: 4.472885\n",
      "Train Epoch: 0 [140800/482500 (29%)]\tLoss: 4.162272\n",
      "Train Epoch: 0 [140960/482500 (29%)]\tLoss: 2.023309\n",
      "Train Epoch: 0 [141120/482500 (29%)]\tLoss: 4.046735\n",
      "Train Epoch: 0 [141280/482500 (29%)]\tLoss: 3.005522\n",
      "Train Epoch: 0 [141440/482500 (29%)]\tLoss: 4.057857\n",
      "Train Epoch: 0 [141600/482500 (29%)]\tLoss: 4.704906\n",
      "Train Epoch: 0 [141760/482500 (29%)]\tLoss: 2.662003\n",
      "Train Epoch: 0 [141920/482500 (29%)]\tLoss: 3.125422\n",
      "Train Epoch: 0 [142080/482500 (29%)]\tLoss: 3.155711\n",
      "Train Epoch: 0 [142240/482500 (29%)]\tLoss: 1.958647\n",
      "Train Epoch: 0 [142400/482500 (30%)]\tLoss: 3.219414\n",
      "Train Epoch: 0 [142560/482500 (30%)]\tLoss: 1.997540\n",
      "Train Epoch: 0 [142720/482500 (30%)]\tLoss: 2.208308\n",
      "Train Epoch: 0 [142880/482500 (30%)]\tLoss: 2.914418\n",
      "Train Epoch: 0 [143040/482500 (30%)]\tLoss: 36.972198\n",
      "Train Epoch: 0 [143200/482500 (30%)]\tLoss: 6.653695\n",
      "Train Epoch: 0 [143360/482500 (30%)]\tLoss: 4.804936\n",
      "Train Epoch: 0 [143520/482500 (30%)]\tLoss: 2.196850\n",
      "Train Epoch: 0 [143680/482500 (30%)]\tLoss: 2.541004\n",
      "Train Epoch: 0 [143840/482500 (30%)]\tLoss: 3.762664\n",
      "Train Epoch: 0 [144000/482500 (30%)]\tLoss: 3.655727\n",
      "Train Epoch: 0 [144160/482500 (30%)]\tLoss: 2.987637\n",
      "Train Epoch: 0 [144320/482500 (30%)]\tLoss: 3.001735\n",
      "Train Epoch: 0 [144480/482500 (30%)]\tLoss: 1.345943\n",
      "Train Epoch: 0 [144640/482500 (30%)]\tLoss: 4.411742\n",
      "Train Epoch: 0 [144800/482500 (30%)]\tLoss: 3.230763\n",
      "Train Epoch: 0 [144960/482500 (30%)]\tLoss: 1.714915\n",
      "Train Epoch: 0 [145120/482500 (30%)]\tLoss: 2.301609\n",
      "Train Epoch: 0 [145280/482500 (30%)]\tLoss: 2.750474\n",
      "Train Epoch: 0 [145440/482500 (30%)]\tLoss: 2.364882\n",
      "Train Epoch: 0 [145600/482500 (30%)]\tLoss: 1.285973\n",
      "Train Epoch: 0 [145760/482500 (30%)]\tLoss: 1.606195\n",
      "Train Epoch: 0 [145920/482500 (30%)]\tLoss: 3.822180\n",
      "Train Epoch: 0 [146080/482500 (30%)]\tLoss: 3.006202\n",
      "Train Epoch: 0 [146240/482500 (30%)]\tLoss: 2.540244\n",
      "Train Epoch: 0 [146400/482500 (30%)]\tLoss: 2.643839\n",
      "Train Epoch: 0 [146560/482500 (30%)]\tLoss: 3.123197\n",
      "Train Epoch: 0 [146720/482500 (30%)]\tLoss: 2.566130\n",
      "Train Epoch: 0 [146880/482500 (30%)]\tLoss: 2.663679\n",
      "Train Epoch: 0 [147040/482500 (30%)]\tLoss: 3.629958\n",
      "Train Epoch: 0 [147200/482500 (31%)]\tLoss: 2.734170\n",
      "Train Epoch: 0 [147360/482500 (31%)]\tLoss: 3.309441\n",
      "Train Epoch: 0 [147520/482500 (31%)]\tLoss: 1.625458\n",
      "Train Epoch: 0 [147680/482500 (31%)]\tLoss: 2.717755\n",
      "Train Epoch: 0 [147840/482500 (31%)]\tLoss: 1.577186\n",
      "Train Epoch: 0 [148000/482500 (31%)]\tLoss: 1.596379\n",
      "Train Epoch: 0 [148160/482500 (31%)]\tLoss: 1.771031\n",
      "Train Epoch: 0 [148320/482500 (31%)]\tLoss: 2.164398\n",
      "Train Epoch: 0 [148480/482500 (31%)]\tLoss: 2.188332\n",
      "Train Epoch: 0 [148640/482500 (31%)]\tLoss: 0.968330\n",
      "Train Epoch: 0 [148800/482500 (31%)]\tLoss: 4.712226\n",
      "Train Epoch: 0 [148960/482500 (31%)]\tLoss: 2.132215\n",
      "Train Epoch: 0 [149120/482500 (31%)]\tLoss: 3.009230\n",
      "Train Epoch: 0 [149280/482500 (31%)]\tLoss: 1.814802\n",
      "Train Epoch: 0 [149440/482500 (31%)]\tLoss: 1.495170\n",
      "Train Epoch: 0 [149600/482500 (31%)]\tLoss: 2.840349\n",
      "Train Epoch: 0 [149760/482500 (31%)]\tLoss: 1.609630\n",
      "Train Epoch: 0 [149920/482500 (31%)]\tLoss: 2.670250\n",
      "Train Epoch: 0 [150080/482500 (31%)]\tLoss: 2.779053\n",
      "Train Epoch: 0 [150240/482500 (31%)]\tLoss: 1.372777\n",
      "Train Epoch: 0 [150400/482500 (31%)]\tLoss: 1.301455\n",
      "Train Epoch: 0 [150560/482500 (31%)]\tLoss: 4.280657\n",
      "Train Epoch: 0 [150720/482500 (31%)]\tLoss: 2.224762\n",
      "Train Epoch: 0 [150880/482500 (31%)]\tLoss: 9.215445\n",
      "Train Epoch: 0 [151040/482500 (31%)]\tLoss: 3.449734\n",
      "Train Epoch: 0 [151200/482500 (31%)]\tLoss: 1.646532\n",
      "Train Epoch: 0 [151360/482500 (31%)]\tLoss: 2.685115\n",
      "Train Epoch: 0 [151520/482500 (31%)]\tLoss: 3.042261\n",
      "Train Epoch: 0 [151680/482500 (31%)]\tLoss: 1.494655\n",
      "Train Epoch: 0 [151840/482500 (31%)]\tLoss: 2.982178\n",
      "Train Epoch: 0 [152000/482500 (32%)]\tLoss: 2.224656\n",
      "Train Epoch: 0 [152160/482500 (32%)]\tLoss: 2.248606\n",
      "Train Epoch: 0 [152320/482500 (32%)]\tLoss: 2.203398\n",
      "Train Epoch: 0 [152480/482500 (32%)]\tLoss: 1.206622\n",
      "Train Epoch: 0 [152640/482500 (32%)]\tLoss: 1.397511\n",
      "Train Epoch: 0 [152800/482500 (32%)]\tLoss: 1.917444\n",
      "Train Epoch: 0 [152960/482500 (32%)]\tLoss: 1.846090\n",
      "Train Epoch: 0 [153120/482500 (32%)]\tLoss: 2.580670\n",
      "Train Epoch: 0 [153280/482500 (32%)]\tLoss: 1.703449\n",
      "Train Epoch: 0 [153440/482500 (32%)]\tLoss: 1.791921\n",
      "Train Epoch: 0 [153600/482500 (32%)]\tLoss: 1.737193\n",
      "Train Epoch: 0 [153760/482500 (32%)]\tLoss: 2.015406\n",
      "Train Epoch: 0 [153920/482500 (32%)]\tLoss: 2.561762\n",
      "Train Epoch: 0 [154080/482500 (32%)]\tLoss: 6.953874\n",
      "Train Epoch: 0 [154240/482500 (32%)]\tLoss: 1.814786\n",
      "Train Epoch: 0 [154400/482500 (32%)]\tLoss: 1.529013\n",
      "Train Epoch: 0 [154560/482500 (32%)]\tLoss: 1.772734\n",
      "Train Epoch: 0 [154720/482500 (32%)]\tLoss: 1.804958\n",
      "Train Epoch: 0 [154880/482500 (32%)]\tLoss: 0.596516\n",
      "Train Epoch: 0 [155040/482500 (32%)]\tLoss: 1.025964\n",
      "Train Epoch: 0 [155200/482500 (32%)]\tLoss: 2.000344\n",
      "Train Epoch: 0 [155360/482500 (32%)]\tLoss: 2.674048\n",
      "Train Epoch: 0 [155520/482500 (32%)]\tLoss: 2.939698\n",
      "Train Epoch: 0 [155680/482500 (32%)]\tLoss: 2.292339\n",
      "Train Epoch: 0 [155840/482500 (32%)]\tLoss: 5.441235\n",
      "Train Epoch: 0 [156000/482500 (32%)]\tLoss: 6.844069\n",
      "Train Epoch: 0 [156160/482500 (32%)]\tLoss: 1.273213\n",
      "Train Epoch: 0 [156320/482500 (32%)]\tLoss: 2.690115\n",
      "Train Epoch: 0 [156480/482500 (32%)]\tLoss: 1.428080\n",
      "Train Epoch: 0 [156640/482500 (32%)]\tLoss: 1.489446\n",
      "Train Epoch: 0 [156800/482500 (32%)]\tLoss: 2.317927\n",
      "Train Epoch: 0 [156960/482500 (33%)]\tLoss: 2.646397\n",
      "Train Epoch: 0 [157120/482500 (33%)]\tLoss: 1.769013\n",
      "Train Epoch: 0 [157280/482500 (33%)]\tLoss: 1.674481\n",
      "Train Epoch: 0 [157440/482500 (33%)]\tLoss: 1.404119\n",
      "Train Epoch: 0 [157600/482500 (33%)]\tLoss: 1.888492\n",
      "Train Epoch: 0 [157760/482500 (33%)]\tLoss: 1.800470\n",
      "Train Epoch: 0 [157920/482500 (33%)]\tLoss: 2.871903\n",
      "Train Epoch: 0 [158080/482500 (33%)]\tLoss: 1.405210\n",
      "Train Epoch: 0 [158240/482500 (33%)]\tLoss: 1.845315\n",
      "Train Epoch: 0 [158400/482500 (33%)]\tLoss: 1.528545\n",
      "Train Epoch: 0 [158560/482500 (33%)]\tLoss: 2.198083\n",
      "Train Epoch: 0 [158720/482500 (33%)]\tLoss: 1.789086\n",
      "Train Epoch: 0 [158880/482500 (33%)]\tLoss: 1.669433\n",
      "Train Epoch: 0 [159040/482500 (33%)]\tLoss: 1.001127\n",
      "Train Epoch: 0 [159200/482500 (33%)]\tLoss: 1.449732\n",
      "Train Epoch: 0 [159360/482500 (33%)]\tLoss: 1584.447388\n",
      "Train Epoch: 0 [159520/482500 (33%)]\tLoss: 60.826942\n",
      "Train Epoch: 0 [159680/482500 (33%)]\tLoss: 24.580568\n",
      "Train Epoch: 0 [159840/482500 (33%)]\tLoss: 6.503582\n",
      "Train Epoch: 0 [160000/482500 (33%)]\tLoss: 5.840132\n",
      "Train Epoch: 0 [160160/482500 (33%)]\tLoss: 4.681774\n",
      "Train Epoch: 0 [160320/482500 (33%)]\tLoss: 4.861850\n",
      "Train Epoch: 0 [160480/482500 (33%)]\tLoss: 2.469079\n",
      "Train Epoch: 0 [160640/482500 (33%)]\tLoss: 3.087694\n",
      "Train Epoch: 0 [160800/482500 (33%)]\tLoss: 15.868229\n",
      "Train Epoch: 0 [160960/482500 (33%)]\tLoss: 1503.582275\n",
      "Train Epoch: 0 [161120/482500 (33%)]\tLoss: 61.040398\n",
      "Train Epoch: 0 [161280/482500 (33%)]\tLoss: 19.543236\n",
      "Train Epoch: 0 [161440/482500 (33%)]\tLoss: 7.475821\n",
      "Train Epoch: 0 [161600/482500 (33%)]\tLoss: 5.855290\n",
      "Train Epoch: 0 [161760/482500 (34%)]\tLoss: 5.699137\n",
      "Train Epoch: 0 [161920/482500 (34%)]\tLoss: 286.944458\n",
      "Train Epoch: 0 [162080/482500 (34%)]\tLoss: 2454.320801\n",
      "Train Epoch: 0 [162240/482500 (34%)]\tLoss: 92.712151\n",
      "Train Epoch: 0 [162400/482500 (34%)]\tLoss: 31.499641\n",
      "Train Epoch: 0 [162560/482500 (34%)]\tLoss: 4.401248\n",
      "Train Epoch: 0 [162720/482500 (34%)]\tLoss: 17.701956\n",
      "Train Epoch: 0 [162880/482500 (34%)]\tLoss: 28.957050\n",
      "Train Epoch: 0 [163040/482500 (34%)]\tLoss: 7.117423\n",
      "Train Epoch: 0 [163200/482500 (34%)]\tLoss: 2.880812\n",
      "Train Epoch: 0 [163360/482500 (34%)]\tLoss: 6.566218\n",
      "Train Epoch: 0 [163520/482500 (34%)]\tLoss: 4.670440\n",
      "Train Epoch: 0 [163680/482500 (34%)]\tLoss: 6.187572\n",
      "Train Epoch: 0 [163840/482500 (34%)]\tLoss: 5.108910\n",
      "Train Epoch: 0 [164000/482500 (34%)]\tLoss: 4.935846\n",
      "Train Epoch: 0 [164160/482500 (34%)]\tLoss: 2.360743\n",
      "Train Epoch: 0 [164320/482500 (34%)]\tLoss: 6.220534\n",
      "Train Epoch: 0 [164480/482500 (34%)]\tLoss: 23.715937\n",
      "Train Epoch: 0 [164640/482500 (34%)]\tLoss: 5.620828\n",
      "Train Epoch: 0 [164800/482500 (34%)]\tLoss: 4.951591\n",
      "Train Epoch: 0 [164960/482500 (34%)]\tLoss: 5.012110\n",
      "Train Epoch: 0 [165120/482500 (34%)]\tLoss: 544.133789\n",
      "Train Epoch: 0 [165280/482500 (34%)]\tLoss: 25.869366\n",
      "Train Epoch: 0 [165440/482500 (34%)]\tLoss: 10.364008\n",
      "Train Epoch: 0 [165600/482500 (34%)]\tLoss: 9.550955\n",
      "Train Epoch: 0 [165760/482500 (34%)]\tLoss: 7.079518\n",
      "Train Epoch: 0 [165920/482500 (34%)]\tLoss: 7.975850\n",
      "Train Epoch: 0 [166080/482500 (34%)]\tLoss: 9.917130\n",
      "Train Epoch: 0 [166240/482500 (34%)]\tLoss: 6.852958\n",
      "Train Epoch: 0 [166400/482500 (34%)]\tLoss: 6.105721\n",
      "Train Epoch: 0 [166560/482500 (35%)]\tLoss: 4.088232\n",
      "Train Epoch: 0 [166720/482500 (35%)]\tLoss: 6.006678\n",
      "Train Epoch: 0 [166880/482500 (35%)]\tLoss: 3.695103\n",
      "Train Epoch: 0 [167040/482500 (35%)]\tLoss: 4.683895\n",
      "Train Epoch: 0 [167200/482500 (35%)]\tLoss: 4.359290\n",
      "Train Epoch: 0 [167360/482500 (35%)]\tLoss: 5.051569\n",
      "Train Epoch: 0 [167520/482500 (35%)]\tLoss: 5.212544\n",
      "Train Epoch: 0 [167680/482500 (35%)]\tLoss: 4.463914\n",
      "Train Epoch: 0 [167840/482500 (35%)]\tLoss: 6.059553\n",
      "Train Epoch: 0 [168000/482500 (35%)]\tLoss: 6.279541\n",
      "Train Epoch: 0 [168160/482500 (35%)]\tLoss: 6.859227\n",
      "Train Epoch: 0 [168320/482500 (35%)]\tLoss: 3.265039\n",
      "Train Epoch: 0 [168480/482500 (35%)]\tLoss: 4.032202\n",
      "Train Epoch: 0 [168640/482500 (35%)]\tLoss: 6.397575\n",
      "Train Epoch: 0 [168800/482500 (35%)]\tLoss: 3.402133\n",
      "Train Epoch: 0 [168960/482500 (35%)]\tLoss: 1.887785\n",
      "Train Epoch: 0 [169120/482500 (35%)]\tLoss: 5.543646\n",
      "Train Epoch: 0 [169280/482500 (35%)]\tLoss: 4.196047\n",
      "Train Epoch: 0 [169440/482500 (35%)]\tLoss: 2.226865\n",
      "Train Epoch: 0 [169600/482500 (35%)]\tLoss: 3.526869\n",
      "Train Epoch: 0 [169760/482500 (35%)]\tLoss: 2.066803\n",
      "Train Epoch: 0 [169920/482500 (35%)]\tLoss: 3.234323\n",
      "Train Epoch: 0 [170080/482500 (35%)]\tLoss: 3.383443\n",
      "Train Epoch: 0 [170240/482500 (35%)]\tLoss: 2.587979\n",
      "Train Epoch: 0 [170400/482500 (35%)]\tLoss: 2.640498\n",
      "Train Epoch: 0 [170560/482500 (35%)]\tLoss: 3.641568\n",
      "Train Epoch: 0 [170720/482500 (35%)]\tLoss: 2.681283\n",
      "Train Epoch: 0 [170880/482500 (35%)]\tLoss: 2.553101\n",
      "Train Epoch: 0 [171040/482500 (35%)]\tLoss: 1.479035\n",
      "Train Epoch: 0 [171200/482500 (35%)]\tLoss: 2.526337\n",
      "Train Epoch: 0 [171360/482500 (36%)]\tLoss: 3.612468\n",
      "Train Epoch: 0 [171520/482500 (36%)]\tLoss: 1.762549\n",
      "Train Epoch: 0 [171680/482500 (36%)]\tLoss: 1.115217\n",
      "Train Epoch: 0 [171840/482500 (36%)]\tLoss: 2.314693\n",
      "Train Epoch: 0 [172000/482500 (36%)]\tLoss: 2.599833\n",
      "Train Epoch: 0 [172160/482500 (36%)]\tLoss: 3.540836\n",
      "Train Epoch: 0 [172320/482500 (36%)]\tLoss: 1.735110\n",
      "Train Epoch: 0 [172480/482500 (36%)]\tLoss: 3.462636\n",
      "Train Epoch: 0 [172640/482500 (36%)]\tLoss: 1.453218\n",
      "Train Epoch: 0 [172800/482500 (36%)]\tLoss: 3.348502\n",
      "Train Epoch: 0 [172960/482500 (36%)]\tLoss: 1.780023\n",
      "Train Epoch: 0 [173120/482500 (36%)]\tLoss: 2.838074\n",
      "Train Epoch: 0 [173280/482500 (36%)]\tLoss: 1.994041\n",
      "Train Epoch: 0 [173440/482500 (36%)]\tLoss: 2.051135\n",
      "Train Epoch: 0 [173600/482500 (36%)]\tLoss: 3.390831\n",
      "Train Epoch: 0 [173760/482500 (36%)]\tLoss: 2.835923\n",
      "Train Epoch: 0 [173920/482500 (36%)]\tLoss: 2.903371\n",
      "Train Epoch: 0 [174080/482500 (36%)]\tLoss: 2.468728\n",
      "Train Epoch: 0 [174240/482500 (36%)]\tLoss: 3.282797\n",
      "Train Epoch: 0 [174400/482500 (36%)]\tLoss: 3.674409\n",
      "Train Epoch: 0 [174560/482500 (36%)]\tLoss: 1.232959\n",
      "Train Epoch: 0 [174720/482500 (36%)]\tLoss: 3.460868\n",
      "Train Epoch: 0 [174880/482500 (36%)]\tLoss: 1.887973\n",
      "Train Epoch: 0 [175040/482500 (36%)]\tLoss: 7.638155\n",
      "Train Epoch: 0 [175200/482500 (36%)]\tLoss: 2.669135\n",
      "Train Epoch: 0 [175360/482500 (36%)]\tLoss: 2.769845\n",
      "Train Epoch: 0 [175520/482500 (36%)]\tLoss: 4.019313\n",
      "Train Epoch: 0 [175680/482500 (36%)]\tLoss: 3.892428\n",
      "Train Epoch: 0 [175840/482500 (36%)]\tLoss: 2.899176\n",
      "Train Epoch: 0 [176000/482500 (36%)]\tLoss: 3.807575\n",
      "Train Epoch: 0 [176160/482500 (37%)]\tLoss: 3.208585\n",
      "Train Epoch: 0 [176320/482500 (37%)]\tLoss: 0.820563\n",
      "Train Epoch: 0 [176480/482500 (37%)]\tLoss: 3.474872\n",
      "Train Epoch: 0 [176640/482500 (37%)]\tLoss: 2.080609\n",
      "Train Epoch: 0 [176800/482500 (37%)]\tLoss: 1.018536\n",
      "Train Epoch: 0 [176960/482500 (37%)]\tLoss: 1.696714\n",
      "Train Epoch: 0 [177120/482500 (37%)]\tLoss: 2.014301\n",
      "Train Epoch: 0 [177280/482500 (37%)]\tLoss: 2.078075\n",
      "Train Epoch: 0 [177440/482500 (37%)]\tLoss: 2.061586\n",
      "Train Epoch: 0 [177600/482500 (37%)]\tLoss: 1.441570\n",
      "Train Epoch: 0 [177760/482500 (37%)]\tLoss: 1.835514\n",
      "Train Epoch: 0 [177920/482500 (37%)]\tLoss: 3.435859\n",
      "Train Epoch: 0 [178080/482500 (37%)]\tLoss: 2.306869\n",
      "Train Epoch: 0 [178240/482500 (37%)]\tLoss: 2.531384\n",
      "Train Epoch: 0 [178400/482500 (37%)]\tLoss: 2.278574\n",
      "Train Epoch: 0 [178560/482500 (37%)]\tLoss: 2.816523\n",
      "Train Epoch: 0 [178720/482500 (37%)]\tLoss: 2.235263\n",
      "Train Epoch: 0 [178880/482500 (37%)]\tLoss: 1.423010\n",
      "Train Epoch: 0 [179040/482500 (37%)]\tLoss: 2.937013\n",
      "Train Epoch: 0 [179200/482500 (37%)]\tLoss: 1.811237\n",
      "Train Epoch: 0 [179360/482500 (37%)]\tLoss: 1.733577\n",
      "Train Epoch: 0 [179520/482500 (37%)]\tLoss: 2.226521\n",
      "Train Epoch: 0 [179680/482500 (37%)]\tLoss: 3.369873\n",
      "Train Epoch: 0 [179840/482500 (37%)]\tLoss: 2.617627\n",
      "Train Epoch: 0 [180000/482500 (37%)]\tLoss: 2.079187\n",
      "Train Epoch: 0 [180160/482500 (37%)]\tLoss: 1.611794\n",
      "Train Epoch: 0 [180320/482500 (37%)]\tLoss: 2.572408\n",
      "Train Epoch: 0 [180480/482500 (37%)]\tLoss: 1.995577\n",
      "Train Epoch: 0 [180640/482500 (37%)]\tLoss: 1.991791\n",
      "Train Epoch: 0 [180800/482500 (37%)]\tLoss: 1.390924\n",
      "Train Epoch: 0 [180960/482500 (38%)]\tLoss: 2.227727\n",
      "Train Epoch: 0 [181120/482500 (38%)]\tLoss: 2.021409\n",
      "Train Epoch: 0 [181280/482500 (38%)]\tLoss: 1.680832\n",
      "Train Epoch: 0 [181440/482500 (38%)]\tLoss: 2.457154\n",
      "Train Epoch: 0 [181600/482500 (38%)]\tLoss: 1.172583\n",
      "Train Epoch: 0 [181760/482500 (38%)]\tLoss: 2.325142\n",
      "Train Epoch: 0 [181920/482500 (38%)]\tLoss: 2.145654\n",
      "Train Epoch: 0 [182080/482500 (38%)]\tLoss: 1.469116\n",
      "Train Epoch: 0 [182240/482500 (38%)]\tLoss: 1.642091\n",
      "Train Epoch: 0 [182400/482500 (38%)]\tLoss: 1.484151\n",
      "Train Epoch: 0 [182560/482500 (38%)]\tLoss: 2.542351\n",
      "Train Epoch: 0 [182720/482500 (38%)]\tLoss: 2.988661\n",
      "Train Epoch: 0 [182880/482500 (38%)]\tLoss: 1.577240\n",
      "Train Epoch: 0 [183040/482500 (38%)]\tLoss: 1.695220\n",
      "Train Epoch: 0 [183200/482500 (38%)]\tLoss: 2.985829\n",
      "Train Epoch: 0 [183360/482500 (38%)]\tLoss: 1.490560\n",
      "Train Epoch: 0 [183520/482500 (38%)]\tLoss: 0.980662\n",
      "Train Epoch: 0 [183680/482500 (38%)]\tLoss: 1.018697\n",
      "Train Epoch: 0 [183840/482500 (38%)]\tLoss: 1.738128\n",
      "Train Epoch: 0 [184000/482500 (38%)]\tLoss: 2.376142\n",
      "Train Epoch: 0 [184160/482500 (38%)]\tLoss: 1.790429\n",
      "Train Epoch: 0 [184320/482500 (38%)]\tLoss: 3.140291\n",
      "Train Epoch: 0 [184480/482500 (38%)]\tLoss: 2.280387\n",
      "Train Epoch: 0 [184640/482500 (38%)]\tLoss: 1.800078\n",
      "Train Epoch: 0 [184800/482500 (38%)]\tLoss: 1.636686\n",
      "Train Epoch: 0 [184960/482500 (38%)]\tLoss: 1.680663\n",
      "Train Epoch: 0 [185120/482500 (38%)]\tLoss: 1.472303\n",
      "Train Epoch: 0 [185280/482500 (38%)]\tLoss: 1.504081\n",
      "Train Epoch: 0 [185440/482500 (38%)]\tLoss: 1.254512\n",
      "Train Epoch: 0 [185600/482500 (38%)]\tLoss: 1.940097\n",
      "Train Epoch: 0 [185760/482500 (38%)]\tLoss: 2.030664\n",
      "Train Epoch: 0 [185920/482500 (39%)]\tLoss: 1.624170\n",
      "Train Epoch: 0 [186080/482500 (39%)]\tLoss: 1.833051\n",
      "Train Epoch: 0 [186240/482500 (39%)]\tLoss: 3.476170\n",
      "Train Epoch: 0 [186400/482500 (39%)]\tLoss: 1.464694\n",
      "Train Epoch: 0 [186560/482500 (39%)]\tLoss: 2.148052\n",
      "Train Epoch: 0 [186720/482500 (39%)]\tLoss: 1.825280\n",
      "Train Epoch: 0 [186880/482500 (39%)]\tLoss: 2.060114\n",
      "Train Epoch: 0 [187040/482500 (39%)]\tLoss: 2.072574\n",
      "Train Epoch: 0 [187200/482500 (39%)]\tLoss: 1.993469\n",
      "Train Epoch: 0 [187360/482500 (39%)]\tLoss: 1.308893\n",
      "Train Epoch: 0 [187520/482500 (39%)]\tLoss: 1.718063\n",
      "Train Epoch: 0 [187680/482500 (39%)]\tLoss: 2.103733\n",
      "Train Epoch: 0 [187840/482500 (39%)]\tLoss: 1.233857\n",
      "Train Epoch: 0 [188000/482500 (39%)]\tLoss: 36.666916\n",
      "Train Epoch: 0 [188160/482500 (39%)]\tLoss: 9.991503\n",
      "Train Epoch: 0 [188320/482500 (39%)]\tLoss: 3.292892\n",
      "Train Epoch: 0 [188480/482500 (39%)]\tLoss: 2.985186\n",
      "Train Epoch: 0 [188640/482500 (39%)]\tLoss: 2.320324\n",
      "Train Epoch: 0 [188800/482500 (39%)]\tLoss: 1.662486\n",
      "Train Epoch: 0 [188960/482500 (39%)]\tLoss: 2.616041\n",
      "Train Epoch: 0 [189120/482500 (39%)]\tLoss: 2.394294\n",
      "Train Epoch: 0 [189280/482500 (39%)]\tLoss: 1.304354\n",
      "Train Epoch: 0 [189440/482500 (39%)]\tLoss: 1.477997\n",
      "Train Epoch: 0 [189600/482500 (39%)]\tLoss: 2.100549\n",
      "Train Epoch: 0 [189760/482500 (39%)]\tLoss: 1.738380\n",
      "Train Epoch: 0 [189920/482500 (39%)]\tLoss: 2.552545\n",
      "Train Epoch: 0 [190080/482500 (39%)]\tLoss: 1.433938\n",
      "Train Epoch: 0 [190240/482500 (39%)]\tLoss: 1.036591\n",
      "Train Epoch: 0 [190400/482500 (39%)]\tLoss: 1.032272\n",
      "Train Epoch: 0 [190560/482500 (39%)]\tLoss: 1.220185\n",
      "Train Epoch: 0 [190720/482500 (40%)]\tLoss: 1.558582\n",
      "Train Epoch: 0 [190880/482500 (40%)]\tLoss: 1.235072\n",
      "Train Epoch: 0 [191040/482500 (40%)]\tLoss: 1.712756\n",
      "Train Epoch: 0 [191200/482500 (40%)]\tLoss: 1.435417\n",
      "Train Epoch: 0 [191360/482500 (40%)]\tLoss: 1.408105\n",
      "Train Epoch: 0 [191520/482500 (40%)]\tLoss: 1.598172\n",
      "Train Epoch: 0 [191680/482500 (40%)]\tLoss: 0.780591\n",
      "Train Epoch: 0 [191840/482500 (40%)]\tLoss: 1.575722\n",
      "Train Epoch: 0 [192000/482500 (40%)]\tLoss: 3335.503662\n",
      "Train Epoch: 0 [192160/482500 (40%)]\tLoss: 154.081009\n",
      "Train Epoch: 0 [192320/482500 (40%)]\tLoss: 17.844732\n",
      "Train Epoch: 0 [192480/482500 (40%)]\tLoss: 5.834179\n",
      "Train Epoch: 0 [192640/482500 (40%)]\tLoss: 13.426284\n",
      "Train Epoch: 0 [192800/482500 (40%)]\tLoss: 5.681745\n",
      "Train Epoch: 0 [192960/482500 (40%)]\tLoss: 3.616673\n",
      "Train Epoch: 0 [193120/482500 (40%)]\tLoss: 3.129641\n",
      "Train Epoch: 0 [193280/482500 (40%)]\tLoss: 4.108271\n",
      "Train Epoch: 0 [193440/482500 (40%)]\tLoss: 1.668965\n",
      "Train Epoch: 0 [193600/482500 (40%)]\tLoss: 3.146276\n",
      "Train Epoch: 0 [193760/482500 (40%)]\tLoss: 1.459891\n",
      "Train Epoch: 0 [193920/482500 (40%)]\tLoss: 3.008396\n",
      "Train Epoch: 0 [194080/482500 (40%)]\tLoss: 1.625086\n",
      "Train Epoch: 0 [194240/482500 (40%)]\tLoss: 2.108639\n",
      "Train Epoch: 0 [194400/482500 (40%)]\tLoss: 2.818423\n",
      "Train Epoch: 0 [194560/482500 (40%)]\tLoss: 3.369654\n",
      "Train Epoch: 0 [194720/482500 (40%)]\tLoss: 3.005430\n",
      "Train Epoch: 0 [194880/482500 (40%)]\tLoss: 1.909094\n",
      "Train Epoch: 0 [195040/482500 (40%)]\tLoss: 1.377791\n",
      "Train Epoch: 0 [195200/482500 (40%)]\tLoss: 2.327650\n",
      "Train Epoch: 0 [195360/482500 (40%)]\tLoss: 1.904169\n",
      "Train Epoch: 0 [195520/482500 (41%)]\tLoss: 2.431384\n",
      "Train Epoch: 0 [195680/482500 (41%)]\tLoss: 1.597855\n",
      "Train Epoch: 0 [195840/482500 (41%)]\tLoss: 1.478054\n",
      "Train Epoch: 0 [196000/482500 (41%)]\tLoss: 4.225571\n",
      "Train Epoch: 0 [196160/482500 (41%)]\tLoss: 1.092274\n",
      "Train Epoch: 0 [196320/482500 (41%)]\tLoss: 1.372221\n",
      "Train Epoch: 0 [196480/482500 (41%)]\tLoss: 1.586441\n",
      "Train Epoch: 0 [196640/482500 (41%)]\tLoss: 2.213697\n",
      "Train Epoch: 0 [196800/482500 (41%)]\tLoss: 1.661113\n",
      "Train Epoch: 0 [196960/482500 (41%)]\tLoss: 1.490917\n",
      "Train Epoch: 0 [197120/482500 (41%)]\tLoss: 1.733540\n",
      "Train Epoch: 0 [197280/482500 (41%)]\tLoss: 1.450561\n",
      "Train Epoch: 0 [197440/482500 (41%)]\tLoss: 1.120733\n",
      "Train Epoch: 0 [197600/482500 (41%)]\tLoss: 1.920862\n",
      "Train Epoch: 0 [197760/482500 (41%)]\tLoss: 1.744580\n",
      "Train Epoch: 0 [197920/482500 (41%)]\tLoss: 1.203895\n",
      "Train Epoch: 0 [198080/482500 (41%)]\tLoss: 1.262083\n",
      "Train Epoch: 0 [198240/482500 (41%)]\tLoss: 1.937464\n",
      "Train Epoch: 0 [198400/482500 (41%)]\tLoss: 1.152417\n",
      "Train Epoch: 0 [198560/482500 (41%)]\tLoss: 1.515753\n",
      "Train Epoch: 0 [198720/482500 (41%)]\tLoss: 2.989226\n",
      "Train Epoch: 0 [198880/482500 (41%)]\tLoss: 1.171142\n",
      "Train Epoch: 0 [199040/482500 (41%)]\tLoss: 1.130677\n",
      "Train Epoch: 0 [199200/482500 (41%)]\tLoss: 1.685606\n",
      "Train Epoch: 0 [199360/482500 (41%)]\tLoss: 1.305608\n",
      "Train Epoch: 0 [199520/482500 (41%)]\tLoss: 1.665860\n",
      "Train Epoch: 0 [199680/482500 (41%)]\tLoss: 82.897697\n",
      "Train Epoch: 0 [199840/482500 (41%)]\tLoss: 12.788435\n",
      "Train Epoch: 0 [200000/482500 (41%)]\tLoss: 8.356668\n",
      "Train Epoch: 0 [200160/482500 (41%)]\tLoss: 3.950277\n",
      "Train Epoch: 0 [200320/482500 (42%)]\tLoss: 1.767446\n",
      "Train Epoch: 0 [200480/482500 (42%)]\tLoss: 0.939926\n",
      "Train Epoch: 0 [200640/482500 (42%)]\tLoss: 1.249787\n",
      "Train Epoch: 0 [200800/482500 (42%)]\tLoss: 1.796636\n",
      "Train Epoch: 0 [200960/482500 (42%)]\tLoss: 2.728081\n",
      "Train Epoch: 0 [201120/482500 (42%)]\tLoss: 2.178510\n",
      "Train Epoch: 0 [201280/482500 (42%)]\tLoss: 1.593408\n",
      "Train Epoch: 0 [201440/482500 (42%)]\tLoss: 1.557964\n",
      "Train Epoch: 0 [201600/482500 (42%)]\tLoss: 1.536325\n",
      "Train Epoch: 0 [201760/482500 (42%)]\tLoss: 2.248322\n",
      "Train Epoch: 0 [201920/482500 (42%)]\tLoss: 2907.431885\n",
      "Train Epoch: 0 [202080/482500 (42%)]\tLoss: 630.139709\n",
      "Train Epoch: 0 [202240/482500 (42%)]\tLoss: 99.659500\n",
      "Train Epoch: 0 [202400/482500 (42%)]\tLoss: 30.489004\n",
      "Train Epoch: 0 [202560/482500 (42%)]\tLoss: 12.109558\n",
      "Train Epoch: 0 [202720/482500 (42%)]\tLoss: 17.834143\n",
      "Train Epoch: 0 [202880/482500 (42%)]\tLoss: 9.193714\n",
      "Train Epoch: 0 [203040/482500 (42%)]\tLoss: 5.601013\n",
      "Train Epoch: 0 [203200/482500 (42%)]\tLoss: 7.099326\n",
      "Train Epoch: 0 [203360/482500 (42%)]\tLoss: 6.546827\n",
      "Train Epoch: 0 [203520/482500 (42%)]\tLoss: 7.944913\n",
      "Train Epoch: 0 [203680/482500 (42%)]\tLoss: 6.113177\n",
      "Train Epoch: 0 [203840/482500 (42%)]\tLoss: 4.203975\n",
      "Train Epoch: 0 [204000/482500 (42%)]\tLoss: 17.271549\n",
      "Train Epoch: 0 [204160/482500 (42%)]\tLoss: 7.787168\n",
      "Train Epoch: 0 [204320/482500 (42%)]\tLoss: 7.017714\n",
      "Train Epoch: 0 [204480/482500 (42%)]\tLoss: 4.085212\n",
      "Train Epoch: 0 [204640/482500 (42%)]\tLoss: 10.244820\n",
      "Train Epoch: 0 [204800/482500 (42%)]\tLoss: 3.113190\n",
      "Train Epoch: 0 [204960/482500 (42%)]\tLoss: 5.822267\n",
      "Train Epoch: 0 [205120/482500 (43%)]\tLoss: 4.689723\n",
      "Train Epoch: 0 [205280/482500 (43%)]\tLoss: 5.152478\n",
      "Train Epoch: 0 [205440/482500 (43%)]\tLoss: 8.315886\n",
      "Train Epoch: 0 [205600/482500 (43%)]\tLoss: 5.111890\n",
      "Train Epoch: 0 [205760/482500 (43%)]\tLoss: 4.137507\n",
      "Train Epoch: 0 [205920/482500 (43%)]\tLoss: 4.349966\n",
      "Train Epoch: 0 [206080/482500 (43%)]\tLoss: 7.389092\n",
      "Train Epoch: 0 [206240/482500 (43%)]\tLoss: 3.504539\n",
      "Train Epoch: 0 [206400/482500 (43%)]\tLoss: 6.516795\n",
      "Train Epoch: 0 [206560/482500 (43%)]\tLoss: 5.310730\n",
      "Train Epoch: 0 [206720/482500 (43%)]\tLoss: 4.460511\n",
      "Train Epoch: 0 [206880/482500 (43%)]\tLoss: 4.706916\n",
      "Train Epoch: 0 [207040/482500 (43%)]\tLoss: 3.375395\n",
      "Train Epoch: 0 [207200/482500 (43%)]\tLoss: 4.491521\n",
      "Train Epoch: 0 [207360/482500 (43%)]\tLoss: 3.513484\n",
      "Train Epoch: 0 [207520/482500 (43%)]\tLoss: 3.446666\n",
      "Train Epoch: 0 [207680/482500 (43%)]\tLoss: 15.504833\n",
      "Train Epoch: 0 [207840/482500 (43%)]\tLoss: 5.774136\n",
      "Train Epoch: 0 [208000/482500 (43%)]\tLoss: 3.711326\n",
      "Train Epoch: 0 [208160/482500 (43%)]\tLoss: 5.628931\n",
      "Train Epoch: 0 [208320/482500 (43%)]\tLoss: 3.429253\n",
      "Train Epoch: 0 [208480/482500 (43%)]\tLoss: 2.433470\n",
      "Train Epoch: 0 [208640/482500 (43%)]\tLoss: 4.803208\n",
      "Train Epoch: 0 [208800/482500 (43%)]\tLoss: 4.309653\n",
      "Train Epoch: 0 [208960/482500 (43%)]\tLoss: 2.624038\n",
      "Train Epoch: 0 [209120/482500 (43%)]\tLoss: 4.715268\n",
      "Train Epoch: 0 [209280/482500 (43%)]\tLoss: 3.686233\n",
      "Train Epoch: 0 [209440/482500 (43%)]\tLoss: 4.733047\n",
      "Train Epoch: 0 [209600/482500 (43%)]\tLoss: 2.922269\n",
      "Train Epoch: 0 [209760/482500 (43%)]\tLoss: 8.055249\n",
      "Train Epoch: 0 [209920/482500 (44%)]\tLoss: 5.273255\n",
      "Train Epoch: 0 [210080/482500 (44%)]\tLoss: 2.444279\n",
      "Train Epoch: 0 [210240/482500 (44%)]\tLoss: 3.964358\n",
      "Train Epoch: 0 [210400/482500 (44%)]\tLoss: 1.928860\n",
      "Train Epoch: 0 [210560/482500 (44%)]\tLoss: 3.545444\n",
      "Train Epoch: 0 [210720/482500 (44%)]\tLoss: 2.650975\n",
      "Train Epoch: 0 [210880/482500 (44%)]\tLoss: 2.690572\n",
      "Train Epoch: 0 [211040/482500 (44%)]\tLoss: 1.492819\n",
      "Train Epoch: 0 [211200/482500 (44%)]\tLoss: 1.902565\n",
      "Train Epoch: 0 [211360/482500 (44%)]\tLoss: 2.786088\n",
      "Train Epoch: 0 [211520/482500 (44%)]\tLoss: 2.588948\n",
      "Train Epoch: 0 [211680/482500 (44%)]\tLoss: 3.081869\n",
      "Train Epoch: 0 [211840/482500 (44%)]\tLoss: 3.344901\n",
      "Train Epoch: 0 [212000/482500 (44%)]\tLoss: 4.445992\n",
      "Train Epoch: 0 [212160/482500 (44%)]\tLoss: 3.484996\n",
      "Train Epoch: 0 [212320/482500 (44%)]\tLoss: 1.986421\n",
      "Train Epoch: 0 [212480/482500 (44%)]\tLoss: 3.034595\n",
      "Train Epoch: 0 [212640/482500 (44%)]\tLoss: 2.859465\n",
      "Train Epoch: 0 [212800/482500 (44%)]\tLoss: 1.724267\n",
      "Train Epoch: 0 [212960/482500 (44%)]\tLoss: 1.986362\n",
      "Train Epoch: 0 [213120/482500 (44%)]\tLoss: 4.242015\n",
      "Train Epoch: 0 [213280/482500 (44%)]\tLoss: 3.185026\n",
      "Train Epoch: 0 [213440/482500 (44%)]\tLoss: 1.862399\n",
      "Train Epoch: 0 [213600/482500 (44%)]\tLoss: 2.035077\n",
      "Train Epoch: 0 [213760/482500 (44%)]\tLoss: 2.050739\n",
      "Train Epoch: 0 [213920/482500 (44%)]\tLoss: 2.294804\n",
      "Train Epoch: 0 [214080/482500 (44%)]\tLoss: 2.893214\n",
      "Train Epoch: 0 [214240/482500 (44%)]\tLoss: 2.813747\n",
      "Train Epoch: 0 [214400/482500 (44%)]\tLoss: 1.362658\n",
      "Train Epoch: 0 [214560/482500 (44%)]\tLoss: 3.464503\n",
      "Train Epoch: 0 [214720/482500 (45%)]\tLoss: 2.624443\n",
      "Train Epoch: 0 [214880/482500 (45%)]\tLoss: 2.066392\n",
      "Train Epoch: 0 [215040/482500 (45%)]\tLoss: 1.865364\n",
      "Train Epoch: 0 [215200/482500 (45%)]\tLoss: 2.187625\n",
      "Train Epoch: 0 [215360/482500 (45%)]\tLoss: 2.516351\n",
      "Train Epoch: 0 [215520/482500 (45%)]\tLoss: 1.773129\n",
      "Train Epoch: 0 [215680/482500 (45%)]\tLoss: 1.110928\n",
      "Train Epoch: 0 [215840/482500 (45%)]\tLoss: 2.391818\n",
      "Train Epoch: 0 [216000/482500 (45%)]\tLoss: 3.224970\n",
      "Train Epoch: 0 [216160/482500 (45%)]\tLoss: 21.097580\n",
      "Train Epoch: 0 [216320/482500 (45%)]\tLoss: 25.399214\n",
      "Train Epoch: 0 [216480/482500 (45%)]\tLoss: 21.330462\n",
      "Train Epoch: 0 [216640/482500 (45%)]\tLoss: 9.224925\n",
      "Train Epoch: 0 [216800/482500 (45%)]\tLoss: 3.752560\n",
      "Train Epoch: 0 [216960/482500 (45%)]\tLoss: 3.855927\n",
      "Train Epoch: 0 [217120/482500 (45%)]\tLoss: 5.357313\n",
      "Train Epoch: 0 [217280/482500 (45%)]\tLoss: 18.559784\n",
      "Train Epoch: 0 [217440/482500 (45%)]\tLoss: 4.105803\n",
      "Train Epoch: 0 [217600/482500 (45%)]\tLoss: 2.735472\n",
      "Train Epoch: 0 [217760/482500 (45%)]\tLoss: 3.203635\n",
      "Train Epoch: 0 [217920/482500 (45%)]\tLoss: 2.461619\n",
      "Train Epoch: 0 [218080/482500 (45%)]\tLoss: 2.043547\n",
      "Train Epoch: 0 [218240/482500 (45%)]\tLoss: 2.098061\n",
      "Train Epoch: 0 [218400/482500 (45%)]\tLoss: 2.265418\n",
      "Train Epoch: 0 [218560/482500 (45%)]\tLoss: 2.332912\n",
      "Train Epoch: 0 [218720/482500 (45%)]\tLoss: 2.779696\n",
      "Train Epoch: 0 [218880/482500 (45%)]\tLoss: 1.822548\n",
      "Train Epoch: 0 [219040/482500 (45%)]\tLoss: 2.085529\n",
      "Train Epoch: 0 [219200/482500 (45%)]\tLoss: 2.441337\n",
      "Train Epoch: 0 [219360/482500 (45%)]\tLoss: 18.820225\n",
      "Train Epoch: 0 [219520/482500 (45%)]\tLoss: 10.823078\n",
      "Train Epoch: 0 [219680/482500 (46%)]\tLoss: 3.045531\n",
      "Train Epoch: 0 [219840/482500 (46%)]\tLoss: 3.537829\n",
      "Train Epoch: 0 [220000/482500 (46%)]\tLoss: 2.057765\n",
      "Train Epoch: 0 [220160/482500 (46%)]\tLoss: 2.147171\n",
      "Train Epoch: 0 [220320/482500 (46%)]\tLoss: 1.746465\n",
      "Train Epoch: 0 [220480/482500 (46%)]\tLoss: 3.662291\n",
      "Train Epoch: 0 [220640/482500 (46%)]\tLoss: 2.265905\n",
      "Train Epoch: 0 [220800/482500 (46%)]\tLoss: 1.979394\n",
      "Train Epoch: 0 [220960/482500 (46%)]\tLoss: 1.657195\n",
      "Train Epoch: 0 [221120/482500 (46%)]\tLoss: 1.964244\n",
      "Train Epoch: 0 [221280/482500 (46%)]\tLoss: 1.762511\n",
      "Train Epoch: 0 [221440/482500 (46%)]\tLoss: 2.484728\n",
      "Train Epoch: 0 [221600/482500 (46%)]\tLoss: 3.755559\n",
      "Train Epoch: 0 [221760/482500 (46%)]\tLoss: 2.753148\n",
      "Train Epoch: 0 [221920/482500 (46%)]\tLoss: 2.678271\n",
      "Train Epoch: 0 [222080/482500 (46%)]\tLoss: 1.976638\n",
      "Train Epoch: 0 [222240/482500 (46%)]\tLoss: 2.363120\n",
      "Train Epoch: 0 [222400/482500 (46%)]\tLoss: 2.366792\n",
      "Train Epoch: 0 [222560/482500 (46%)]\tLoss: 2.281487\n",
      "Train Epoch: 0 [222720/482500 (46%)]\tLoss: 1.592375\n",
      "Train Epoch: 0 [222880/482500 (46%)]\tLoss: 3.696717\n",
      "Train Epoch: 0 [223040/482500 (46%)]\tLoss: 2.957221\n",
      "Train Epoch: 0 [223200/482500 (46%)]\tLoss: 2.190117\n",
      "Train Epoch: 0 [223360/482500 (46%)]\tLoss: 6.630040\n",
      "Train Epoch: 0 [223520/482500 (46%)]\tLoss: 23.249325\n",
      "Train Epoch: 0 [223680/482500 (46%)]\tLoss: 1.543911\n",
      "Train Epoch: 0 [223840/482500 (46%)]\tLoss: 2.463663\n",
      "Train Epoch: 0 [224000/482500 (46%)]\tLoss: 2.637894\n",
      "Train Epoch: 0 [224160/482500 (46%)]\tLoss: 2.549542\n",
      "Train Epoch: 0 [224320/482500 (46%)]\tLoss: 1.742326\n",
      "Train Epoch: 0 [224480/482500 (47%)]\tLoss: 2.018396\n",
      "Train Epoch: 0 [224640/482500 (47%)]\tLoss: 1.707170\n",
      "Train Epoch: 0 [224800/482500 (47%)]\tLoss: 4.492876\n",
      "Train Epoch: 0 [224960/482500 (47%)]\tLoss: 1.680871\n",
      "Train Epoch: 0 [225120/482500 (47%)]\tLoss: 3.463143\n",
      "Train Epoch: 0 [225280/482500 (47%)]\tLoss: 1.707441\n",
      "Train Epoch: 0 [225440/482500 (47%)]\tLoss: 1.896820\n",
      "Train Epoch: 0 [225600/482500 (47%)]\tLoss: 1.526491\n",
      "Train Epoch: 0 [225760/482500 (47%)]\tLoss: 1.569662\n",
      "Train Epoch: 0 [225920/482500 (47%)]\tLoss: 2.291523\n",
      "Train Epoch: 0 [226080/482500 (47%)]\tLoss: 3.594674\n",
      "Train Epoch: 0 [226240/482500 (47%)]\tLoss: 1.855853\n",
      "Train Epoch: 0 [226400/482500 (47%)]\tLoss: 170.087601\n",
      "Train Epoch: 0 [226560/482500 (47%)]\tLoss: 18.319748\n",
      "Train Epoch: 0 [226720/482500 (47%)]\tLoss: 8.043118\n",
      "Train Epoch: 0 [226880/482500 (47%)]\tLoss: 5.741628\n",
      "Train Epoch: 0 [227040/482500 (47%)]\tLoss: 4.827491\n",
      "Train Epoch: 0 [227200/482500 (47%)]\tLoss: 5.428840\n",
      "Train Epoch: 0 [227360/482500 (47%)]\tLoss: 4.668809\n",
      "Train Epoch: 0 [227520/482500 (47%)]\tLoss: 3.988813\n",
      "Train Epoch: 0 [227680/482500 (47%)]\tLoss: 4.238868\n",
      "Train Epoch: 0 [227840/482500 (47%)]\tLoss: 4.309045\n",
      "Train Epoch: 0 [228000/482500 (47%)]\tLoss: 4.432794\n",
      "Train Epoch: 0 [228160/482500 (47%)]\tLoss: 3.550554\n",
      "Train Epoch: 0 [228320/482500 (47%)]\tLoss: 3.596662\n",
      "Train Epoch: 0 [228480/482500 (47%)]\tLoss: 3.774680\n",
      "Train Epoch: 0 [228640/482500 (47%)]\tLoss: 3.976129\n",
      "Train Epoch: 0 [228800/482500 (47%)]\tLoss: 2.998379\n",
      "Train Epoch: 0 [228960/482500 (47%)]\tLoss: 40.010597\n",
      "Train Epoch: 0 [229120/482500 (47%)]\tLoss: 14.441509\n",
      "Train Epoch: 0 [229280/482500 (48%)]\tLoss: 12.472736\n",
      "Train Epoch: 0 [229440/482500 (48%)]\tLoss: 4.155255\n",
      "Train Epoch: 0 [229600/482500 (48%)]\tLoss: 5.520327\n",
      "Train Epoch: 0 [229760/482500 (48%)]\tLoss: 3.660554\n",
      "Train Epoch: 0 [229920/482500 (48%)]\tLoss: 2.746825\n",
      "Train Epoch: 0 [230080/482500 (48%)]\tLoss: 3.657292\n",
      "Train Epoch: 0 [230240/482500 (48%)]\tLoss: 3.567118\n",
      "Train Epoch: 0 [230400/482500 (48%)]\tLoss: 3.561737\n",
      "Train Epoch: 0 [230560/482500 (48%)]\tLoss: 4.191914\n",
      "Train Epoch: 0 [230720/482500 (48%)]\tLoss: 4.240982\n",
      "Train Epoch: 0 [230880/482500 (48%)]\tLoss: 3.431084\n",
      "Train Epoch: 0 [231040/482500 (48%)]\tLoss: 1.718243\n",
      "Train Epoch: 0 [231200/482500 (48%)]\tLoss: 2.297508\n",
      "Train Epoch: 0 [231360/482500 (48%)]\tLoss: 2.668189\n",
      "Train Epoch: 0 [231520/482500 (48%)]\tLoss: 67.762230\n",
      "Train Epoch: 0 [231680/482500 (48%)]\tLoss: 6.605706\n",
      "Train Epoch: 0 [231840/482500 (48%)]\tLoss: 6.248962\n",
      "Train Epoch: 0 [232000/482500 (48%)]\tLoss: 10.551393\n",
      "Train Epoch: 0 [232160/482500 (48%)]\tLoss: 2.602102\n",
      "Train Epoch: 0 [232320/482500 (48%)]\tLoss: 2.586389\n",
      "Train Epoch: 0 [232480/482500 (48%)]\tLoss: 3.836965\n",
      "Train Epoch: 0 [232640/482500 (48%)]\tLoss: 4.028711\n",
      "Train Epoch: 0 [232800/482500 (48%)]\tLoss: 3.973605\n",
      "Train Epoch: 0 [232960/482500 (48%)]\tLoss: 2.426457\n",
      "Train Epoch: 0 [233120/482500 (48%)]\tLoss: 1.789919\n",
      "Train Epoch: 0 [233280/482500 (48%)]\tLoss: 2.741557\n",
      "Train Epoch: 0 [233440/482500 (48%)]\tLoss: 2.566793\n",
      "Train Epoch: 0 [233600/482500 (48%)]\tLoss: 2.290846\n",
      "Train Epoch: 0 [233760/482500 (48%)]\tLoss: 3.235369\n",
      "Train Epoch: 0 [233920/482500 (48%)]\tLoss: 2.620848\n",
      "Train Epoch: 0 [234080/482500 (49%)]\tLoss: 3.926808\n",
      "Train Epoch: 0 [234240/482500 (49%)]\tLoss: 4.117082\n",
      "Train Epoch: 0 [234400/482500 (49%)]\tLoss: 3.661976\n",
      "Train Epoch: 0 [234560/482500 (49%)]\tLoss: 3.272099\n",
      "Train Epoch: 0 [234720/482500 (49%)]\tLoss: 662.960388\n",
      "Train Epoch: 0 [234880/482500 (49%)]\tLoss: 21.216393\n",
      "Train Epoch: 0 [235040/482500 (49%)]\tLoss: 13.799725\n",
      "Train Epoch: 0 [235200/482500 (49%)]\tLoss: 3.046661\n",
      "Train Epoch: 0 [235360/482500 (49%)]\tLoss: 3.543633\n",
      "Train Epoch: 0 [235520/482500 (49%)]\tLoss: 3.371856\n",
      "Train Epoch: 0 [235680/482500 (49%)]\tLoss: 2.856341\n",
      "Train Epoch: 0 [235840/482500 (49%)]\tLoss: 4.322238\n",
      "Train Epoch: 0 [236000/482500 (49%)]\tLoss: 20.218220\n",
      "Train Epoch: 0 [236160/482500 (49%)]\tLoss: 4.893587\n",
      "Train Epoch: 0 [236320/482500 (49%)]\tLoss: 9.377181\n",
      "Train Epoch: 0 [236480/482500 (49%)]\tLoss: 4.019435\n",
      "Train Epoch: 0 [236640/482500 (49%)]\tLoss: 2.060813\n",
      "Train Epoch: 0 [236800/482500 (49%)]\tLoss: 2.110733\n",
      "Train Epoch: 0 [236960/482500 (49%)]\tLoss: 4.905408\n",
      "Train Epoch: 0 [237120/482500 (49%)]\tLoss: 2.446437\n",
      "Train Epoch: 0 [237280/482500 (49%)]\tLoss: 2.631770\n",
      "Train Epoch: 0 [237440/482500 (49%)]\tLoss: 2.568425\n",
      "Train Epoch: 0 [237600/482500 (49%)]\tLoss: 2.444521\n",
      "Train Epoch: 0 [237760/482500 (49%)]\tLoss: 1.559305\n",
      "Train Epoch: 0 [237920/482500 (49%)]\tLoss: 1.854654\n",
      "Train Epoch: 0 [238080/482500 (49%)]\tLoss: 3.007516\n",
      "Train Epoch: 0 [238240/482500 (49%)]\tLoss: 2.448081\n",
      "Train Epoch: 0 [238400/482500 (49%)]\tLoss: 14.236736\n",
      "Train Epoch: 0 [238560/482500 (49%)]\tLoss: 2.145935\n",
      "Train Epoch: 0 [238720/482500 (49%)]\tLoss: 2.511351\n",
      "Train Epoch: 0 [238880/482500 (50%)]\tLoss: 1.865905\n",
      "Train Epoch: 0 [239040/482500 (50%)]\tLoss: 2.346586\n",
      "Train Epoch: 0 [239200/482500 (50%)]\tLoss: 3.038855\n",
      "Train Epoch: 0 [239360/482500 (50%)]\tLoss: 3.065585\n",
      "Train Epoch: 0 [239520/482500 (50%)]\tLoss: 1.876455\n",
      "Train Epoch: 0 [239680/482500 (50%)]\tLoss: 2.874951\n",
      "Train Epoch: 0 [239840/482500 (50%)]\tLoss: 1.922102\n",
      "Train Epoch: 0 [240000/482500 (50%)]\tLoss: 1.599197\n",
      "Train Epoch: 0 [240160/482500 (50%)]\tLoss: 1.787218\n",
      "Train Epoch: 0 [240320/482500 (50%)]\tLoss: 2.107698\n",
      "Train Epoch: 0 [240480/482500 (50%)]\tLoss: 2.385927\n",
      "Train Epoch: 0 [240640/482500 (50%)]\tLoss: 2.130177\n",
      "Train Epoch: 0 [240800/482500 (50%)]\tLoss: 3.039658\n",
      "Train Epoch: 0 [240960/482500 (50%)]\tLoss: 2.420982\n",
      "Train Epoch: 0 [241120/482500 (50%)]\tLoss: 3.548784\n",
      "Train Epoch: 0 [241280/482500 (50%)]\tLoss: 2.651989\n",
      "Train Epoch: 0 [241440/482500 (50%)]\tLoss: 2.524641\n",
      "Train Epoch: 0 [241600/482500 (50%)]\tLoss: 2.224218\n",
      "Train Epoch: 0 [241760/482500 (50%)]\tLoss: 1.946031\n",
      "Train Epoch: 0 [241920/482500 (50%)]\tLoss: 1.960745\n",
      "Train Epoch: 0 [242080/482500 (50%)]\tLoss: 2.330892\n",
      "Train Epoch: 0 [242240/482500 (50%)]\tLoss: 1.938244\n",
      "Train Epoch: 0 [242400/482500 (50%)]\tLoss: 1.782795\n",
      "Train Epoch: 0 [242560/482500 (50%)]\tLoss: 1.879478\n",
      "Train Epoch: 0 [242720/482500 (50%)]\tLoss: 2.211389\n",
      "Train Epoch: 0 [242880/482500 (50%)]\tLoss: 1.915204\n",
      "Train Epoch: 0 [243040/482500 (50%)]\tLoss: 2.575211\n",
      "Train Epoch: 0 [243200/482500 (50%)]\tLoss: 2.226613\n",
      "Train Epoch: 0 [243360/482500 (50%)]\tLoss: 2.474749\n",
      "Train Epoch: 0 [243520/482500 (50%)]\tLoss: 1.885056\n",
      "Train Epoch: 0 [243680/482500 (51%)]\tLoss: 166.622849\n",
      "Train Epoch: 0 [243840/482500 (51%)]\tLoss: 45.963428\n",
      "Train Epoch: 0 [244000/482500 (51%)]\tLoss: 5.111543\n",
      "Train Epoch: 0 [244160/482500 (51%)]\tLoss: 2.399401\n",
      "Train Epoch: 0 [244320/482500 (51%)]\tLoss: 2.735070\n",
      "Train Epoch: 0 [244480/482500 (51%)]\tLoss: 2.666208\n",
      "Train Epoch: 0 [244640/482500 (51%)]\tLoss: 52.657795\n",
      "Train Epoch: 0 [244800/482500 (51%)]\tLoss: 7.371208\n",
      "Train Epoch: 0 [244960/482500 (51%)]\tLoss: 2.728760\n",
      "Train Epoch: 0 [245120/482500 (51%)]\tLoss: 3.165366\n",
      "Train Epoch: 0 [245280/482500 (51%)]\tLoss: 2.674425\n",
      "Train Epoch: 0 [245440/482500 (51%)]\tLoss: 2.398638\n",
      "Train Epoch: 0 [245600/482500 (51%)]\tLoss: 2.070557\n",
      "Train Epoch: 0 [245760/482500 (51%)]\tLoss: 2.114587\n",
      "Train Epoch: 0 [245920/482500 (51%)]\tLoss: 2.000272\n",
      "Train Epoch: 0 [246080/482500 (51%)]\tLoss: 2.439192\n",
      "Train Epoch: 0 [246240/482500 (51%)]\tLoss: 1.961783\n",
      "Train Epoch: 0 [246400/482500 (51%)]\tLoss: 1.351854\n",
      "Train Epoch: 0 [246560/482500 (51%)]\tLoss: 2.600618\n",
      "Train Epoch: 0 [246720/482500 (51%)]\tLoss: 2.788422\n",
      "Train Epoch: 0 [246880/482500 (51%)]\tLoss: 2.715652\n",
      "Train Epoch: 0 [247040/482500 (51%)]\tLoss: 1.822959\n",
      "Train Epoch: 0 [247200/482500 (51%)]\tLoss: 2.712145\n",
      "Train Epoch: 0 [247360/482500 (51%)]\tLoss: 2.171187\n",
      "Train Epoch: 0 [247520/482500 (51%)]\tLoss: 1.413866\n",
      "Train Epoch: 0 [247680/482500 (51%)]\tLoss: 1.438331\n",
      "Train Epoch: 0 [247840/482500 (51%)]\tLoss: 1.293828\n",
      "Train Epoch: 0 [248000/482500 (51%)]\tLoss: 2.506864\n",
      "Train Epoch: 0 [248160/482500 (51%)]\tLoss: 2.561859\n",
      "Train Epoch: 0 [248320/482500 (51%)]\tLoss: 2.148029\n",
      "Train Epoch: 0 [248480/482500 (51%)]\tLoss: 16.690065\n",
      "Train Epoch: 0 [248640/482500 (52%)]\tLoss: 4.642495\n",
      "Train Epoch: 0 [248800/482500 (52%)]\tLoss: 1.134614\n",
      "Train Epoch: 0 [248960/482500 (52%)]\tLoss: 4.594451\n",
      "Train Epoch: 0 [249120/482500 (52%)]\tLoss: 2.952257\n",
      "Train Epoch: 0 [249280/482500 (52%)]\tLoss: 1.438677\n",
      "Train Epoch: 0 [249440/482500 (52%)]\tLoss: 2.311951\n",
      "Train Epoch: 0 [249600/482500 (52%)]\tLoss: 2.334618\n",
      "Train Epoch: 0 [249760/482500 (52%)]\tLoss: 2.334520\n",
      "Train Epoch: 0 [249920/482500 (52%)]\tLoss: 1.926882\n",
      "Train Epoch: 0 [250080/482500 (52%)]\tLoss: 1.804409\n",
      "Train Epoch: 0 [250240/482500 (52%)]\tLoss: 1.810695\n",
      "Train Epoch: 0 [250400/482500 (52%)]\tLoss: 974719424.000000\n",
      "Train Epoch: 0 [250560/482500 (52%)]\tLoss: 67.693115\n",
      "Train Epoch: 0 [250720/482500 (52%)]\tLoss: 13.414217\n",
      "Train Epoch: 0 [250880/482500 (52%)]\tLoss: 11.711628\n",
      "Train Epoch: 0 [251040/482500 (52%)]\tLoss: 2.875904\n",
      "Train Epoch: 0 [251200/482500 (52%)]\tLoss: 5.599458\n",
      "Train Epoch: 0 [251360/482500 (52%)]\tLoss: 2.653676\n",
      "Train Epoch: 0 [251520/482500 (52%)]\tLoss: 4.928841\n",
      "Train Epoch: 0 [251680/482500 (52%)]\tLoss: 4.935368\n",
      "Train Epoch: 0 [251840/482500 (52%)]\tLoss: 2.671717\n",
      "Train Epoch: 0 [252000/482500 (52%)]\tLoss: 4.004720\n",
      "Train Epoch: 0 [252160/482500 (52%)]\tLoss: 3.936951\n",
      "Train Epoch: 0 [252320/482500 (52%)]\tLoss: 2.693050\n",
      "Train Epoch: 0 [252480/482500 (52%)]\tLoss: 4.121110\n",
      "Train Epoch: 0 [252640/482500 (52%)]\tLoss: 6.308332\n",
      "Train Epoch: 0 [252800/482500 (52%)]\tLoss: 6.198484\n",
      "Train Epoch: 0 [252960/482500 (52%)]\tLoss: 4.817181\n",
      "Train Epoch: 0 [253120/482500 (52%)]\tLoss: 4.642818\n",
      "Train Epoch: 0 [253280/482500 (52%)]\tLoss: 2.817357\n",
      "Train Epoch: 0 [253440/482500 (53%)]\tLoss: 3.557230\n",
      "Train Epoch: 0 [253600/482500 (53%)]\tLoss: 3.462537\n",
      "Train Epoch: 0 [253760/482500 (53%)]\tLoss: 2.792338\n",
      "Train Epoch: 0 [253920/482500 (53%)]\tLoss: 3.393135\n",
      "Train Epoch: 0 [254080/482500 (53%)]\tLoss: 3.268744\n",
      "Train Epoch: 0 [254240/482500 (53%)]\tLoss: 4.381431\n",
      "Train Epoch: 0 [254400/482500 (53%)]\tLoss: 2.231299\n",
      "Train Epoch: 0 [254560/482500 (53%)]\tLoss: 2.080302\n",
      "Train Epoch: 0 [254720/482500 (53%)]\tLoss: 3.194197\n",
      "Train Epoch: 0 [254880/482500 (53%)]\tLoss: 3.593121\n",
      "Train Epoch: 0 [255040/482500 (53%)]\tLoss: 3.546756\n",
      "Train Epoch: 0 [255200/482500 (53%)]\tLoss: 18.032150\n",
      "Train Epoch: 0 [255360/482500 (53%)]\tLoss: 3.113228\n",
      "Train Epoch: 0 [255520/482500 (53%)]\tLoss: 22.880428\n",
      "Train Epoch: 0 [255680/482500 (53%)]\tLoss: 4.238249\n",
      "Train Epoch: 0 [255840/482500 (53%)]\tLoss: 3.821303\n",
      "Train Epoch: 0 [256000/482500 (53%)]\tLoss: 2.716538\n",
      "Train Epoch: 0 [256160/482500 (53%)]\tLoss: 3.336535\n",
      "Train Epoch: 0 [256320/482500 (53%)]\tLoss: 5.105008\n",
      "Train Epoch: 0 [256480/482500 (53%)]\tLoss: 1.872164\n",
      "Train Epoch: 0 [256640/482500 (53%)]\tLoss: 201.630997\n",
      "Train Epoch: 0 [256800/482500 (53%)]\tLoss: 30.832661\n",
      "Train Epoch: 0 [256960/482500 (53%)]\tLoss: 6.992096\n",
      "Train Epoch: 0 [257120/482500 (53%)]\tLoss: 10.707612\n",
      "Train Epoch: 0 [257280/482500 (53%)]\tLoss: 10.724342\n",
      "Train Epoch: 0 [257440/482500 (53%)]\tLoss: 10.071131\n",
      "Train Epoch: 0 [257600/482500 (53%)]\tLoss: 6.605789\n",
      "Train Epoch: 0 [257760/482500 (53%)]\tLoss: 7.267220\n",
      "Train Epoch: 0 [257920/482500 (53%)]\tLoss: 8.831029\n",
      "Train Epoch: 0 [258080/482500 (53%)]\tLoss: 9.857705\n",
      "Train Epoch: 0 [258240/482500 (54%)]\tLoss: 4.738086\n",
      "Train Epoch: 0 [258400/482500 (54%)]\tLoss: 4.386775\n",
      "Train Epoch: 0 [258560/482500 (54%)]\tLoss: 10.405956\n",
      "Train Epoch: 0 [258720/482500 (54%)]\tLoss: 6.774271\n",
      "Train Epoch: 0 [258880/482500 (54%)]\tLoss: 6.769260\n",
      "Train Epoch: 0 [259040/482500 (54%)]\tLoss: 5.443572\n",
      "Train Epoch: 0 [259200/482500 (54%)]\tLoss: 5.114934\n",
      "Train Epoch: 0 [259360/482500 (54%)]\tLoss: 7.296576\n",
      "Train Epoch: 0 [259520/482500 (54%)]\tLoss: 3.851967\n",
      "Train Epoch: 0 [259680/482500 (54%)]\tLoss: 5.454750\n",
      "Train Epoch: 0 [259840/482500 (54%)]\tLoss: 3.906432\n",
      "Train Epoch: 0 [260000/482500 (54%)]\tLoss: 4.365294\n",
      "Train Epoch: 0 [260160/482500 (54%)]\tLoss: 5.586780\n",
      "Train Epoch: 0 [260320/482500 (54%)]\tLoss: 2.870836\n",
      "Train Epoch: 0 [260480/482500 (54%)]\tLoss: 3.671373\n",
      "Train Epoch: 0 [260640/482500 (54%)]\tLoss: 3.501686\n",
      "Train Epoch: 0 [260800/482500 (54%)]\tLoss: 5.926489\n",
      "Train Epoch: 0 [260960/482500 (54%)]\tLoss: 3.491270\n",
      "Train Epoch: 0 [261120/482500 (54%)]\tLoss: 3.608065\n",
      "Train Epoch: 0 [261280/482500 (54%)]\tLoss: 3.712149\n",
      "Train Epoch: 0 [261440/482500 (54%)]\tLoss: 3.855363\n",
      "Train Epoch: 0 [261600/482500 (54%)]\tLoss: 1.487797\n",
      "Train Epoch: 0 [261760/482500 (54%)]\tLoss: 4.362329\n",
      "Train Epoch: 0 [261920/482500 (54%)]\tLoss: 3.076502\n",
      "Train Epoch: 0 [262080/482500 (54%)]\tLoss: 6.794985\n",
      "Train Epoch: 0 [262240/482500 (54%)]\tLoss: 3.855574\n",
      "Train Epoch: 0 [262400/482500 (54%)]\tLoss: 3.765793\n",
      "Train Epoch: 0 [262560/482500 (54%)]\tLoss: 4.178928\n",
      "Train Epoch: 0 [262720/482500 (54%)]\tLoss: 4.321465\n",
      "Train Epoch: 0 [262880/482500 (54%)]\tLoss: 2.946286\n",
      "Train Epoch: 0 [263040/482500 (55%)]\tLoss: 4.197100\n",
      "Train Epoch: 0 [263200/482500 (55%)]\tLoss: 3.650800\n",
      "Train Epoch: 0 [263360/482500 (55%)]\tLoss: 3.294253\n",
      "Train Epoch: 0 [263520/482500 (55%)]\tLoss: 4.012571\n",
      "Train Epoch: 0 [263680/482500 (55%)]\tLoss: 3.454084\n",
      "Train Epoch: 0 [263840/482500 (55%)]\tLoss: 3.239191\n",
      "Train Epoch: 0 [264000/482500 (55%)]\tLoss: 2.178822\n",
      "Train Epoch: 0 [264160/482500 (55%)]\tLoss: 4.263083\n",
      "Train Epoch: 0 [264320/482500 (55%)]\tLoss: 225.869476\n",
      "Train Epoch: 0 [264480/482500 (55%)]\tLoss: 65.864532\n",
      "Train Epoch: 0 [264640/482500 (55%)]\tLoss: 8.097232\n",
      "Train Epoch: 0 [264800/482500 (55%)]\tLoss: 6.157581\n",
      "Train Epoch: 0 [264960/482500 (55%)]\tLoss: 6.682795\n",
      "Train Epoch: 0 [265120/482500 (55%)]\tLoss: 3.122375\n",
      "Train Epoch: 0 [265280/482500 (55%)]\tLoss: 4.742208\n",
      "Train Epoch: 0 [265440/482500 (55%)]\tLoss: 3.464517\n",
      "Train Epoch: 0 [265600/482500 (55%)]\tLoss: 85.655930\n",
      "Train Epoch: 0 [265760/482500 (55%)]\tLoss: 6.825986\n",
      "Train Epoch: 0 [265920/482500 (55%)]\tLoss: 5.447217\n",
      "Train Epoch: 0 [266080/482500 (55%)]\tLoss: 4.157366\n",
      "Train Epoch: 0 [266240/482500 (55%)]\tLoss: 4.576086\n",
      "Train Epoch: 0 [266400/482500 (55%)]\tLoss: 3.976033\n",
      "Train Epoch: 0 [266560/482500 (55%)]\tLoss: 3.078366\n",
      "Train Epoch: 0 [266720/482500 (55%)]\tLoss: 3.394997\n",
      "Train Epoch: 0 [266880/482500 (55%)]\tLoss: 5.140670\n",
      "Train Epoch: 0 [267040/482500 (55%)]\tLoss: 2.763372\n",
      "Train Epoch: 0 [267200/482500 (55%)]\tLoss: 4.009643\n",
      "Train Epoch: 0 [267360/482500 (55%)]\tLoss: 2.844203\n",
      "Train Epoch: 0 [267520/482500 (55%)]\tLoss: 388852715880448.000000\n",
      "Train Epoch: 0 [267680/482500 (55%)]\tLoss: 150.305069\n",
      "Train Epoch: 0 [267840/482500 (56%)]\tLoss: 83.904137\n",
      "Train Epoch: 0 [268000/482500 (56%)]\tLoss: 12.620563\n",
      "Train Epoch: 0 [268160/482500 (56%)]\tLoss: 7.535713\n",
      "Train Epoch: 0 [268320/482500 (56%)]\tLoss: 8.474073\n",
      "Train Epoch: 0 [268480/482500 (56%)]\tLoss: 14.216188\n",
      "Train Epoch: 0 [268640/482500 (56%)]\tLoss: 7.465603\n",
      "Train Epoch: 0 [268800/482500 (56%)]\tLoss: 20.689091\n",
      "Train Epoch: 0 [268960/482500 (56%)]\tLoss: 7.568444\n",
      "Train Epoch: 0 [269120/482500 (56%)]\tLoss: 7.294108\n",
      "Train Epoch: 0 [269280/482500 (56%)]\tLoss: 6.448277\n",
      "Train Epoch: 0 [269440/482500 (56%)]\tLoss: 10.325626\n",
      "Train Epoch: 0 [269600/482500 (56%)]\tLoss: 6.333363\n",
      "Train Epoch: 0 [269760/482500 (56%)]\tLoss: 4.643523\n",
      "Train Epoch: 0 [269920/482500 (56%)]\tLoss: 4.068853\n",
      "Train Epoch: 0 [270080/482500 (56%)]\tLoss: 5.434273\n",
      "Train Epoch: 0 [270240/482500 (56%)]\tLoss: 6.271471\n",
      "Train Epoch: 0 [270400/482500 (56%)]\tLoss: 17.626566\n",
      "Train Epoch: 0 [270560/482500 (56%)]\tLoss: 4.826352\n",
      "Train Epoch: 0 [270720/482500 (56%)]\tLoss: 6.019139\n",
      "Train Epoch: 0 [270880/482500 (56%)]\tLoss: 81.912704\n",
      "Train Epoch: 0 [271040/482500 (56%)]\tLoss: 45.590763\n",
      "Train Epoch: 0 [271200/482500 (56%)]\tLoss: 25.299747\n",
      "Train Epoch: 0 [271360/482500 (56%)]\tLoss: 10.918774\n",
      "Train Epoch: 0 [271520/482500 (56%)]\tLoss: 7.950034\n",
      "Train Epoch: 0 [271680/482500 (56%)]\tLoss: 9.814319\n",
      "Train Epoch: 0 [271840/482500 (56%)]\tLoss: 10.431067\n",
      "Train Epoch: 0 [272000/482500 (56%)]\tLoss: 11.140789\n",
      "Train Epoch: 0 [272160/482500 (56%)]\tLoss: 14.849321\n",
      "Train Epoch: 0 [272320/482500 (56%)]\tLoss: 9.304130\n",
      "Train Epoch: 0 [272480/482500 (56%)]\tLoss: 11.696149\n",
      "Train Epoch: 0 [272640/482500 (57%)]\tLoss: 9.032765\n",
      "Train Epoch: 0 [272800/482500 (57%)]\tLoss: 11.451797\n",
      "Train Epoch: 0 [272960/482500 (57%)]\tLoss: 8.829616\n",
      "Train Epoch: 0 [273120/482500 (57%)]\tLoss: 8.303190\n",
      "Train Epoch: 0 [273280/482500 (57%)]\tLoss: 7.746868\n",
      "Train Epoch: 0 [273440/482500 (57%)]\tLoss: 10.824648\n",
      "Train Epoch: 0 [273600/482500 (57%)]\tLoss: 9.692259\n",
      "Train Epoch: 0 [273760/482500 (57%)]\tLoss: 7.458519\n",
      "Train Epoch: 0 [273920/482500 (57%)]\tLoss: 8.119747\n",
      "Train Epoch: 0 [274080/482500 (57%)]\tLoss: 5.874111\n",
      "Train Epoch: 0 [274240/482500 (57%)]\tLoss: 8.026501\n",
      "Train Epoch: 0 [274400/482500 (57%)]\tLoss: 6.602339\n",
      "Train Epoch: 0 [274560/482500 (57%)]\tLoss: 6.601645\n",
      "Train Epoch: 0 [274720/482500 (57%)]\tLoss: 5.927361\n",
      "Train Epoch: 0 [274880/482500 (57%)]\tLoss: 6.249788\n",
      "Train Epoch: 0 [275040/482500 (57%)]\tLoss: 5.287998\n",
      "Train Epoch: 0 [275200/482500 (57%)]\tLoss: 6.362929\n",
      "Train Epoch: 0 [275360/482500 (57%)]\tLoss: 353.922455\n",
      "Train Epoch: 0 [275520/482500 (57%)]\tLoss: 6.194472\n",
      "Train Epoch: 0 [275680/482500 (57%)]\tLoss: 4.934560\n",
      "Train Epoch: 0 [275840/482500 (57%)]\tLoss: 5.070293\n",
      "Train Epoch: 0 [276000/482500 (57%)]\tLoss: 5.932547\n",
      "Train Epoch: 0 [276160/482500 (57%)]\tLoss: 5.058068\n",
      "Train Epoch: 0 [276320/482500 (57%)]\tLoss: 11.697699\n",
      "Train Epoch: 0 [276480/482500 (57%)]\tLoss: 7.453154\n",
      "Train Epoch: 0 [276640/482500 (57%)]\tLoss: 5.528912\n",
      "Train Epoch: 0 [276800/482500 (57%)]\tLoss: 4.327532\n",
      "Train Epoch: 0 [276960/482500 (57%)]\tLoss: 4.767460\n",
      "Train Epoch: 0 [277120/482500 (57%)]\tLoss: 4.666114\n",
      "Train Epoch: 0 [277280/482500 (57%)]\tLoss: 5.016009\n",
      "Train Epoch: 0 [277440/482500 (58%)]\tLoss: 11.276306\n",
      "Train Epoch: 0 [277600/482500 (58%)]\tLoss: 7.959488\n",
      "Train Epoch: 0 [277760/482500 (58%)]\tLoss: 10.140629\n",
      "Train Epoch: 0 [277920/482500 (58%)]\tLoss: 4.284451\n",
      "Train Epoch: 0 [278080/482500 (58%)]\tLoss: 5.999355\n",
      "Train Epoch: 0 [278240/482500 (58%)]\tLoss: 3.607044\n",
      "Train Epoch: 0 [278400/482500 (58%)]\tLoss: 5.417927\n",
      "Train Epoch: 0 [278560/482500 (58%)]\tLoss: 3.828707\n",
      "Train Epoch: 0 [278720/482500 (58%)]\tLoss: 4.573448\n",
      "Train Epoch: 0 [278880/482500 (58%)]\tLoss: 3.637917\n",
      "Train Epoch: 0 [279040/482500 (58%)]\tLoss: 4.346647\n",
      "Train Epoch: 0 [279200/482500 (58%)]\tLoss: 4.004869\n",
      "Train Epoch: 0 [279360/482500 (58%)]\tLoss: 4.110161\n",
      "Train Epoch: 0 [279520/482500 (58%)]\tLoss: 3.508265\n",
      "Train Epoch: 0 [279680/482500 (58%)]\tLoss: 3.895897\n",
      "Train Epoch: 0 [279840/482500 (58%)]\tLoss: 2.999215\n",
      "Train Epoch: 0 [280000/482500 (58%)]\tLoss: 3.299870\n",
      "Train Epoch: 0 [280160/482500 (58%)]\tLoss: 4.034627\n",
      "Train Epoch: 0 [280320/482500 (58%)]\tLoss: 3.395794\n",
      "Train Epoch: 0 [280480/482500 (58%)]\tLoss: 3.505707\n",
      "Train Epoch: 0 [280640/482500 (58%)]\tLoss: 2.921818\n",
      "Train Epoch: 0 [280800/482500 (58%)]\tLoss: 2.875980\n",
      "Train Epoch: 0 [280960/482500 (58%)]\tLoss: 3.291691\n",
      "Train Epoch: 0 [281120/482500 (58%)]\tLoss: 2.714079\n",
      "Train Epoch: 0 [281280/482500 (58%)]\tLoss: 4.383851\n",
      "Train Epoch: 0 [281440/482500 (58%)]\tLoss: 2.138338\n",
      "Train Epoch: 0 [281600/482500 (58%)]\tLoss: 3.228488\n",
      "Train Epoch: 0 [281760/482500 (58%)]\tLoss: 2.293440\n",
      "Train Epoch: 0 [281920/482500 (58%)]\tLoss: 3.523481\n",
      "Train Epoch: 0 [282080/482500 (58%)]\tLoss: 2.676394\n",
      "Train Epoch: 0 [282240/482500 (58%)]\tLoss: 3.411278\n",
      "Train Epoch: 0 [282400/482500 (59%)]\tLoss: 2.087248\n",
      "Train Epoch: 0 [282560/482500 (59%)]\tLoss: 2.070575\n",
      "Train Epoch: 0 [282720/482500 (59%)]\tLoss: 2.425187\n",
      "Train Epoch: 0 [282880/482500 (59%)]\tLoss: 3.747258\n",
      "Train Epoch: 0 [283040/482500 (59%)]\tLoss: 3.362630\n",
      "Train Epoch: 0 [283200/482500 (59%)]\tLoss: 3.410735\n",
      "Train Epoch: 0 [283360/482500 (59%)]\tLoss: 2.209203\n",
      "Train Epoch: 0 [283520/482500 (59%)]\tLoss: 2.196341\n",
      "Train Epoch: 0 [283680/482500 (59%)]\tLoss: 2.488351\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-af950977bd11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mlog_interval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_mse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#     vis.line(X=torch.ones((1,1)).cpu()*epoch,Y=torch.Tensor([epoch_loss]).unsqueeze(0).cpu(),win=loss_window,update='append',name='loss')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#         vis.line(X=torch.ones((1,1)).cpu()*epoch,Y=torch.Tensor([epoch_mse]).unsqueeze(0).cpu(),win=loss_window,update='append',name='mse_loss')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-7059a8bb683f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, writer)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mmse_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epoch, epoch + 200):\n",
    "    log_interval = 20\n",
    "    epoch, epoch_loss, epoch_mse = train(epoch, writer)\n",
    "#     vis.line(X=torch.ones((1,1)).cpu()*epoch,Y=torch.Tensor([epoch_loss]).unsqueeze(0).cpu(),win=loss_window,update='append',name='loss')\n",
    "#         vis.line(X=torch.ones((1,1)).cpu()*epoch,Y=torch.Tensor([epoch_mse]).unsqueeze(0).cpu(),win=loss_window,update='append',name='mse_loss')\n",
    "    writer.add_scalar('Loss/loss', epoch_loss, epoch)\n",
    "    writer.add_scalar('Loss/mse_loss', epoch_mse, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict()\n",
    "            }, 'checkpoints/highway_attention.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda:0\n",
      "convVAE(\n",
      "  (condnn): CondNN(\n",
      "    (cnn): Conv3d(\n",
      "      (adap_pool): AdaptiveAvgPool3d(output_size=(25, 100, 600))\n",
      "      (conv_layer1): Sequential(\n",
      "        (0): Conv3d(1, 16, kernel_size=(2, 3, 3), stride=(1, 1, 1))\n",
      "        (1): LeakyReLU(negative_slope=0.01)\n",
      "        (2): Conv3d(16, 16, kernel_size=(2, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (3): LeakyReLU(negative_slope=0.01)\n",
      "        (4): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (conv_layer2): Sequential(\n",
      "        (0): Conv3d(16, 32, kernel_size=(2, 3, 3), stride=(1, 1, 1))\n",
      "        (1): LeakyReLU(negative_slope=0.01)\n",
      "        (2): Conv3d(32, 32, kernel_size=(2, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (3): LeakyReLU(negative_slope=0.01)\n",
      "        (4): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (conv_layer5): Conv3d(32, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1))\n",
      "      (adap_pool2): AdaptiveAvgPool3d(output_size=(6, 10, 60))\n",
      "    )\n",
      "    (Attention): Attention(\n",
      "      (encoder_att): Linear(in_features=67, out_features=64, bias=True)\n",
      "      (condition_att): Linear(in_features=8, out_features=64, bias=True)\n",
      "      (full_att): Linear(in_features=64, out_features=1, bias=True)\n",
      "      (relu): ReLU()\n",
      "      (softmax): Softmax(dim=1)\n",
      "    )\n",
      "    (fc1): Linear(in_features=72, out_features=300, bias=True)\n",
      "  )\n",
      "  (encoder): Encoder(\n",
      "    (sequential): Sequential(\n",
      "      (0): Linear(in_features=304, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=1024, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "    (linear_means): Linear(in_features=512, out_features=50, bias=True)\n",
      "    (linear_log_var): Linear(in_features=512, out_features=50, bias=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (sequential): Sequential(\n",
      "      (0): Linear(in_features=350, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=1024, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (5): ReLU()\n",
      "      (6): Linear(in_features=512, out_features=4, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device\", device)\n",
    "\n",
    "model = convVAE(sample_size = X_dim, \n",
    "                  cnnout_size = cnn_out_size, \n",
    "                  cond_out_size = cond_out_size, \n",
    "                  encoder_layer_sizes = [512,1024,512], \n",
    "                  latent_size = z_dim, \n",
    "                  decoder_layer_sizes = [512,1024,512]).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "checkpoint = torch.load('checkpoints/highway_attention.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "\n",
    "model.eval()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "%matplotlib\n",
    "from utils.HighWay import plotData, plotOrientSpeed, plotAlpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "358222\n"
     ]
    }
   ],
   "source": [
    "test_data = test_loader.dataset\n",
    "viz_idx =   torch.randint(0,len(test_data),[1]).item()  \n",
    "#  变道场景idx\n",
    "#  \n",
    "print(viz_idx)\n",
    "\n",
    "batch = test_data[viz_idx]\n",
    "startgoal = torch.from_numpy(batch[\"start_goal\"]).to(device)\n",
    "occ = torch.from_numpy(batch[\"observation\"])\n",
    "occ = occ.unsqueeze(0)\n",
    "occ = occ.unsqueeze(1)\n",
    "adap_pool = nn.AdaptiveAvgPool3d((25,100, 600))\n",
    "occ = adap_pool(occ)\n",
    "occ = occ.to(device)\n",
    "data = torch.from_numpy(batch[\"data\"]).to(device)\n",
    "\n",
    "occ=occ.cpu().detach().numpy()\n",
    "startgoal=startgoal.cpu().detach().numpy()\n",
    "data=data.cpu().detach().numpy()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "plotData(occ, startgoal, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161608\n"
     ]
    }
   ],
   "source": [
    "test_data = test_loader.dataset\n",
    "viz_idx =   torch.randint(0,len(test_data),[1]).item()  \n",
    "#  变道场景idx\n",
    "#  308958 82146 161608\n",
    "viz_idx = 161608\n",
    "print(viz_idx)\n",
    "\n",
    "batch = test_data[viz_idx]\n",
    "startgoal = torch.from_numpy(batch[\"start_goal\"]).to(device)\n",
    "occ = torch.from_numpy(batch[\"observation\"])\n",
    "occ = occ.unsqueeze(0)\n",
    "occ = occ.unsqueeze(1)\n",
    "adap_pool = nn.AdaptiveAvgPool3d((25,100, 600))\n",
    "occ = adap_pool(occ)\n",
    "occ = occ.to(device)\n",
    "\n",
    "data = torch.from_numpy(batch[\"data\"]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    y_viz = torch.randn(1,4).to(device)\n",
    "    for i in range(0, 10):\n",
    "        num_viz = 8\n",
    "        y_viz_p, alpha = model.inference(startgoal.expand(num_viz, X_dim * 2).to(device), \n",
    "                                occ.expand(num_viz, 1, -1, -1, -1).to(device), num_viz)\n",
    "        torch.cuda.empty_cache()\n",
    "        y_viz = torch.cat((y_viz_p, y_viz), dim = 0)\n",
    "\n",
    "y_viz=y_viz.cpu().detach().numpy()\n",
    "occ=occ.cpu().detach().numpy()\n",
    "startgoal=startgoal.cpu().detach().numpy()\n",
    "data=data.cpu().detach().numpy()\n",
    "alpha=alpha.cpu().detach().numpy()\n",
    "torch.cuda.empty_cache()\n",
    "# from utils.NarrowPassage import plotCondition, plotSample, plotSpeed, plotSampleAttention\n",
    "\n",
    "y_viz=y_viz[:-1]\n",
    "plotData(occ, startgoal, y_viz)\n",
    "plotOrientSpeed(startgoal, y_viz)\n",
    "plotAlpha(alpha)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
